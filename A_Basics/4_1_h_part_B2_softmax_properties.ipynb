{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B2: Softmax Properties - Temperature & Numerical Stability\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to **Part B2** of the Softmax Series!\n",
    "\n",
    "**StatQuest Style**: \"Now let's explore the cool properties of Softmax... clearly!\"\n",
    "\n",
    "### What We'll Cover\n",
    "\n",
    "1. Temperature parameter (controlling sharpness)\n",
    "2. Numerical stability trick (avoiding overflow)\n",
    "3. Confidence vs uncertainty\n",
    "4. Edge cases and practical considerations\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand the temperature parameter\n",
    "- Implement numerically stable softmax\n",
    "- Interpret confidence levels\n",
    "- Handle edge cases properly\n",
    "\n",
    "**Let's dive in!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Libraries imported\n",
      "âœ“ Ready to explore Softmax properties!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print('âœ“ Libraries imported')\n",
    "print('âœ“ Ready to explore Softmax properties!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Temperature Parameter\n",
    "\n",
    "## Softmax with Temperature\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         SOFTMAX WITH TEMPERATURE                            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Standard Softmax:                                         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚\n",
    "â”‚  â”‚         e^(x_i)                       â”‚                 â”‚\n",
    "â”‚  â”‚  S_i = â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚                 â”‚\n",
    "â”‚  â”‚        Î£ e^(x_j)                      â”‚                 â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Softmax with Temperature T:                               â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚\n",
    "â”‚  â”‚         e^(x_i/T)                     â”‚                 â”‚\n",
    "â”‚  â”‚  S_i = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”‚                 â”‚\n",
    "â”‚  â”‚        Î£ e^(x_j/T)                    â”‚                 â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Temperature Effects:                                      â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  T â†’ 0 (Low Temperature):                                  â”‚\n",
    "â”‚  â€¢ Sharp distribution                                      â”‚\n",
    "â”‚  â€¢ Confident predictions                                   â”‚\n",
    "â”‚  â€¢ Approaches one-hot encoding                             â”‚\n",
    "â”‚  â€¢ Like ArgMax                                             â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  T = 1 (Standard):                                         â”‚\n",
    "â”‚  â€¢ Normal softmax                                          â”‚\n",
    "â”‚  â€¢ Balanced                                                â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  T â†’ âˆ (High Temperature):                                 â”‚\n",
    "â”‚  â€¢ Soft distribution                                       â”‚\n",
    "â”‚  â€¢ Uncertain predictions                                   â”‚\n",
    "â”‚  â€¢ Approaches uniform distribution                         â”‚\n",
    "â”‚  â€¢ All classes equally likely                              â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOFTMAX WITH DIFFERENT TEMPERATURES\n",
      "======================================================================\n",
      "Logits: [2.  1.  0.1]\n",
      "\n",
      "Temperature T = 0.5:\n",
      "  Cat        0.8638 ( 86.4%)\n",
      "  Dog        0.1169 ( 11.7%)\n",
      "  Mouse      0.0193 (  1.9%)\n",
      "\n",
      "Temperature T = 1.0:\n",
      "  Cat        0.6590 ( 65.9%)\n",
      "  Dog        0.2424 ( 24.2%)\n",
      "  Mouse      0.0986 (  9.9%)\n",
      "\n",
      "Temperature T = 2.0:\n",
      "  Cat        0.5017 ( 50.2%)\n",
      "  Dog        0.3043 ( 30.4%)\n",
      "  Mouse      0.1940 ( 19.4%)\n",
      "\n",
      "Temperature T = 5.0:\n",
      "  Cat        0.3996 ( 40.0%)\n",
      "  Dog        0.3272 ( 32.7%)\n",
      "  Mouse      0.2733 ( 27.3%)\n",
      "\n",
      "======================================================================\n",
      "OBSERVATION:\n",
      "  Low T â†’ Sharp (confident)\n",
      "  High T â†’ Soft (uncertain)\n"
     ]
    }
   ],
   "source": [
    "# Implement softmax with temperature\n",
    "def softmax_temperature(logits, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Calculate softmax with temperature parameter.\n",
    "    \n",
    "    Args:\n",
    "        logits: Raw network outputs\n",
    "        temperature: Temperature parameter (default=1.0)\n",
    "    \n",
    "    Returns:\n",
    "        Probability distribution\n",
    "    \"\"\"\n",
    "    scaled_logits = logits / temperature\n",
    "    exp_logits = np.exp(scaled_logits)\n",
    "    return exp_logits / np.sum(exp_logits)\n",
    "\n",
    "# Test with different temperatures\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "classes = ['Cat', 'Dog', 'Mouse']\n",
    "temperatures = [0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "print('SOFTMAX WITH DIFFERENT TEMPERATURES')\n",
    "print('='*70)\n",
    "print(f'Logits: {logits}\\n')\n",
    "\n",
    "for T in temperatures:\n",
    "    probs = softmax_temperature(logits, T)\n",
    "    print(f'Temperature T = {T}:')\n",
    "    for cls, p in zip(classes, probs):\n",
    "        print(f'  {cls:<10} {p:.4f} ({p*100:>5.1f}%)')\n",
    "    print()\n",
    "\n",
    "print('='*70)\n",
    "print('OBSERVATION:')\n",
    "print('  Low T â†’ Sharp (confident)')\n",
    "print('  High T â†’ Soft (uncertain)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Numerical Stability\n",
    "\n",
    "## The Overflow Problem\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         NUMERICAL STABILITY PROBLEM                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Problem: Large logits cause overflow                      â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Example:                                                  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚\n",
    "â”‚  â”‚  Logits: [1000, 1001, 999]           â”‚                 â”‚\n",
    "â”‚  â”‚                                       â”‚                 â”‚\n",
    "â”‚  â”‚  e^1000 = HUGE NUMBER!                â”‚                 â”‚\n",
    "â”‚  â”‚  â†’ Overflow error                     â”‚                 â”‚\n",
    "â”‚  â”‚  â†’ NaN or Inf                         â”‚                 â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Solution: Subtract maximum before exp                     â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Stable Softmax:                                           â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚\n",
    "â”‚  â”‚  1. Find max: M = max(x)             â”‚                 â”‚\n",
    "â”‚  â”‚  2. Subtract: x' = x - M             â”‚                 â”‚\n",
    "â”‚  â”‚  3. Apply softmax to x'              â”‚                 â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Why this works:                                           â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚\n",
    "â”‚  â”‚  e^(x_i - M)     e^x_i / e^M         â”‚                 â”‚\n",
    "â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”‚                 â”‚\n",
    "â”‚  â”‚  Î£ e^(x_j - M)   Î£ e^x_j / e^M       â”‚                 â”‚\n",
    "â”‚  â”‚                                       â”‚                 â”‚\n",
    "â”‚  â”‚  e^M cancels out!                     â”‚                 â”‚\n",
    "â”‚  â”‚  Result is mathematically identical   â”‚                 â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  After subtracting max:                                    â”‚\n",
    "â”‚  â€¢ Largest value becomes 0                                 â”‚\n",
    "â”‚  â€¢ All others are negative                                 â”‚\n",
    "â”‚  â€¢ e^0 = 1 (safe!)                                         â”‚\n",
    "â”‚  â€¢ e^(negative) < 1 (safe!)                                â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMERICAL STABILITY DEMONSTRATION\n",
      "======================================================================\n",
      "Small logits: [1. 2. 3.]\n",
      "Unstable softmax: [0.09003057 0.24472847 0.66524096]\n",
      "Stable softmax:   [0.09003057 0.24472847 0.66524096]\n",
      "\n",
      "Large logits: [1000. 1001.  999.]\n",
      "Unstable softmax: [nan nan nan]\n",
      "Stable softmax:   [0.24472847 0.66524096 0.09003057]\n",
      "\n",
      "======================================================================\n",
      "CONCLUSION:\n",
      "  Always use the stable version in production!\n",
      "  Subtract max before exponentiating.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-417633057.py:35: RuntimeWarning: overflow encountered in exp\n",
      "  print('Unstable softmax:', np.exp(large_logits) / np.sum(np.exp(large_logits)))\n",
      "/tmp/ipython-input-417633057.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  print('Unstable softmax:', np.exp(large_logits) / np.sum(np.exp(large_logits)))\n"
     ]
    }
   ],
   "source": [
    "# Implement numerically stable softmax\n",
    "def softmax_stable(logits):\n",
    "    \"\"\"\n",
    "    Numerically stable softmax implementation.\n",
    "    \n",
    "    Args:\n",
    "        logits: Raw network outputs\n",
    "    \n",
    "    Returns:\n",
    "        Probability distribution\n",
    "    \"\"\"\n",
    "    # Step 1: Subtract maximum for numerical stability\n",
    "    logits_shifted = logits - np.max(logits)\n",
    "    \n",
    "    # Step 2: Exponentiate\n",
    "    exp_logits = np.exp(logits_shifted)\n",
    "    \n",
    "    # Step 3: Normalize\n",
    "    return exp_logits / np.sum(exp_logits)\n",
    "\n",
    "# Compare unstable vs stable\n",
    "print('NUMERICAL STABILITY DEMONSTRATION')\n",
    "print('='*70)\n",
    "\n",
    "# Small logits (both work)\n",
    "small_logits = np.array([1.0, 2.0, 3.0])\n",
    "print('Small logits:', small_logits)\n",
    "print('Unstable softmax:', np.exp(small_logits) / np.sum(np.exp(small_logits)))\n",
    "print('Stable softmax:  ', softmax_stable(small_logits))\n",
    "print()\n",
    "\n",
    "# Large logits (unstable fails)\n",
    "large_logits = np.array([1000.0, 1001.0, 999.0])\n",
    "print('Large logits:', large_logits)\n",
    "print('Unstable softmax:', np.exp(large_logits) / np.sum(np.exp(large_logits)))\n",
    "print('Stable softmax:  ', softmax_stable(large_logits))\n",
    "print()\n",
    "\n",
    "print('='*70)\n",
    "print('CONCLUSION:')\n",
    "print('  Always use the stable version in production!')\n",
    "print('  Subtract max before exponentiating.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Temperature Parameter**:\n",
    "   - Controls distribution sharpness\n",
    "   - Low T â†’ confident (sharp)\n",
    "   - High T â†’ uncertain (soft)\n",
    "   - Used in knowledge distillation\n",
    "\n",
    "2. **Numerical Stability**:\n",
    "   - Subtract max before exp\n",
    "   - Prevents overflow\n",
    "   - Mathematically equivalent\n",
    "   - Always use in production\n",
    "\n",
    "3. **Best Practices**:\n",
    "   - Always implement stable version\n",
    "   - Consider temperature for specific tasks\n",
    "   - Monitor for NaN/Inf values\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In **Part C1**, we'll:\n",
    "- Derive the softmax derivative (case i=j)\n",
    "- Use the quotient rule\n",
    "- Get the beautiful result: S_i(1 - S_i)\n",
    "- Understand the geometric interpretation\n",
    "\n",
    "**Double Bam!** ğŸ’¥ğŸ’¥"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
