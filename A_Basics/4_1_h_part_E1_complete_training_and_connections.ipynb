{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part E1: Complete Training & Connections to Advanced Architectures\n",
    "\n",
    "**StatQuest Style**: \"Putting it all together... clearly!\"\n",
    "\n",
    "### Complete Training Loop + RNN/Transformer/GAN Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE TRAINING LOOP\n",
      "======================================================================\n",
      "True label: Cat [1. 0. 0.]\n",
      "\n",
      "Epoch 1:\n",
      "  Prediction: [0.33333333 0.33333333 0.33333333]\n",
      "  Loss: 1.0986\n",
      "Epoch 2:\n",
      "  Prediction: [0.35591307 0.32204346 0.32204346]\n",
      "  Loss: 1.0331\n",
      "Epoch 3:\n",
      "  Prediction: [0.3783555  0.31082225 0.31082225]\n",
      "  Loss: 0.9719\n",
      "Epoch 4:\n",
      "  Prediction: [0.40052275 0.29973863 0.29973863]\n",
      "  Loss: 0.9150\n",
      "Epoch 5:\n",
      "  Prediction: [0.42229338 0.28885331 0.28885331]\n",
      "  Loss: 0.8621\n",
      "======================================================================\n",
      "Network is learning! Loss decreasing, Cat probability increasing!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "def train_step(logits, y_true, learning_rate=0.1):\n",
    "    \"\"\"Complete training step\"\"\"\n",
    "    # Forward pass\n",
    "    y_pred = softmax(logits)\n",
    "    loss = cross_entropy_loss(y_true, y_pred)\n",
    "    \n",
    "    # Backward pass (the magic gradient!)\n",
    "    gradient = y_pred - y_true\n",
    "    \n",
    "    # Update\n",
    "    logits_new = logits - learning_rate * gradient\n",
    "    \n",
    "    return logits_new, loss, y_pred\n",
    "\n",
    "# Training example\n",
    "logits = np.array([0.5, 0.5, 0.5])  # Start random\n",
    "y_true = np.array([1.0, 0.0, 0.0])  # True: Cat\n",
    "\n",
    "print('COMPLETE TRAINING LOOP')\n",
    "print('='*70)\n",
    "print(f'True label: Cat {y_true}\\n')\n",
    "\n",
    "for epoch in range(5):\n",
    "    logits, loss, y_pred = train_step(logits, y_true)\n",
    "    print(f'Epoch {epoch+1}:')\n",
    "    print(f'  Prediction: {y_pred}')\n",
    "    print(f'  Loss: {loss:.4f}')\n",
    "\n",
    "print('='*70)\n",
    "print('Network is learning! Loss decreasing, Cat probability increasing!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connections to Advanced Architectures\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         SOFTMAX IN ADVANCED ARCHITECTURES                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚  1. RNNs (Recurrent Neural Networks)                       â”‚\n",
    "â”‚     â€¢ Softmax at each time step                            â”‚\n",
    "â”‚     â€¢ Predicts next word/character                         â”‚\n",
    "â”‚     â€¢ Example: Language modeling                           â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  2. Transformers (Attention Mechanisms)                    â”‚\n",
    "â”‚     â€¢ Softmax in attention weights                         â”‚\n",
    "â”‚     â€¢ Decides which tokens to focus on                     â”‚\n",
    "â”‚     â€¢ Example: BERT, GPT, ChatGPT                          â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  3. GANs (Generative Adversarial Networks)                 â”‚\n",
    "â”‚     â€¢ Discriminator uses softmax                           â”‚\n",
    "â”‚     â€¢ Real vs Fake classification                          â”‚\n",
    "â”‚     â€¢ Example: Image generation                            â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  4. CNNs (Convolutional Neural Networks)                   â”‚\n",
    "â”‚     â€¢ Final classification layer                           â”‚\n",
    "â”‚     â€¢ Image classification                                 â”‚\n",
    "â”‚     â€¢ Example: ResNet, VGG                                 â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  5. Reinforcement Learning                                 â”‚\n",
    "â”‚     â€¢ Policy networks                                      â”‚\n",
    "â”‚     â€¢ Action selection                                     â”‚\n",
    "â”‚     â€¢ Example: AlphaGo, game AI                            â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Series Summary\n",
    "\n",
    "## What We Learned\n",
    "\n",
    "**Part A: Entropy & Information Theory**\n",
    "- Surprise = -logâ‚‚(p)\n",
    "- Entropy = Expected surprise\n",
    "- Measures uncertainty\n",
    "\n",
    "**Part B: Softmax**\n",
    "- Converts logits to probabilities\n",
    "- S_i = e^(x_i) / Î£ e^(x_j)\n",
    "- Temperature & numerical stability\n",
    "\n",
    "**Part C: Derivatives**\n",
    "- Case i=j: S_i(1 - S_i)\n",
    "- Case iâ‰ j: -S_i Ã— S_j\n",
    "- Complete Jacobian matrix\n",
    "\n",
    "**Part D: Cross Entropy & Magic**\n",
    "- Loss = -Î£ y_true Ã— log(y_pred)\n",
    "- Combined gradient: y_pred - y_true\n",
    "- Beautiful simplification!\n",
    "\n",
    "**Part E: Training & Applications**\n",
    "- Complete training loop\n",
    "- Used in RNNs, Transformers, GANs, CNNs\n",
    "- Foundation of modern deep learning\n",
    "\n",
    "## The Journey\n",
    "\n",
    "From simple coin flips to state-of-the-art AI!\n",
    "\n",
    "**QUADRUPLE BAM!** ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥\n",
    "\n",
    "---\n",
    "\n",
    "*You now understand softmax from first principles to advanced applications!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
