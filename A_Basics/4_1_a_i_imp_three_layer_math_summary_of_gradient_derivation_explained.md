### The Complete Backpropagation Algorithm:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  1. FORWARD PASS:                                      â”‚
â”‚     â€¢ Compute hâ‚ = Wâ‚Â·x + bâ‚                          â”‚
â”‚     â€¢ Compute hâ‚‚ = Wâ‚‚Â·hâ‚ + bâ‚‚                         â”‚
â”‚     â€¢ Compute Å· = Wâ‚ƒÂ·hâ‚‚ + bâ‚ƒ                          â”‚
â”‚                                                         â”‚
â”‚  2. COMPUTE LOSS:                                      â”‚
â”‚     â€¢ L = (Å· - y)Â²                                    â”‚
â”‚                                                         â”‚
â”‚  3. BACKWARD PASS (Compute Gradients):                  |
|     Generalized = âˆ‚L/âˆ‚Wn = 2 * error * Wn+1 * hn-1      |
|                   âˆ‚L/âˆ‚Wn = 2 * error * Wn+1             |
â”‚     â€¢ âˆ‚L/âˆ‚Wâ‚ƒ = 2(Å· - y) Ã— hâ‚‚                        â”‚
â”‚     â€¢ âˆ‚L/âˆ‚bâ‚ƒ = 2(Å· - y)                              â”‚
â”‚     â€¢ âˆ‚L/âˆ‚Wâ‚‚ = 2(Å· - y) Ã— Wâ‚ƒ Ã— hâ‚                   â”‚
â”‚     â€¢ âˆ‚L/âˆ‚bâ‚‚ = 2(Å· - y) Ã— Wâ‚ƒ                         â”‚
â”‚     â€¢ âˆ‚L/âˆ‚Wâ‚ = 2(Å· - y) Ã— Wâ‚ƒ Ã— Wâ‚‚ Ã— x               â”‚
â”‚     â€¢ âˆ‚L/âˆ‚bâ‚ = 2(Å· - y) Ã— Wâ‚ƒ Ã— Wâ‚‚                   â”‚
â”‚                                                        â”‚
â”‚  4. UPDATE PARAMETERS:                                 â”‚
â”‚     â€¢ Wâ‚ƒ â† Wâ‚ƒ - Î± Ã— (âˆ‚L/âˆ‚Wâ‚ƒ)                         â”‚
â”‚     â€¢ bâ‚ƒ â† bâ‚ƒ - Î± Ã— (âˆ‚L/âˆ‚bâ‚ƒ)                         â”‚
â”‚     â€¢ Wâ‚‚ â† Wâ‚‚ - Î± Ã— (âˆ‚L/âˆ‚Wâ‚‚)                         â”‚
â”‚     â€¢ bâ‚‚ â† bâ‚‚ - Î± Ã— (âˆ‚L/âˆ‚bâ‚‚)                         â”‚
â”‚     â€¢ Wâ‚ â† Wâ‚ - Î± Ã— (âˆ‚L/âˆ‚Wâ‚)                         â”‚
â”‚     â€¢ bâ‚ â† bâ‚ - Î± Ã— (âˆ‚L/âˆ‚bâ‚)                         â”‚
â”‚                                                         â”‚
â”‚  5. REPEAT until convergence                           â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”— Connection to Code

In Python/NumPy, the backward pass would look like:

```python
def backward(x, y, y_pred, h1, h2, W3, W2, learning_rate=0.01):
    """
    Compute gradients and update parameters for 3-layer linear network.
    """
    n = len(x)  # batch size
    
    # Error signal
    error = y_pred - y
    
    # Layer 3 gradients
    dW3 = (2.0 / n) * np.sum(error * h2)
    db3 = (2.0 / n) * np.sum(error)
    
    # Layer 2 gradients
    dW2 = (2.0 / n) * np.sum(error * W3 * h1)
    db2 = (2.0 / n) * np.sum(error * W3)
    
    # Layer 1 gradients
    dW1 = (2.0 / n) * np.sum(error * W3 * W2 * x)
    db1 = (2.0 / n) * np.sum(error * W3 * W2)
    
    # Update parameters
    W3 -= learning_rate * dW3
    b3 -= learning_rate * db3
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1
    
    return W3, b3, W2, b2, W1, b1
```
