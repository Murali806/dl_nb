# Mathematical Derivation of Gradients for N-Layer Neural Network with General Activation Functions

This document provides a **generalized mathematical framework** for backpropagation in an **n-layer neural network** with **arbitrary activation functions**.

---

## âœ… Problem Setup

### Network Architecture (N Layers, General Activation)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                         â”‚
â”‚  Input (x) â†’ Layer 1 (Ïƒâ‚) â†’ Layer 2 (Ïƒâ‚‚) â†’ ... â†’ Layer L (Ïƒâ‚—) â†’ Å·    â”‚
â”‚                                                                         â”‚
â”‚  Where Ïƒáµ¢ represents the activation function for layer i               â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### General Notation for L-Layer Network

**Layer l (for l = 1, 2, ..., L):**
- **Input**: hâ½Ë¡â»Â¹â¾ âˆˆ â„â¿Ë¡â»Â¹ (output from previous layer)
  - Special case: hâ½â°â¾ = x (network input)
- **Weights**: Wâ½Ë¡â¾ âˆˆ â„â¿Ë¡â»Â¹Ë£â¿Ë¡
- **Bias**: bâ½Ë¡â¾ âˆˆ â„â¿Ë¡
- **Linear output**: zâ½Ë¡â¾ = hâ½Ë¡â»Â¹â¾Wâ½Ë¡â¾ + bâ½Ë¡â¾ âˆˆ â„â¿Ë¡
- **Activation**: hâ½Ë¡â¾ = Ïƒâ‚—(zâ½Ë¡â¾) âˆˆ â„â¿Ë¡
  - Special case: hâ½á´¸â¾ = Å· (network output)

### Dimensions Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  Layer l:                                                   â”‚
â”‚  hâ½Ë¡â»Â¹â¾: (nâ‚—â‚‹â‚,)  â†’  Wâ½Ë¡â¾: (nâ‚—â‚‹â‚, nâ‚—)  â†’  zâ½Ë¡â¾: (nâ‚—,)    â”‚
â”‚                      bâ½Ë¡â¾: (nâ‚—,)        â†’  hâ½Ë¡â¾: (nâ‚—,)    â”‚
â”‚                                                             â”‚
â”‚  For batch of m samples:                                    â”‚
â”‚  Hâ½Ë¡â»Â¹â¾: (m, nâ‚—â‚‹â‚) â†’ Wâ½Ë¡â¾: (nâ‚—â‚‹â‚, nâ‚—) â†’ Zâ½Ë¡â¾: (m, nâ‚—)    â”‚
â”‚                      bâ½Ë¡â¾: (nâ‚—,)       â†’ Hâ½Ë¡â¾: (m, nâ‚—)    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Forward Propagation (General Form)

### Single Sample

```
For l = 1 to L:
    zâ½Ë¡â¾ = hâ½Ë¡â»Â¹â¾Wâ½Ë¡â¾ + bâ½Ë¡â¾
    hâ½Ë¡â¾ = Ïƒâ‚—(zâ½Ë¡â¾)

Output: Å· = hâ½á´¸â¾
```

### Batch of m Samples

```
For l = 1 to L:
    Zâ½Ë¡â¾ = Hâ½Ë¡â»Â¹â¾Wâ½Ë¡â¾ + bâ½Ë¡â¾    (m, nâ‚—)
    Hâ½Ë¡â¾ = Ïƒâ‚—(Zâ½Ë¡â¾)              (m, nâ‚—)

Output: Å¶ = Hâ½á´¸â¾
```

---

## ğŸ“‰ Loss Function

For regression (MSE):
```
L = (1/2m) Ã— ||Å¶ - Y||Â²
```

For classification (Cross-Entropy):
```
L = -(1/m) Ã— Î£áµ¢ Î£â±¼ yáµ¢â±¼ log(Å·áµ¢â±¼)
```

**General derivative with respect to output:**
```
âˆ‚L/âˆ‚Å¶ = f(Å¶, Y)
```

Examples:
- MSE: âˆ‚L/âˆ‚Å¶ = Å¶ - Y
- Cross-Entropy with Softmax: âˆ‚L/âˆ‚Å¶ = Å¶ - Y (simplified)

---

## ğŸ”„ Backpropagation: General Framework

### Key Principle: Chain Rule

For any layer l, we compute gradients by propagating the error backward:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  âˆ‚L/âˆ‚zâ½Ë¡â¾ = âˆ‚L/âˆ‚hâ½Ë¡â¾ âŠ™ Ïƒâ‚—'(zâ½Ë¡â¾)                          â”‚
â”‚                                                             â”‚
â”‚  where Ïƒâ‚—'(zâ½Ë¡â¾) is the derivative of activation function â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ General Gradient Formulas

### For Layer l (l = 1, 2, ..., L)

**Step 1: Gradient with respect to activation output**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  If l = L (output layer):                                  â”‚
â”‚      âˆ‚L/âˆ‚hâ½á´¸â¾ = âˆ‚L/âˆ‚Å¶                                     â”‚
â”‚                                                             â”‚
â”‚  If l < L (hidden layer):                                  â”‚
â”‚      âˆ‚L/âˆ‚hâ½Ë¡â¾ = (âˆ‚L/âˆ‚zâ½Ë¡âºÂ¹â¾) Ã— (Wâ½Ë¡âºÂ¹â¾)áµ€                  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Step 2: Gradient with respect to linear output (pre-activation)**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  âˆ‚L/âˆ‚zâ½Ë¡â¾ = (âˆ‚L/âˆ‚hâ½Ë¡â¾) âŠ™ Ïƒâ‚—'(zâ½Ë¡â¾)                        â”‚
â”‚                                                             â”‚
â”‚  where âŠ™ is element-wise multiplication                    â”‚
â”‚  and Ïƒâ‚—'(zâ½Ë¡â¾) is the activation derivative               â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Step 3: Gradient with respect to weights**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  Single sample:                                             â”‚
â”‚  âˆ‚L/âˆ‚Wâ½Ë¡â¾ = (hâ½Ë¡â»Â¹â¾)áµ€ Ã— (âˆ‚L/âˆ‚zâ½Ë¡â¾)                       â”‚
â”‚                                                             â”‚
â”‚  Batch (m samples):                                         â”‚
â”‚  âˆ‚L/âˆ‚Wâ½Ë¡â¾ = (1/m) Ã— (Hâ½Ë¡â»Â¹â¾)áµ€ Ã— (âˆ‚L/âˆ‚Zâ½Ë¡â¾)               â”‚
â”‚                                                             â”‚
â”‚  Shape: (nâ‚—â‚‹â‚, nâ‚—)                                         â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Step 4: Gradient with respect to bias**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  Single sample:                                             â”‚
â”‚  âˆ‚L/âˆ‚bâ½Ë¡â¾ = âˆ‚L/âˆ‚zâ½Ë¡â¾                                       â”‚
â”‚                                                             â”‚
â”‚  Batch (m samples):                                         â”‚
â”‚  âˆ‚L/âˆ‚bâ½Ë¡â¾ = (1/m) Ã— Î£(âˆ‚L/âˆ‚Zâ½Ë¡â¾)                           â”‚
â”‚                                                             â”‚
â”‚  Shape: (nâ‚—,)                                               â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Complete Backpropagation Algorithm

### Batch Training (m samples)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                               â•‘
â•‘  FORWARD PASS:                                                â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â•‘
â•‘  Hâ½â°â¾ = X                                                     â•‘
â•‘  For l = 1 to L:                                              â•‘
â•‘      Zâ½Ë¡â¾ = Hâ½Ë¡â»Â¹â¾Wâ½Ë¡â¾ + bâ½Ë¡â¾                                â•‘
â•‘      Hâ½Ë¡â¾ = Ïƒâ‚—(Zâ½Ë¡â¾)                                         â•‘
â•‘  Å¶ = Hâ½á´¸â¾                                                     â•‘
â•‘                                                               â•‘
â•‘  COMPUTE LOSS:                                                â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                               â•‘
â•‘  L = loss_function(Å¶, Y)                                     â•‘
â•‘                                                               â•‘
â•‘  BACKWARD PASS:                                               â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â•‘
â•‘  âˆ‚L/âˆ‚Hâ½á´¸â¾ = âˆ‚L/âˆ‚Å¶                                           â•‘
â•‘                                                               â•‘
â•‘  For l = L down to 1:                                         â•‘
â•‘      # Gradient through activation                           â•‘
â•‘      âˆ‚L/âˆ‚Zâ½Ë¡â¾ = (âˆ‚L/âˆ‚Hâ½Ë¡â¾) âŠ™ Ïƒâ‚—'(Zâ½Ë¡â¾)                      â•‘
â•‘                                                               â•‘
â•‘      # Weight and bias gradients                             â•‘
â•‘      âˆ‚L/âˆ‚Wâ½Ë¡â¾ = (1/m) Ã— (Hâ½Ë¡â»Â¹â¾)áµ€ Ã— (âˆ‚L/âˆ‚Zâ½Ë¡â¾)             â•‘
â•‘      âˆ‚L/âˆ‚bâ½Ë¡â¾ = (1/m) Ã— Î£(âˆ‚L/âˆ‚Zâ½Ë¡â¾)                         â•‘
â•‘                                                               â•‘
â•‘      # Propagate to previous layer (if not input layer)      â•‘
â•‘      If l > 1:                                                â•‘
â•‘          âˆ‚L/âˆ‚Hâ½Ë¡â»Â¹â¾ = (âˆ‚L/âˆ‚Zâ½Ë¡â¾) Ã— (Wâ½Ë¡â¾)áµ€                  â•‘
â•‘                                                               â•‘
â•‘  UPDATE PARAMETERS:                                           â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â•‘
â•‘  For l = 1 to L:                                              â•‘
â•‘      Wâ½Ë¡â¾ â† Wâ½Ë¡â¾ - Î± Ã— (âˆ‚L/âˆ‚Wâ½Ë¡â¾)                           â•‘
â•‘      bâ½Ë¡â¾ â† bâ½Ë¡â¾ - Î± Ã— (âˆ‚L/âˆ‚bâ½Ë¡â¾)                           â•‘
â•‘                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ”§ Activation Function Derivatives

### Common Activation Functions and Their Derivatives

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  1. SIGMOID: Ïƒ(z) = 1 / (1 + eâ»á¶»)                         â”‚
â”‚     Ïƒ'(z) = Ïƒ(z) Ã— (1 - Ïƒ(z))                             â”‚
â”‚     Ïƒ'(z) = h âŠ™ (1 - h)    [where h = Ïƒ(z)]              â”‚
â”‚                                                             â”‚
â”‚  2. TANH: Ïƒ(z) = tanh(z)                                   â”‚
â”‚     Ïƒ'(z) = 1 - tanhÂ²(z)                                   â”‚
â”‚     Ïƒ'(z) = 1 - hÂ²         [where h = tanh(z)]            â”‚
â”‚                                                             â”‚
â”‚  3. RELU: Ïƒ(z) = max(0, z)                                 â”‚
â”‚     Ïƒ'(z) = 1 if z > 0, else 0                            â”‚
â”‚     Ïƒ'(z) = (z > 0)        [indicator function]            â”‚
â”‚                                                             â”‚
â”‚  4. LEAKY RELU: Ïƒ(z) = max(Î±z, z)  [Î± = 0.01]            â”‚
â”‚     Ïƒ'(z) = 1 if z > 0, else Î±                            â”‚
â”‚                                                             â”‚
â”‚  5. ELU: Ïƒ(z) = z if z > 0, else Î±(eá¶» - 1)               â”‚
â”‚     Ïƒ'(z) = 1 if z > 0, else Ïƒ(z) + Î±                    â”‚
â”‚                                                             â”‚
â”‚  6. SOFTMAX (for output layer):                            â”‚
â”‚     Ïƒ(z)áµ¢ = eá¶»â± / Î£â±¼ eá¶»Ê²                                  â”‚
â”‚     With cross-entropy: âˆ‚L/âˆ‚z = Å· - y (simplified)        â”‚
â”‚                                                             â”‚
â”‚  7. LINEAR (no activation):                                â”‚
â”‚     Ïƒ(z) = z                                               â”‚
â”‚     Ïƒ'(z) = 1                                              â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’» Python/NumPy Implementation (General Framework)

```python
import numpy as np

class ActivationFunction:
    """Base class for activation functions"""
    def forward(self, z):
        raise NotImplementedError
    
    def derivative(self, z, h=None):
        """
        Compute derivative.
        z: pre-activation values
        h: post-activation values (optional, for efficiency)
        """
        raise NotImplementedError

class Sigmoid(ActivationFunction):
    def forward(self, z):
        return 1 / (1 + np.exp(-z))
    
    def derivative(self, z, h=None):
        if h is None:
            h = self.forward(z)
        return h * (1 - h)

class Tanh(ActivationFunction):
    def forward(self, z):
        return np.tanh(z)
    
    def derivative(self, z, h=None):
        if h is None:
            h = self.forward(z)
        return 1 - h**2

class ReLU(ActivationFunction):
    def forward(self, z):
        return np.maximum(0, z)
    
    def derivative(self, z, h=None):
        return (z > 0).astype(float)

class LeakyReLU(ActivationFunction):
    def __init__(self, alpha=0.01):
        self.alpha = alpha
    
    def forward(self, z):
        return np.where(z > 0, z, self.alpha * z)
    
    def derivative(self, z, h=None):
        return np.where(z > 0, 1.0, self.alpha)

class Linear(ActivationFunction):
    def forward(self, z):
        return z
    
    def derivative(self, z, h=None):
        return np.ones_like(z)


class NLayerNetwork:
    """
    General N-layer neural network with arbitrary activation functions.
    """
    def __init__(self, layer_sizes, activations, lr=0.01):
        """
        Args:
            layer_sizes: List of layer sizes [input_dim, hidden1, hidden2, ..., output_dim]
            activations: List of activation functions for each layer
            lr: Learning rate
        """
        self.layer_sizes = layer_sizes
        self.num_layers = len(layer_sizes) - 1  # Number of weight layers
        self.activations = activations
        self.lr = lr
        
        # Initialize weights and biases
        self.weights = []
        self.biases = []
        
        for l in range(self.num_layers):
            # Xavier/He initialization
            fan_in = layer_sizes[l]
            fan_out = layer_sizes[l + 1]
            
            # Use He initialization for ReLU-like, Xavier for others
            if isinstance(activations[l], (ReLU, LeakyReLU)):
                std = np.sqrt(2.0 / fan_in)
            else:
                std = np.sqrt(2.0 / (fan_in + fan_out))
            
            W = np.random.randn(fan_in, fan_out) * std
            b = np.zeros(fan_out)
            
            self.weights.append(W)
            self.biases.append(b)
        
        # Storage for forward pass (needed for backward pass)
        self.Z = []  # Pre-activation values
        self.H = []  # Post-activation values
    
    def forward(self, X):
        """
        Forward pass through all layers.
        X: (batch_size, input_dim)
        """
        self.Z = []
        self.H = [X]  # H[0] = input
        
        for l in range(self.num_layers):
            # Linear transformation
            Z = self.H[l] @ self.weights[l] + self.biases[l]
            self.Z.append(Z)
            
            # Activation
            H = self.activations[l].forward(Z)
            self.H.append(H)
        
        return self.H[-1]  # Return output
    
    def backward(self, X, Y_true, Y_pred):
        """
        Backward pass through all layers.
        X: (batch_size, input_dim)
        Y_true: (batch_size, output_dim)
        Y_pred: (batch_size, output_dim)
        """
        m = X.shape[0]  # Batch size
        
        # Initialize gradient storage
        dL_dW = [None] * self.num_layers
        dL_db = [None] * self.num_layers
        
        # Output layer gradient (assuming MSE loss)
        dL_dH = Y_pred - Y_true  # (batch, output_dim)
        
        # Backward pass through layers
        for l in range(self.num_layers - 1, -1, -1):
            # Gradient through activation
            activation_derivative = self.activations[l].derivative(
                self.Z[l], 
                self.H[l + 1]
            )
            dL_dZ = dL_dH * activation_derivative  # Element-wise
            
            # Weight gradient
            dL_dW[l] = (1/m) * (self.H[l].T @ dL_dZ)
            
            # Bias gradient
            dL_db[l] = (1/m) * np.sum(dL_dZ, axis=0)
            
            # Propagate to previous layer (if not input layer)
            if l > 0:
                dL_dH = dL_dZ @ self.weights[l].T
        
        # Update parameters
        for l in range(self.num_layers):
            self.weights[l] -= self.lr * dL_dW[l]
            self.biases[l] -= self.lr * dL_db[l]
    
    def compute_loss(self, Y_true, Y_pred):
        """MSE loss"""
        return 0.5 * np.mean((Y_pred - Y_true)**2)
    
    def train(self, X, Y, epochs=1000, verbose=True):
        """Training loop"""
        losses = []
        
        for epoch in range(epochs):
            # Forward pass
            Y_pred = self.forward(X)
            loss = self.compute_loss(Y, Y_pred)
            losses.append(loss)
            
            # Backward pass
            self.backward(X, Y, Y_pred)
            
            if verbose and (epoch + 1) % 100 == 0:
                print(f"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}")
        
        return losses


# Example usage
if __name__ == "__main__":
    # Generate synthetic data
    np.random.seed(42)
    X = np.random.randn(100, 2)  # 100 samples, 2 features
    Y = np.sum(X**2, axis=1, keepdims=True)  # Target: sum of squares
    
    # Define network architecture
    layer_sizes = [2, 10, 8, 5, 1]  # 4-layer network
    activations = [
        ReLU(),      # Layer 1: ReLU
        Tanh(),      # Layer 2: Tanh
        ReLU(),      # Layer 3: ReLU
        Linear()     # Layer 4 (output): Linear
    ]
    
    # Create and train network
    model = NLayerNetwork(
        layer_sizes=layer_sizes,
        activations=activations,
        lr=0.01
    )
    
    print("Training 4-layer network with mixed activations...")
    losses = model.train(X, Y, epochs=1000)
    
    # Test
    Y_pred = model.forward(X)
    final_loss = model.compute_loss(Y, Y_pred)
    print(f"\nFinal Loss: {final_loss:.4f}")
    
    # Example with different architecture
    print("\n" + "="*60)
    print("Training 2-layer network with Sigmoid...")
    
    model2 = NLayerNetwork(
        layer_sizes=[2, 15, 1],
        activations=[Sigmoid(), Linear()],
        lr=0.1
    )
    
    losses2 = model2.train(X, Y, epochs=1000)
    Y_pred2 = model2.forward(X)
    final_loss2 = model2.compute_loss(Y, Y_pred2)
    print(f"\nFinal Loss: {final_loss2:.4f}")
```

---

## ğŸ¯ Key Insights

### 1. **Modularity**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  The general framework separates:                          â”‚
â”‚  â€¢ Network structure (layer sizes)                         â”‚
â”‚  â€¢ Activation functions (pluggable)                        â”‚
â”‚  â€¢ Loss function (can be changed)                          â”‚
â”‚  â€¢ Optimization (gradient descent)                         â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. **Activation Function Abstraction**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  Each activation function provides:                        â”‚
â”‚  â€¢ forward(z): Compute activation                          â”‚
â”‚  â€¢ derivative(z, h): Compute derivative                    â”‚
â”‚                                                             â”‚
â”‚  This allows easy addition of new activations!             â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. **Gradient Flow Pattern**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  For each layer l (backward):                              â”‚
â”‚  1. âˆ‚L/âˆ‚Zâ½Ë¡â¾ = (âˆ‚L/âˆ‚Hâ½Ë¡â¾) âŠ™ Ïƒâ‚—'(Zâ½Ë¡â¾)                    â”‚
â”‚  2. âˆ‚L/âˆ‚Wâ½Ë¡â¾ = (Hâ½Ë¡â»Â¹â¾)áµ€ Ã— (âˆ‚L/âˆ‚Zâ½Ë¡â¾)                    â”‚
â”‚  3. âˆ‚L/âˆ‚bâ½Ë¡â¾ = Î£(âˆ‚L/âˆ‚Zâ½Ë¡â¾)                                â”‚
â”‚  4. âˆ‚L/âˆ‚Hâ½Ë¡â»Â¹â¾ = (âˆ‚L/âˆ‚Zâ½Ë¡â¾) Ã— (Wâ½Ë¡â¾)áµ€                     â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. **Computational Efficiency**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  â€¢ Store activations during forward pass                   â”‚
â”‚  â€¢ Reuse for derivative computation                        â”‚
â”‚  â€¢ Example: Ïƒ'(z) = h(1-h) uses h, not z                  â”‚
â”‚  â€¢ Saves computation in backward pass                      â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5. **Scalability**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  This framework scales to:                                 â”‚
â”‚  â€¢ Any number of layers (2, 10, 100, ...)                 â”‚
â”‚  â€¢ Any layer sizes (10, 1000, 10000, ...)                 â”‚
â”‚  â€¢ Any activation functions                                â”‚
â”‚  â€¢ Any loss function (with appropriate âˆ‚L/âˆ‚Å¶)            â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Dimension Tracking Template

For an L-layer network with batch size m:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                               â•‘
â•‘  FORWARD PASS DIMENSIONS:                                     â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                    â•‘
â•‘  Hâ½â°â¾ = X:           (m, nâ‚€)                                 â•‘
â•‘  Zâ½Â¹â¾ = Hâ½â°â¾Wâ½Â¹â¾:    (m, nâ‚)    Wâ½Â¹â¾: (nâ‚€, nâ‚)             â•‘
â•‘  Hâ½Â¹â¾ = Ïƒâ‚(Zâ½Â¹â¾):    (m, nâ‚)                                 â•‘
â•‘  Zâ½Â²â¾ = Hâ½Â¹â¾Wâ½Â²â¾:    (m, nâ‚‚)    Wâ½Â²â¾: (nâ‚, nâ‚‚)             â•‘
â•‘  Hâ½Â²â¾ = Ïƒâ‚‚(Zâ½Â²â¾):    (m, nâ‚‚)                                 â•‘
â•‘  ...                                                          â•‘
â•‘  Zâ½á´¸â¾ = Hâ½á´¸â»Â¹â¾Wâ½á´¸â¾:  (m, nâ‚—)    Wâ½á´¸â¾: (nâ‚—â‚‹â‚, nâ‚—)           â•‘
â•‘  Å¶ = Hâ½á´¸â¾:           (m, nâ‚—)                                 â•‘
â•‘                                                               â•‘
â•‘  BACKWARD PASS DIMENSIONS:                                    â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â•‘
â•‘  âˆ‚L/âˆ‚Hâ½á´¸â¾:           (m, nâ‚—)                                 â•‘
â•‘  âˆ‚L/âˆ‚Zâ½á´¸â¾:           (m, nâ‚—)                                 â•‘
â•‘  âˆ‚L/âˆ‚Wâ½á´¸â¾:           (nâ‚—â‚‹â‚, nâ‚—)                              â•‘
â•‘  âˆ‚L/âˆ‚bâ½á´¸â¾:           (nâ‚—,)                                   â•‘
â•‘  âˆ‚L/âˆ‚Hâ½á´¸â»Â¹â¾:         (m, nâ‚—â‚‹â‚)                               â•‘
â•‘  ...                                                          â•‘
â•‘  âˆ‚L/âˆ‚Zâ½Â¹â¾:           (m, nâ‚)                                 â•‘
â•‘  âˆ‚L/âˆ‚Wâ½Â¹â¾:           (nâ‚€, nâ‚)                                â•‘
â•‘  âˆ‚L/âˆ‚bâ½Â¹â¾:           (nâ‚,)                                   â•‘
â•‘                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸŒŸ Summary

### Universal Backpropagation Formula:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                               â•‘
â•‘  For any layer l in an L-layer network:                      â•‘
â•‘                                                               â•‘
â•‘  1. FORWARD:                                                  â•‘
â•‘     Zâ½Ë¡â¾ = Hâ½Ë¡â»Â¹â¾Wâ½Ë¡â¾ + bâ½Ë¡â¾                                 â•‘
â•‘     Hâ½Ë¡â¾ = Ïƒâ‚—(Zâ½Ë¡â¾)                                          â•‘
â•‘                                                               â•‘
â•‘  2. BACKWARD:                                                 â•‘
â•‘     âˆ‚L/âˆ‚Zâ½Ë¡â¾ = (âˆ‚L/âˆ‚Hâ½Ë¡â¾) âŠ™ Ïƒâ‚—'(Zâ½Ë¡â¾)                       â•‘
â•‘     âˆ‚L/âˆ‚Wâ½Ë¡â¾ = (1/m) Ã— (Hâ½Ë¡â»Â¹â¾)áµ€ Ã— (âˆ‚L/âˆ‚Zâ½Ë¡â¾)              â•‘
â•‘     âˆ‚L/âˆ‚bâ½Ë¡â¾ = (1/m) Ã— Î£(âˆ‚L/âˆ‚Zâ½Ë¡â¾)                          â•‘
â•‘     âˆ‚L/âˆ‚Hâ½Ë¡â»Â¹â¾ = (âˆ‚L/âˆ‚Zâ½Ë¡â¾) Ã— (Wâ½Ë¡â¾)áµ€                       â•‘
â•‘                                                               â•‘
â•‘  3. UPDATE:                                                   â•‘
â•‘     Wâ½Ë¡â¾ â† Wâ½Ë¡â¾ - Î± Ã— (âˆ‚L/âˆ‚Wâ½Ë¡â¾)                            â•‘
â•‘     bâ½Ë¡â¾ â† bâ½Ë¡â¾ - Î± Ã— (âˆ‚L/âˆ‚bâ½Ë¡â¾)                            â•‘
â•‘                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Key Advantages:

1. **Works for any number of layers**
2. **Works with any activation function** (just provide Ïƒ and Ïƒ')
3. **Works with any loss function** (just provide âˆ‚L/âˆ‚Å¶)
4. **Efficient** (same complexity as forward pass)
5. **Modular** (easy to extend and modify)

---

**This is the universal framework for deep learning!** ğŸ‰

All modern deep learning frameworks (PyTorch, TensorFlow, JAX) implement this general pattern with automatic differentiation. Understanding this framework is key to understanding how neural networks learn! ğŸš€
