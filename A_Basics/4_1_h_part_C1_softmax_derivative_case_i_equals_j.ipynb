{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C1: Softmax Derivative (Case i=j)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**StatQuest Style**: \"Time to calculate derivatives... step-by-step!\"\n",
    "\n",
    "### What We'll Cover\n",
    "\n",
    "1. Why we need the derivative\n",
    "2. The quotient rule\n",
    "3. Case i=j derivation\n",
    "4. Beautiful result: S_i(1 - S_i)\n",
    "\n",
    "**Let's derive!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Ready for calculus!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('âœ“ Ready for calculus!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Derivative Formula\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         SOFTMAX DERIVATIVE (CASE i=j)                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Softmax: S_i = e^(x_i) / Î£ e^(x_j)                        â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Want: âˆ‚S_i/âˆ‚x_i (derivative w.r.t. its own input)         â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Use Quotient Rule: (f/g)' = (f'g - fg')/gÂ²                â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  f = e^(x_i)     â†’  f' = e^(x_i)                           â”‚\n",
    "â”‚  g = Î£ e^(x_j)   â†’  g' = e^(x_i)                           â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  âˆ‚S_i/âˆ‚x_i = [e^(x_i)Â·Î£e^(x_j) - e^(x_i)Â·e^(x_i)] / (Î£e^(x_j))Â²  â”‚\n",
    "â”‚            = e^(x_i)/Î£e^(x_j) Â· [Î£e^(x_j) - e^(x_i)] / Î£e^(x_j)  â”‚\n",
    "â”‚            = S_i Â· (1 - S_i)                                â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Result: âˆ‚S_i/âˆ‚x_i = S_i(1 - S_i)                          â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOFTMAX DERIVATIVE (i=j)\n",
      "============================================================\n",
      "Probabilities: [0.65900114 0.24243297 0.09856589]\n",
      "\n",
      "Derivatives (âˆ‚S_i/âˆ‚x_i):\n",
      "  Class 0: 0.6590 Ã— (1 - 0.6590) = 0.2247\n",
      "  Class 1: 0.2424 Ã— (1 - 0.2424) = 0.1837\n",
      "  Class 2: 0.0986 Ã— (1 - 0.0986) = 0.0889\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "def softmax_derivative_i_equals_j(S_i):\n",
    "    \"\"\"Derivative when i=j: S_i(1 - S_i)\"\"\"\n",
    "    return S_i * (1 - S_i)\n",
    "\n",
    "# Example\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "probs = softmax(logits)\n",
    "\n",
    "print('SOFTMAX DERIVATIVE (i=j)')\n",
    "print('='*60)\n",
    "print(f'Probabilities: {probs}')\n",
    "print('\\nDerivatives (âˆ‚S_i/âˆ‚x_i):')\n",
    "for i, p in enumerate(probs):\n",
    "    deriv = softmax_derivative_i_equals_j(p)\n",
    "    print(f'  Class {i}: {p:.4f} Ã— (1 - {p:.4f}) = {deriv:.4f}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "**Key Result**: âˆ‚S_i/âˆ‚x_i = S_i(1 - S_i)\n",
    "\n",
    "This is the derivative when we differentiate a softmax output with respect to its own input.\n",
    "\n",
    "**Next**: Part C2 covers case iâ‰ j: âˆ‚S_i/âˆ‚x_j = -S_i Ã— S_j\n",
    "\n",
    "**Double Bam!** ðŸ’¥ðŸ’¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Softmax Jacobian Matrix\n",
    "\n",
    "## What is a Jacobian?\n",
    "\n",
    "**Simple Definition:** A Jacobian is a matrix of **all partial derivatives** when you have:\n",
    "- **Multiple inputs** (a vector)\n",
    "- **Multiple outputs** (another vector)\n",
    "\n",
    "It shows how **each output** changes with respect to **each input**.\n",
    "\n",
    "---\n",
    "\n",
    "## Softmax: A Quick Recap\n",
    "\n",
    "### Softmax Function:\n",
    "```\n",
    "Given logits: z = [zâ‚, zâ‚‚, zâ‚ƒ, ..., zâ‚™]\n",
    "\n",
    "Softmax output: p = [pâ‚, pâ‚‚, pâ‚ƒ, ..., pâ‚™]\n",
    "\n",
    "Where: páµ¢ = e^(záµ¢) / Î£â±¼ e^(zâ±¼)\n",
    "```\n",
    "\n",
    "### Key Property:\n",
    "**All probabilities sum to 1:** Î£áµ¢ páµ¢ = 1\n",
    "\n",
    "This creates **interdependence** - changing one logit affects ALL probabilities!\n",
    "\n",
    "---\n",
    "\n",
    "## The Jacobian Matrix: What Does It Represent?\n",
    "\n",
    "### Structure:\n",
    "```\n",
    "         âˆ‚pâ‚/âˆ‚zâ‚   âˆ‚pâ‚/âˆ‚zâ‚‚   âˆ‚pâ‚/âˆ‚zâ‚ƒ   ...   âˆ‚pâ‚/âˆ‚zâ‚™\n",
    "         âˆ‚pâ‚‚/âˆ‚zâ‚   âˆ‚pâ‚‚/âˆ‚zâ‚‚   âˆ‚pâ‚‚/âˆ‚zâ‚ƒ   ...   âˆ‚pâ‚‚/âˆ‚zâ‚™\n",
    "J =      âˆ‚pâ‚ƒ/âˆ‚zâ‚   âˆ‚pâ‚ƒ/âˆ‚zâ‚‚   âˆ‚pâ‚ƒ/âˆ‚zâ‚ƒ   ...   âˆ‚pâ‚ƒ/âˆ‚zâ‚™\n",
    "         ...       ...       ...       ...   ...\n",
    "         âˆ‚pâ‚™/âˆ‚zâ‚   âˆ‚pâ‚™/âˆ‚zâ‚‚   âˆ‚pâ‚™/âˆ‚zâ‚ƒ   ...   âˆ‚pâ‚™/âˆ‚zâ‚™\n",
    "```\n",
    "\n",
    "### Reading the Matrix:\n",
    "- **Row i:** How probability páµ¢ changes when we change each logit\n",
    "- **Column j:** How all probabilities change when we change logit zâ±¼\n",
    "- **Element (i,j):** How probability páµ¢ changes when we change logit zâ±¼\n",
    "\n",
    "---\n",
    "\n",
    "## Two Cases: Diagonal vs Off-Diagonal\n",
    "\n",
    "### Case 1: Diagonal Elements (i = j)\n",
    "**Question:** How does páµ¢ change when we change its own logit záµ¢?\n",
    "\n",
    "```\n",
    "âˆ‚páµ¢/âˆ‚záµ¢ = páµ¢(1 - páµ¢)\n",
    "```\n",
    "\n",
    "**Intuition:**\n",
    "- Increasing záµ¢ **increases** páµ¢ (positive derivative)\n",
    "- But it's **dampened** by (1 - páµ¢) because probabilities must sum to 1\n",
    "- If páµ¢ is already large (close to 1), increasing záµ¢ has less effect\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "If páµ¢ = 0.7:\n",
    "âˆ‚páµ¢/âˆ‚záµ¢ = 0.7 Ã— (1 - 0.7) = 0.7 Ã— 0.3 = 0.21\n",
    "\n",
    "If páµ¢ = 0.1:\n",
    "âˆ‚páµ¢/âˆ‚záµ¢ = 0.1 Ã— (1 - 0.1) = 0.1 Ã— 0.9 = 0.09\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Case 2: Off-Diagonal Elements (i â‰  j)\n",
    "**Question:** How does páµ¢ change when we change a *different* logit zâ±¼?\n",
    "\n",
    "```\n",
    "âˆ‚páµ¢/âˆ‚zâ±¼ = -páµ¢ Ã— pâ±¼\n",
    "```\n",
    "\n",
    "**Intuition:**\n",
    "- Increasing zâ±¼ **decreases** páµ¢ (negative derivative)\n",
    "- This is the \"competition\" effect - making one probability larger forces others smaller\n",
    "- The effect is proportional to both páµ¢ and pâ±¼\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "If páµ¢ = 0.3 and pâ±¼ = 0.5:\n",
    "âˆ‚páµ¢/âˆ‚zâ±¼ = -0.3 Ã— 0.5 = -0.15\n",
    "\n",
    "Meaning: Increasing zâ±¼ by 1 decreases páµ¢ by approximately 0.15\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Visual Example: 3-Class Softmax\n",
    "\n",
    "### Given:\n",
    "```\n",
    "Logits: z = [2.0, 1.0, 0.5]\n",
    "Probabilities: p = [0.659, 0.242, 0.099]\n",
    "```\n",
    "\n",
    "### The Jacobian Matrix:\n",
    "\n",
    "```\n",
    "         âˆ‚pâ‚/âˆ‚zâ‚        âˆ‚pâ‚/âˆ‚zâ‚‚        âˆ‚pâ‚/âˆ‚zâ‚ƒ\n",
    "J =      âˆ‚pâ‚‚/âˆ‚zâ‚        âˆ‚pâ‚‚/âˆ‚zâ‚‚        âˆ‚pâ‚‚/âˆ‚zâ‚ƒ\n",
    "         âˆ‚pâ‚ƒ/âˆ‚zâ‚        âˆ‚pâ‚ƒ/âˆ‚zâ‚‚        âˆ‚pâ‚ƒ/âˆ‚zâ‚ƒ\n",
    "```\n",
    "\n",
    "### Computing Each Element:\n",
    "\n",
    "**Diagonal (i = j):**\n",
    "```\n",
    "J[0,0] = pâ‚(1-pâ‚) = 0.659 Ã— (1-0.659) = 0.659 Ã— 0.341 = 0.225\n",
    "J[1,1] = pâ‚‚(1-pâ‚‚) = 0.242 Ã— (1-0.242) = 0.242 Ã— 0.758 = 0.183\n",
    "J[2,2] = pâ‚ƒ(1-pâ‚ƒ) = 0.099 Ã— (1-0.099) = 0.099 Ã— 0.901 = 0.089\n",
    "```\n",
    "\n",
    "**Off-Diagonal (i â‰  j):**\n",
    "```\n",
    "J[0,1] = -pâ‚Ã—pâ‚‚ = -0.659 Ã— 0.242 = -0.159\n",
    "J[0,2] = -pâ‚Ã—pâ‚ƒ = -0.659 Ã— 0.099 = -0.065\n",
    "J[1,0] = -pâ‚‚Ã—pâ‚ = -0.242 Ã— 0.659 = -0.159\n",
    "J[1,2] = -pâ‚‚Ã—pâ‚ƒ = -0.242 Ã— 0.099 = -0.024\n",
    "J[2,0] = -pâ‚ƒÃ—pâ‚ = -0.099 Ã— 0.659 = -0.065\n",
    "J[2,1] = -pâ‚ƒÃ—pâ‚‚ = -0.099 Ã— 0.242 = -0.024\n",
    "```\n",
    "\n",
    "### Final Jacobian:\n",
    "```\n",
    "       zâ‚       zâ‚‚       zâ‚ƒ\n",
    "pâ‚  [ 0.225  -0.159  -0.065]\n",
    "pâ‚‚  [-0.159   0.183  -0.024]\n",
    "pâ‚ƒ  [-0.065  -0.024   0.089]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Interpreting the Jacobian\n",
    "\n",
    "### Row 1 (How pâ‚ changes):\n",
    "```\n",
    "[ 0.225  -0.159  -0.065]\n",
    "```\n",
    "- Increasing zâ‚ by 1 â†’ pâ‚ increases by ~0.225 âœ“\n",
    "- Increasing zâ‚‚ by 1 â†’ pâ‚ decreases by ~0.159 âœ—\n",
    "- Increasing zâ‚ƒ by 1 â†’ pâ‚ decreases by ~0.065 âœ—\n",
    "\n",
    "### Column 1 (Effect of changing zâ‚):\n",
    "```\n",
    "[ 0.225]\n",
    "[-0.159]\n",
    "[-0.065]\n",
    "```\n",
    "- Increasing zâ‚ by 1:\n",
    "  - pâ‚ increases by ~0.225 âœ“\n",
    "  - pâ‚‚ decreases by ~0.159 âœ—\n",
    "  - pâ‚ƒ decreases by ~0.065 âœ—\n",
    "\n",
    "**Notice:** The changes sum to approximately 0 (because probabilities must sum to 1)!\n",
    "\n",
    "---\n",
    "\n",
    "## Key Properties of Softmax Jacobian\n",
    "\n",
    "### Property 1: Diagonal Elements are Positive\n",
    "```\n",
    "âˆ‚páµ¢/âˆ‚záµ¢ = páµ¢(1 - páµ¢) > 0  (always positive)\n",
    "```\n",
    "**Meaning:** Increasing a logit always increases its own probability.\n",
    "\n",
    "### Property 2: Off-Diagonal Elements are Negative\n",
    "```\n",
    "âˆ‚páµ¢/âˆ‚zâ±¼ = -páµ¢ Ã— pâ±¼ < 0  (always negative, when iâ‰ j)\n",
    "```\n",
    "**Meaning:** Increasing one logit always decreases other probabilities.\n",
    "\n",
    "### Property 3: Rows Sum to Zero\n",
    "```\n",
    "Î£â±¼ âˆ‚páµ¢/âˆ‚zâ±¼ = 0\n",
    "```\n",
    "**Meaning:** The total change in páµ¢ across all logit changes is zero (conservation of probability).\n",
    "\n",
    "### Property 4: Columns Sum to Zero\n",
    "```\n",
    "Î£áµ¢ âˆ‚páµ¢/âˆ‚zâ±¼ = 0\n",
    "```\n",
    "**Meaning:** When you change one logit, the total probability change across all outputs is zero.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Do We Need the Jacobian?\n",
    "\n",
    "### In Backpropagation:\n",
    "\n",
    "When computing gradients through softmax, we need to know:\n",
    "> \"How does the loss change with respect to the logits?\"\n",
    "\n",
    "Using the chain rule:\n",
    "```\n",
    "âˆ‚Loss/âˆ‚zâ±¼ = Î£áµ¢ (âˆ‚Loss/âˆ‚páµ¢) Ã— (âˆ‚páµ¢/âˆ‚zâ±¼)\n",
    "            â†‘                  â†‘\n",
    "      From next layer    From Jacobian\n",
    "```\n",
    "\n",
    "The Jacobian tells us how to propagate gradients from probability space back to logit space.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Example: Gradient Flow\n",
    "\n",
    "### Setup:\n",
    "```\n",
    "Logits: z = [2.0, 1.0, 0.5]\n",
    "Probabilities: p = [0.659, 0.242, 0.099]\n",
    "True label: class 0 (one-hot: [1, 0, 0])\n",
    "\n",
    "Loss gradient w.r.t. probabilities:\n",
    "âˆ‚Loss/âˆ‚p = [pâ‚-1, pâ‚‚-0, pâ‚ƒ-0] = [-0.341, 0.242, 0.099]\n",
    "```\n",
    "\n",
    "### Computing âˆ‚Loss/âˆ‚zâ‚:\n",
    "```\n",
    "âˆ‚Loss/âˆ‚zâ‚ = (âˆ‚Loss/âˆ‚pâ‚)Ã—(âˆ‚pâ‚/âˆ‚zâ‚) + (âˆ‚Loss/âˆ‚pâ‚‚)Ã—(âˆ‚pâ‚‚/âˆ‚zâ‚) + (âˆ‚Loss/âˆ‚pâ‚ƒ)Ã—(âˆ‚pâ‚ƒ/âˆ‚zâ‚)\n",
    "          = (-0.341)Ã—(0.225) + (0.242)Ã—(-0.159) + (0.099)Ã—(-0.065)\n",
    "          = -0.077 - 0.038 - 0.006\n",
    "          = -0.121\n",
    "```\n",
    "\n",
    "This tells us: **Decrease zâ‚ by 0.121** to reduce the loss.\n",
    "\n",
    "---\n",
    "\n",
    "## Code Breakdown\n",
    "\n",
    "```python\n",
    "def softmax_jacobian(probs):\n",
    "    \"\"\"Complete Jacobian matrix\"\"\"\n",
    "    n = len(probs)\n",
    "    jacobian = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):           # For each output probability\n",
    "        for j in range(n):       # For each input logit\n",
    "            if i == j:\n",
    "                # Diagonal: how páµ¢ changes w.r.t. its own logit záµ¢\n",
    "                jacobian[i, j] = probs[i] * (1 - probs[i])\n",
    "            else:\n",
    "                # Off-diagonal: how páµ¢ changes w.r.t. other logit zâ±¼\n",
    "                jacobian[i, j] = -probs[i] * probs[j]\n",
    "    \n",
    "    return jacobian\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Visualization: Heatmap Interpretation\n",
    "\n",
    "When you visualize the Jacobian as a heatmap:\n",
    "\n",
    "- **Bright diagonal:** Strong positive derivatives (self-reinforcement)\n",
    "- **Dark off-diagonal:** Negative derivatives (competition)\n",
    "- **Symmetry:** J[i,j] and J[j,i] have the same magnitude (but different meaning!)\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### The Softmax Jacobian Matrix:\n",
    "\n",
    "| Element | Formula | Meaning | Sign |\n",
    "|---------|---------|---------|------|\n",
    "| **Diagonal** (i=j) | páµ¢(1-páµ¢) | How páµ¢ responds to its own logit záµ¢ | Positive âœ“ |\n",
    "| **Off-diagonal** (iâ‰ j) | -páµ¢Ã—pâ±¼ | How páµ¢ responds to other logit zâ±¼ | Negative âœ— |\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Softmax creates interdependence** - changing one logit affects ALL probabilities\n",
    "2. **Diagonal elements** show self-reinforcement (positive feedback)\n",
    "3. **Off-diagonal elements** show competition (negative feedback)\n",
    "4. **Conservation property** - rows and columns sum to zero\n",
    "5. **Essential for backpropagation** - converts probability gradients to logit gradients\n",
    "\n",
    "### The Big Picture:\n",
    "\n",
    "> The Jacobian captures the **complete sensitivity** of the softmax function.\n",
    "> It's the mathematical description of how the \"probability distribution\" responds\n",
    "> to changes in the \"raw scores\" (logits).\n",
    "\n",
    "ðŸŽ¯ **In neural networks, this is how we backpropagate through the final classification layer!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
