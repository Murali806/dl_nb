---

## ğŸ“Š Complete Gradient Summary

For a three-layer network with **n neurons per layer**:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                               â•‘
â•‘  OUTPUT LAYER (Layer 3):                                     â•‘
â•‘  âˆ‚L/âˆ‚Wâ‚ƒ = hâ‚‚áµ€ Ã— (Å· - y)          Shape: (nâ‚‚, k)            â•‘
â•‘  âˆ‚L/âˆ‚bâ‚ƒ = Å· - y                   Shape: (k,)               â•‘
â•‘                                                               â•‘
â•‘  HIDDEN LAYER 2 (with tanh):                                 â•‘
â•‘  âˆ‚L/âˆ‚hâ‚‚ = (Å· - y) Ã— Wâ‚ƒáµ€          Shape: (nâ‚‚,)              â•‘
â•‘  âˆ‚L/âˆ‚zâ‚‚ = (âˆ‚L/âˆ‚hâ‚‚) âŠ™ (1 - hâ‚‚Â²)   Shape: (nâ‚‚,)              â•‘
â•‘  âˆ‚L/âˆ‚Wâ‚‚ = hâ‚áµ€ Ã— (âˆ‚L/âˆ‚zâ‚‚)         Shape: (nâ‚, nâ‚‚)           â•‘
â•‘  âˆ‚L/âˆ‚bâ‚‚ = âˆ‚L/âˆ‚zâ‚‚                  Shape: (nâ‚‚,)              â•‘
â•‘                                                               â•‘
â•‘  HIDDEN LAYER 1 (with tanh):                                 â•‘
â•‘  âˆ‚L/âˆ‚hâ‚ = (âˆ‚L/âˆ‚zâ‚‚) Ã— Wâ‚‚áµ€         Shape: (nâ‚,)              â•‘
â•‘  âˆ‚L/âˆ‚zâ‚ = (âˆ‚L/âˆ‚hâ‚) âŠ™ (1 - hâ‚Â²)   Shape: (nâ‚,)              â•‘
â•‘  âˆ‚L/âˆ‚Wâ‚ = xáµ€ Ã— (âˆ‚L/âˆ‚zâ‚)          Shape: (d, nâ‚)            â•‘
â•‘  âˆ‚L/âˆ‚bâ‚ = âˆ‚L/âˆ‚zâ‚                  Shape: (nâ‚,)              â•‘
â•‘                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Key Pattern:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  For each layer:                                            â”‚
â”‚                                                             â”‚
â”‚  1. Propagate gradient backward: âˆ‚L/âˆ‚h = (âˆ‚L/âˆ‚z_next) Ã— Wáµ€ â”‚
â”‚  2. Apply activation derivative: âˆ‚L/âˆ‚z = (âˆ‚L/âˆ‚h) âŠ™ (1-hÂ²) â”‚
â”‚  3. Compute weight gradient:     âˆ‚L/âˆ‚W = h_preváµ€ Ã— (âˆ‚L/âˆ‚z) â”‚
â”‚  4. Compute bias gradient:       âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚z            â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Batch Training (Multiple Samples)

For a batch of m samples: X âˆˆ â„áµË£áµˆ

### Forward Pass:
```
Zâ‚ = XWâ‚ + bâ‚        (m, nâ‚)
Hâ‚ = tanh(Zâ‚)        (m, nâ‚)

Zâ‚‚ = Hâ‚Wâ‚‚ + bâ‚‚       (m, nâ‚‚)
Hâ‚‚ = tanh(Zâ‚‚)        (m, nâ‚‚)

Zâ‚ƒ = Hâ‚‚Wâ‚ƒ + bâ‚ƒ       (m, k)
Å¶ = Zâ‚ƒ               (m, k)
```

### Backward Pass:
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                               â•‘
â•‘  âˆ‚L/âˆ‚Zâ‚ƒ = Å¶ - Y                    Shape: (m, k)              â•‘
â•‘  âˆ‚L/âˆ‚Wâ‚ƒ = (1/m) Ã— Hâ‚‚áµ€ Ã— (âˆ‚L/âˆ‚Zâ‚ƒ)   Shape: (nâ‚‚, k)             â•‘
â•‘  âˆ‚L/âˆ‚bâ‚ƒ = (1/m) Ã— Î£(âˆ‚L/âˆ‚Zâ‚ƒ)        Shape: (k,)                â•‘
â•‘                                                               â•‘
â•‘  âˆ‚L/âˆ‚Hâ‚‚ = (âˆ‚L/âˆ‚Zâ‚ƒ) Ã— Wâ‚ƒáµ€           Shape: (m, nâ‚‚)             â•‘
â•‘  âˆ‚L/âˆ‚Zâ‚‚ = (âˆ‚L/âˆ‚Hâ‚‚) âŠ™ (1 - Hâ‚‚Â²)    Shape: (m, nâ‚‚)             â•‘
â•‘  âˆ‚L/âˆ‚Wâ‚‚ = (1/m) Ã— Hâ‚áµ€ Ã— (âˆ‚L/âˆ‚Zâ‚‚)   Shape: (nâ‚, nâ‚‚)            â•‘
â•‘  âˆ‚L/âˆ‚bâ‚‚ = (1/m) Ã— Î£(âˆ‚L/âˆ‚Zâ‚‚)        Shape: (nâ‚‚,)               â•‘
â•‘                                                               â•‘
â•‘  âˆ‚L/âˆ‚Hâ‚ = (âˆ‚L/âˆ‚Zâ‚‚) Ã— Wâ‚‚áµ€           Shape: (m, nâ‚)             â•‘
â•‘  âˆ‚L/âˆ‚Zâ‚ = (âˆ‚L/âˆ‚Hâ‚) âŠ™ (1 - Hâ‚Â²)    Shape: (m, nâ‚)             â•‘
â•‘  âˆ‚L/âˆ‚Wâ‚ = (1/m) Ã— Xáµ€ Ã— (âˆ‚L/âˆ‚Zâ‚)    Shape: (d, nâ‚)             â•‘
â•‘  âˆ‚L/âˆ‚bâ‚ = (1/m) Ã— Î£(âˆ‚L/âˆ‚Zâ‚)        Shape: (nâ‚,)               â•‘
â•‘                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Note:** Î£ means sum over the batch dimension (axis=0)

---

## ğŸ’» Python/NumPy Implementation

```python
import numpy as np

class ThreeLayerNetwork:
    """
    Three-layer neural network with n neurons per layer.
    """
    def __init__(self, input_dim, hidden1_dim, hidden2_dim, output_dim, lr=0.01):
        self.lr = lr
        
        # Xavier initialization
        self.W1 = np.random.randn(input_dim, hidden1_dim) * np.sqrt(2.0 / input_dim)
        self.b1 = np.zeros(hidden1_dim)
        
        self.W2 = np.random.randn(hidden1_dim, hidden2_dim) * np.sqrt(2.0 / hidden1_dim)
        self.b2 = np.zeros(hidden2_dim)
        
        self.W3 = np.random.randn(hidden2_dim, output_dim) * np.sqrt(2.0 / hidden2_dim)
        self.b3 = np.zeros(output_dim)
    
    def tanh(self, x):
        return np.tanh(x)
    
    def tanh_derivative(self, h):
        """Derivative: 1 - tanhÂ²(x) = 1 - hÂ²"""
        return 1 - h**2
    
    def forward(self, X):
        """
        Forward pass.
        X: (batch_size, input_dim)
        """
        # Layer 1
        self.Z1 = X @ self.W1 + self.b1  # (batch, hidden1)
        self.H1 = self.tanh(self.Z1)
        
        # Layer 2
        self.Z2 = self.H1 @ self.W2 + self.b2  # (batch, hidden2)
        self.H2 = self.tanh(self.Z2)
        
        # Layer 3
        self.Z3 = self.H2 @ self.W3 + self.b3  # (batch, output)
        self.Y_pred = self.Z3
        
        return self.Y_pred
    
    def backward(self, X, Y_true):
        """
        Backward pass and parameter update.
        X: (batch_size, input_dim)
        Y_true: (batch_size, output_dim)
        """
        m = X.shape[0]  # batch size
        
        # Output layer gradients
        dL_dZ3 = self.Y_pred - Y_true  # (batch, output)
        dL_dW3 = (1/m) * (self.H2.T @ dL_dZ3)  # (hidden2, output)
        dL_db3 = (1/m) * np.sum(dL_dZ3, axis=0)  # (output,)
        
        # Layer 2 gradients
        dL_dH2 = dL_dZ3 @ self.W3.T  # (batch, hidden2)
        dL_dZ2 = dL_dH2 * self.tanh_derivative(self.H2)  # (batch, hidden2)
        dL_dW2 = (1/m) * (self.H1.T @ dL_dZ2)  # (hidden1, hidden2)
        dL_db2 = (1/m) * np.sum(dL_dZ2, axis=0)  # (hidden2,)
        
        # Layer 1 gradients
        dL_dH1 = dL_dZ2 @ self.W2.T  # (batch, hidden1)
        dL_dZ1 = dL_dH1 * self.tanh_derivative(self.H1)  # (batch, hidden1)
        dL_dW1 = (1/m) * (X.T @ dL_dZ1)  # (input, hidden1)
        dL_db1 = (1/m) * np.sum(dL_dZ1, axis=0)  # (hidden1,)
        
        # Update parameters
        self.W3 -= self.lr * dL_dW3
        self.b3 -= self.lr * dL_db3
        self.W2 -= self.lr * dL_dW2
        self.b2 -= self.lr * dL_db2
        self.W1 -= self.lr * dL_dW1
        self.b1 -= self.lr * dL_db1
    
    def compute_loss(self, Y_true, Y_pred):
        """MSE loss"""
        return 0.5 * np.mean((Y_pred - Y_true)**2)
    
    def train(self, X, Y, epochs=1000):
        """Training loop"""
        losses = []
        
        for epoch in range(epochs):
            # Forward pass
            Y_pred = self.forward(X)
            loss = self.compute_loss(Y, Y_pred)
            losses.append(loss)
            
            # Backward pass
            self.backward(X, Y)
            
            if (epoch + 1) % 100 == 0:
                print(f"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}")
        
        return losses

# Example usage
if __name__ == "__main__":
    # Generate synthetic data
    np.random.seed(42)
    X = np.random.randn(100, 2)  # 100 samples, 2 features
    Y = np.sum(X**2, axis=1, keepdims=True)  # Target: sum of squares
    
    # Create and train network
    model = ThreeLayerNetwork(
        input_dim=2,
        hidden1_dim=10,
        hidden2_dim=5,
        output_dim=1,
        lr=0.01
    )
    
    losses = model.train(X, Y, epochs=1000)
    
    # Test
    Y_pred = model.forward(X)
    final_loss = model.compute_loss(Y, Y_pred)
    print(f"\nFinal Loss: {final_loss:.4f}")
```

---
