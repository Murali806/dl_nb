{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Layer Neural Network: Drug Dosage Response (Inverted U-Shape)\n",
    "\n",
    "## ðŸ“‹ Overview\n",
    "\n",
    "This notebook demonstrates how a **2-layer neural network** (with 1 neuron in each hidden layer) can learn the **optimal drug dosage** relationship - an inverted U-shaped curve where effectiveness peaks at an optimal dose.\n",
    "\n",
    "### Real-World Context\n",
    "\n",
    "In pharmacology, drug effectiveness follows a **dose-response curve**:\n",
    "- **Too little**: Ineffective (underdose)\n",
    "- **Optimal dose**: Maximum therapeutic effect\n",
    "- **Too much**: Reduced effectiveness or toxicity (overdose)\n",
    "\n",
    "### Why 2 Hidden Layers?\n",
    "\n",
    "- **Single perceptron limitation**: Can only learn linear relationships\n",
    "- **Non-linear activation**: Hidden layers enable learning the bell-shaped curve\n",
    "- **Inverted parabola**: Requires non-linear transformation to capture peak effectiveness\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Input (dosage) â†’ Hidden Layer 1 (1 neuron + tanh) â†’ Hidden Layer 2 (1 neuron + tanh) â†’ Output (effectiveness)\n",
    "```\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "1. Understand dose-response relationships in medicine\n",
    "2. Implement manual backpropagation through 2 hidden layers\n",
    "3. Visualize therapeutic window and safety margins\n",
    "4. Identify optimal dosage from neural network predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# TensorFlow for TensorBoard logging\n",
    "import tensorflow as tf\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 2000\n",
    "DOSAGE_RANGE = (0, 100)  # Dosage in mg (0 to 100)\n",
    "NUM_SAMPLES = 200\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "# Network architecture\n",
    "HIDDEN1_SIZE = 1  # First hidden layer: 1 neuron\n",
    "HIDDEN2_SIZE = 1  # Second hidden layer: 1 neuron\n",
    "\n",
    "# Drug dosage parameters\n",
    "OPTIMAL_DOSAGE = 50  # mg (peak effectiveness)\n",
    "MAX_EFFECTIVENESS = 100  # % (at optimal dose)\n",
    "NOISE_STD = 5  # Standard deviation of noise in effectiveness\n",
    "\n",
    "print(\"ðŸ“Š Hyperparameters:\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Dosage Range: {DOSAGE_RANGE} mg\")\n",
    "print(f\"  Training Samples: {int(NUM_SAMPLES * (1 - TEST_SPLIT))}\")\n",
    "print(f\"  Test Samples: {int(NUM_SAMPLES * TEST_SPLIT)}\")\n",
    "print(f\"  Architecture: 1 â†’ {HIDDEN1_SIZE} â†’ {HIDDEN2_SIZE} â†’ 1\")\n",
    "print(f\"\\nðŸ’Š Drug Parameters:\")\n",
    "print(f\"  Optimal Dosage: {OPTIMAL_DOSAGE} mg\")\n",
    "print(f\"  Max Effectiveness: {MAX_EFFECTIVENESS}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Generation\n",
    "\n",
    "Generate synthetic drug dosage-response data following an **inverted parabola**:\n",
    "\n",
    "**Formula**: Effectiveness = MAX - (dosage - OPTIMAL)Â² / scale_factor\n",
    "\n",
    "This creates a bell curve where:\n",
    "- Effectiveness peaks at optimal dosage (50mg)\n",
    "- Effectiveness decreases on both sides (underdose and overdose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_drug_dosage_data(num_samples, dosage_range, optimal_dose, max_effectiveness, noise_std=5):\n",
    "    \"\"\"\n",
    "    Generate drug dosage-response data (inverted U-shape)\n",
    "    \n",
    "    Args:\n",
    "        num_samples: Number of data points\n",
    "        dosage_range: Tuple (min, max) for dosage values in mg\n",
    "        optimal_dose: Dosage at peak effectiveness\n",
    "        max_effectiveness: Maximum effectiveness percentage\n",
    "        noise_std: Standard deviation of Gaussian noise\n",
    "    \n",
    "    Returns:\n",
    "        dosage: Input values (dosage in mg)\n",
    "        effectiveness: Output values (effectiveness %)\n",
    "    \"\"\"\n",
    "    # Generate evenly spaced dosage values\n",
    "    dosage = np.linspace(dosage_range[0], dosage_range[1], num_samples)\n",
    "    \n",
    "    # True relationship: Inverted parabola\n",
    "    # Effectiveness = MAX - (dosage - optimal)Â² / scale_factor\n",
    "    # Scale factor chosen so effectiveness reaches ~0 at boundaries\n",
    "    scale_factor = (optimal_dose ** 2) / max_effectiveness\n",
    "    effectiveness_true = max_effectiveness - ((dosage - optimal_dose) ** 2) / scale_factor\n",
    "    \n",
    "    # Ensure effectiveness doesn't go below 0\n",
    "    effectiveness_true = np.maximum(effectiveness_true, 0)\n",
    "    \n",
    "    # Add Gaussian noise\n",
    "    noise = np.random.normal(0, noise_std, num_samples)\n",
    "    effectiveness = effectiveness_true + noise\n",
    "    \n",
    "    # Clip to valid range [0, 100]\n",
    "    effectiveness = np.clip(effectiveness, 0, 100)\n",
    "    \n",
    "    return dosage.reshape(-1, 1), effectiveness.reshape(-1, 1)\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_drug_dosage_data(NUM_SAMPLES, DOSAGE_RANGE, OPTIMAL_DOSAGE, MAX_EFFECTIVENESS, NOISE_STD)\n",
    "\n",
    "# Split into train and test sets\n",
    "split_idx = int(NUM_SAMPLES * (1 - TEST_SPLIT))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"âœ… Data generated successfully!\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"  Dosage range: [{X.min():.2f}, {X.max():.2f}] mg\")\n",
    "print(f\"  Effectiveness range: [{y.min():.2f}, {y.max():.2f}]%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_train, y_train, alpha=0.6, label='Training Data', s=50)\n",
    "plt.scatter(X_test, y_test, alpha=0.6, label='Test Data', s=50, marker='s')\n",
    "\n",
    "# Plot true dose-response curve\n",
    "dosage_smooth = np.linspace(DOSAGE_RANGE[0], DOSAGE_RANGE[1], 1000)\n",
    "scale_factor = (OPTIMAL_DOSAGE ** 2) / MAX_EFFECTIVENESS\n",
    "effectiveness_smooth = MAX_EFFECTIVENESS - ((dosage_smooth - OPTIMAL_DOSAGE) ** 2) / scale_factor\n",
    "effectiveness_smooth = np.maximum(effectiveness_smooth, 0)\n",
    "plt.plot(dosage_smooth, effectiveness_smooth, 'r--', linewidth=2, \n",
    "         label='True Dose-Response Curve', alpha=0.7)\n",
    "\n",
    "# Mark optimal dosage\n",
    "plt.axvline(x=OPTIMAL_DOSAGE, color='green', linestyle=':', linewidth=2, \n",
    "            label=f'Optimal Dosage ({OPTIMAL_DOSAGE} mg)', alpha=0.7)\n",
    "\n",
    "# Mark therapeutic zones\n",
    "plt.axvspan(0, 25, alpha=0.1, color='red', label='Underdose Zone')\n",
    "plt.axvspan(75, 100, alpha=0.1, color='orange', label='Overdose Zone')\n",
    "plt.axvspan(25, 75, alpha=0.1, color='green', label='Therapeutic Window')\n",
    "\n",
    "plt.xlabel('Dosage (mg)', fontsize=12)\n",
    "plt.ylabel('Effectiveness (%)', fontsize=12)\n",
    "plt.title('Drug Dosage-Response Data (Inverted U-Shape)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=9, loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('drug_dosage_data.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Data visualization saved as 'drug_dosage_data.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Normalization\n",
    "\n",
    "Normalize data for stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate normalization parameters from training data\n",
    "X_mean, X_std = X_train.mean(), X_train.std()\n",
    "y_mean, y_std = y_train.mean(), y_train.std()\n",
    "\n",
    "# Normalize\n",
    "X_train_norm = (X_train - X_mean) / X_std\n",
    "X_test_norm = (X_test - X_mean) / X_std\n",
    "y_train_norm = (y_train - y_mean) / y_std\n",
    "y_test_norm = (y_test - y_mean) / y_std\n",
    "\n",
    "print(\"âœ… Data normalized successfully!\")\n",
    "print(f\"  Dosage: mean={X_mean:.3f} mg, std={X_std:.3f} mg\")\n",
    "print(f\"  Effectiveness: mean={y_mean:.3f}%, std={y_std:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neural Network Implementation\n",
    "\n",
    "### Architecture:\n",
    "```\n",
    "Input (1) â†’ Hidden1 (1 + tanh) â†’ Hidden2 (1 + tanh) â†’ Output (1)\n",
    "```\n",
    "\n",
    "### Forward Propagation:\n",
    "1. **Layer 1**: h1 = tanh(W1 Â· dosage + b1)\n",
    "2. **Layer 2**: h2 = tanh(W2 Â· h1 + b2)\n",
    "3. **Output**: effectiveness = W3 Â· h2 + b3\n",
    "\n",
    "### Activation Function:\n",
    "We use **tanh** (hyperbolic tangent) which outputs values in range [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNetwork:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize 2-layer neural network with 1 neuron in each hidden layer\n",
    "        \"\"\"\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # Initialize weights using Xavier initialization\n",
    "        # Layer 1: Input (1) â†’ Hidden1 (1)\n",
    "        self.W1 = np.random.randn(1, 1) * np.sqrt(2.0 / 1)\n",
    "        self.b1 = np.zeros((1, 1))\n",
    "        \n",
    "        # Layer 2: Hidden1 (1) â†’ Hidden2 (1)\n",
    "        self.W2 = np.random.randn(1, 1) * np.sqrt(2.0 / 1)\n",
    "        self.b2 = np.zeros((1, 1))\n",
    "        \n",
    "        # Layer 3: Hidden2 (1) â†’ Output (1)\n",
    "        self.W3 = np.random.randn(1, 1) * np.sqrt(2.0 / 1)\n",
    "        self.b3 = np.zeros((1, 1))\n",
    "        \n",
    "        # Store activations for backpropagation\n",
    "        self.cache = {}\n",
    "        \n",
    "    def tanh(self, x):\n",
    "        \"\"\"Hyperbolic tangent activation\"\"\"\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def tanh_derivative(self, x):\n",
    "        \"\"\"Derivative of tanh: 1 - tanhÂ²(x)\"\"\"\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through the network\n",
    "        \n",
    "        Args:\n",
    "            X: Input data (n_samples, 1)\n",
    "        \n",
    "        Returns:\n",
    "            y_pred: Predictions (n_samples, 1)\n",
    "        \"\"\"\n",
    "        # Layer 1: Input â†’ Hidden1\n",
    "        self.cache['z1'] = X @ self.W1 + self.b1  # Linear transformation\n",
    "        self.cache['h1'] = self.tanh(self.cache['z1'])  # Activation\n",
    "        \n",
    "        # Layer 2: Hidden1 â†’ Hidden2\n",
    "        self.cache['z2'] = self.cache['h1'] @ self.W2 + self.b2\n",
    "        self.cache['h2'] = self.tanh(self.cache['z2'])\n",
    "        \n",
    "        # Layer 3: Hidden2 â†’ Output (no activation)\n",
    "        self.cache['z3'] = self.cache['h2'] @ self.W3 + self.b3\n",
    "        y_pred = self.cache['z3']\n",
    "        \n",
    "        self.cache['X'] = X\n",
    "        return y_pred\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute Mean Squared Error loss\n",
    "        \"\"\"\n",
    "        n = y_true.shape[0]\n",
    "        loss = np.mean((y_pred - y_true) ** 2)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Backpropagation through the network\n",
    "        \n",
    "        Chain rule application:\n",
    "        dL/dW3 = dL/dy_pred Â· dy_pred/dz3 Â· dz3/dW3\n",
    "        dL/dW2 = dL/dy_pred Â· dy_pred/dz3 Â· dz3/dh2 Â· dh2/dz2 Â· dz2/dW2\n",
    "        dL/dW1 = dL/dy_pred Â· dy_pred/dz3 Â· dz3/dh2 Â· dh2/dz2 Â· dz2/dh1 Â· dh1/dz1 Â· dz1/dW1\n",
    "        \"\"\"\n",
    "        n = y_true.shape[0]\n",
    "        \n",
    "        # Output layer gradient\n",
    "        # dL/dy_pred = 2(y_pred - y_true) / n\n",
    "        dL_dy_pred = 2 * (y_pred - y_true) / n\n",
    "        \n",
    "        # Layer 3 gradients (Output layer)\n",
    "        # dy_pred/dz3 = 1 (no activation)\n",
    "        dL_dz3 = dL_dy_pred\n",
    "        dL_dW3 = self.cache['h2'].T @ dL_dz3\n",
    "        dL_db3 = np.sum(dL_dz3, axis=0, keepdims=True)\n",
    "        \n",
    "        # Layer 2 gradients (Hidden layer 2)\n",
    "        # dz3/dh2 = W3\n",
    "        dL_dh2 = dL_dz3 @ self.W3.T\n",
    "        # dh2/dz2 = tanh'(z2)\n",
    "        dL_dz2 = dL_dh2 * self.tanh_derivative(self.cache['z2'])\n",
    "        dL_dW2 = self.cache['h1'].T @ dL_dz2\n",
    "        dL_db2 = np.sum(dL_dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Layer 1 gradients (Hidden layer 1)\n",
    "        # dz2/dh1 = W2\n",
    "        dL_dh1 = dL_dz2 @ self.W2.T\n",
    "        # dh1/dz1 = tanh'(z1)\n",
    "        dL_dz1 = dL_dh1 * self.tanh_derivative(self.cache['z1'])\n",
    "        dL_dW1 = self.cache['X'].T @ dL_dz1\n",
    "        dL_db1 = np.sum(dL_dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        # Store gradients\n",
    "        self.gradients = {\n",
    "            'dW1': dL_dW1, 'db1': dL_db1,\n",
    "            'dW2': dL_dW2, 'db2': dL_db2,\n",
    "            'dW3': dL_dW3, 'db3': dL_db3\n",
    "        }\n",
    "        \n",
    "        return self.gradients\n",
    "    \n",
    "    def update_parameters(self):\n",
    "        \"\"\"\n",
    "        Update parameters using gradient descent\n",
    "        \"\"\"\n",
    "        self.W1 -= self.lr * self.gradients['dW1']\n",
    "        self.b1 -= self.lr * self.gradients['db1']\n",
    "        self.W2 -= self.lr * self.gradients['dW2']\n",
    "        self.b2 -= self.lr * self.gradients['db2']\n",
    "        self.W3 -= self.lr * self.gradients['dW3']\n",
    "        self.b3 -= self.lr * self.gradients['db3']\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        \"\"\"Return current parameters\"\"\"\n",
    "        return {\n",
    "            'W1': self.W1.copy(), 'b1': self.b1.copy(),\n",
    "            'W2': self.W2.copy(), 'b2': self.b2.copy(),\n",
    "            'W3': self.W3.copy(), 'b3': self.b3.copy()\n",
    "        }\n",
    "\n",
    "# Initialize network\n",
    "model = TwoLayerNetwork(learning_rate=LEARNING_RATE)\n",
    "\n",
    "print(\"âœ… Neural network initialized!\")\n",
    "print(f\"  Architecture: 1 â†’ {HIDDEN1_SIZE} â†’ {HIDDEN2_SIZE} â†’ 1\")\n",
    "print(f\"  Activation: tanh\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(\"\\nðŸ“Š Initial Parameters:\")\n",
    "params = model.get_parameters()\n",
    "for name, value in params.items():\n",
    "    print(f\"  {name}: {value.flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup TensorBoard and MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log directories\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = f\"logs/tensorboard/drug_dosage_{timestamp}\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# MLflow setup\n",
    "mlflow.set_experiment(\"Drug_Dosage_Response\")\n",
    "mlflow.start_run(run_name=f\"2layer_drug_dosage_{timestamp}\")\n",
    "\n",
    "# Log hyperparameters\n",
    "mlflow.log_params({\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"hidden1_size\": HIDDEN1_SIZE,\n",
    "    \"hidden2_size\": HIDDEN2_SIZE,\n",
    "    \"activation\": \"tanh\",\n",
    "    \"num_samples\": NUM_SAMPLES,\n",
    "    \"optimal_dosage\": OPTIMAL_DOSAGE,\n",
    "    \"noise_std\": NOISE_STD\n",
    "})\n",
    "\n",
    "print(\"âœ… TensorBoard and MLflow configured!\")\n",
    "print(f\"  TensorBoard logs: {log_dir}\")\n",
    "print(f\"  MLflow experiment: Drug_Dosage_Response\")\n",
    "print(\"\\nðŸ’¡ To view TensorBoard: tensorboard --logdir=logs/tensorboard\")\n",
    "print(\"ðŸ’¡ To view MLflow: mlflow ui\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'test_loss': [],\n",
    "    'W1': [], 'b1': [],\n",
    "    'W2': [], 'b2': [],\n",
    "    'W3': [], 'b3': [],\n",
    "    'grad_W1': [], 'grad_W2': [], 'grad_W3': []\n",
    "}\n",
    "\n",
    "print(\"ðŸš€ Starting training...\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Forward pass\n",
    "    y_pred_train = model.forward(X_train_norm)\n",
    "    train_loss = model.compute_loss(y_train_norm, y_pred_train)\n",
    "    \n",
    "    # Backward pass\n",
    "    gradients = model.backward(y_train_norm, y_pred_train)\n",
    "    \n",
    "    # Update parameters\n",
    "    model.update_parameters()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred_test = model.forward(X_test_norm)\n",
    "    test_loss = model.compute_loss(y_test_norm, y_pred_test)\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    \n",
    "    params = model.get_parameters()\n",
    "    history['W1'].append(params['W1'][0, 0])\n",
    "    history['b1'].append(params['b1'][0, 0])\n",
    "    history['W2'].append(params['W2'][0, 0])\n",
    "    history['b2'].append(params['b2'][0, 0])\n",
    "    history['W3'].append(params['W3'][0, 0])\n",
    "    history['b3'].append(params['b3'][0, 0])\n",
    "    \n",
    "    history['grad_W1'].append(np.abs(gradients['dW1'][0, 0]))\n",
    "    history['grad_W2'].append(np.abs(gradients['dW2'][0, 0]))\n",
    "    history['grad_W3'].append(np.abs(gradients['dW3'][0, 0]))\n",
    "    \n",
    "    # Log to TensorBoard\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar('Loss/train', train_loss, step=epoch)\n",
    "        tf.summary.scalar('Loss/test', test_loss, step=epoch)\n",
    "        tf.summary.scalar('Parameters/W1', params['W1'][0, 0], step=epoch)\n",
    "        tf.summary.scalar('Parameters/b1', params['b1'][0, 0], step=epoch)\n",
    "        tf.summary.scalar('Parameters/W2', params['W2'][0, 0], step=epoch)\n",
    "        tf.summary.scalar('Parameters/b2', params['b2'][0, 0], step=epoch)\n",
    "        tf.summary.scalar('Parameters/W3', params['W3'][0, 0], step=epoch)\n",
    "        tf.summary.scalar('Parameters/b3', params['b3'][0, 0], step=epoch)\n",
    "        tf.summary.scalar('Gradients/W1', np.abs(gradients['dW1'][0, 0]), step=epoch)\n",
    "        tf.summary.scalar('Gradients/W2', np.abs(gradients['dW2'][0, 0]), step=epoch)\n",
    "        tf.summary.scalar('Gradients/W3', np.abs(gradients['dW3'][0, 0]), step=epoch)\n",
    "    \n",
    "    # Log to MLflow (every 100 epochs)\n",
    "    if epoch % 100 == 0:\n",
    "        mlflow.log_metrics({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss\n",
    "        }, step=epoch)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 200 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:4d}/{EPOCHS} | Train Loss: {train_loss:.6f} | Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "print(\"\\nâœ… Training completed!\")\n",
    "print(f\"  Final train loss: {history['train_loss'][-1]:.6f}\")\n",
    "print(f\"  Final test loss: {history['test_loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "plt.plot(history['test_loss'], label='Test Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Training and Test Loss', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Log scale\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "plt.plot(history['test_loss'], label='Test Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Training and Test Loss (Log Scale)', fontsize=14, fontweight='bold')\n",
    "plt.yscale('log')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('drug_dosage_loss_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Loss curve saved as 'drug_dosage_loss_curve.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Parameter Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Layer 1 parameters\n",
    "axes[0, 0].plot(history['W1'], linewidth=2, color='blue')\n",
    "axes[0, 0].set_title('Weight 1 (W1)', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Value')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(history['b1'], linewidth=2, color='blue')\n",
    "axes[1, 0].set_title('Bias 1 (b1)', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Value')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Layer 2 parameters\n",
    "axes[0, 1].plot(history['W2'], linewidth=2, color='green')\n",
    "axes[0, 1].set_title('Weight 2 (W2)', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Value')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(history['b2'], linewidth=2, color='green')\n",
    "axes[1, 1].set_title('Bias 2 (b2)', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Value')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Output layer parameters\n",
    "axes[0, 2].plot(history['W3'], linewidth=2, color='red')\n",
    "axes[0, 2].set_title('Weight 3 (W3)', fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Epoch')\n",
    "axes[0, 2].set_ylabel('Value')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 2].plot(history['b3'], linewidth=2, color='red')\n",
    "axes[1, 2].set_title('Bias 3 (b3)', fontweight='bold')\n",
    "axes[1, 2].set_xlabel('Epoch')\n",
    "axes[1, 2].set_ylabel('Value')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Parameter Evolution During Training', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('drug_dosage_parameter_evolution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Parameter evolution saved as 'drug_dosage_parameter_evolution.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Gradient Magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['grad_W1'], label='|âˆ‚L/âˆ‚W1|', linewidth=2)\n",
    "plt.plot(history['grad_W2'], label='|âˆ‚L/âˆ‚W2|', linewidth=2)\n",
    "plt.plot(history['grad_W3'], label='|âˆ‚L/âˆ‚W3|', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Gradient Magnitude', fontsize=12)\n",
    "plt.title('Gradient Magnitudes', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['grad_W1'], label='|âˆ‚L/âˆ‚W1|', linewidth=2)\n",
    "plt.plot(history['grad_W2'], label='|âˆ‚L/âˆ‚W2|', linewidth=2)\n",
    "plt.plot(history['grad_W3'], label='|âˆ‚L/âˆ‚W3|', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Gradient Magnitude', fontsize=12)\n",
    "plt.title('Gradient Magnitudes (Log Scale)', fontsize=14, fontweight='bold')\n",
    "plt.yscale('log')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('drug_dosage_gradients.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Gradient magnitudes saved as 'drug_dosage_gradients.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Final Predictions vs True Dose-Response Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for smooth curve\n",
    "dosage_plot = np.linspace(DOSAGE_RANGE[0], DOSAGE_RANGE[1], 1000).reshape(-1, 1)\n",
    "dosage_plot_norm = (dosage_plot - X_mean) / X_std\n",
    "effectiveness_pred_norm = model.forward(dosage_plot_norm)\n",
    "effectiveness_pred = effectiveness_pred_norm * y_std + y_mean\n",
    "\n",
    "# True dose-response curve\n",
    "scale_factor = (OPTIMAL_DOSAGE ** 2) / MAX_EFFECTIVENESS\n",
    "effectiveness_true = MAX_EFFECTIVENESS - ((dosage_plot - OPTIMAL_DOSAGE) ** 2) / scale_factor\n",
    "effectiveness_true = np.maximum(effectiveness_true, 0)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot data points\n",
    "plt.scatter(X_train, y_train, alpha=0.5, s=50, label='Training Data', color='blue')\n",
    "plt.scatter(X_test, y_test, alpha=0.5, s=50, label='Test Data', color='orange', marker='s')\n",
    "\n",
    "# Plot true dose-response curve\n",
    "plt.plot(dosage_plot, effectiveness_true, 'r--', linewidth=3, \n",
    "         label='True Dose-Response Curve', alpha=0.7)\n",
    "\n",
    "# Plot predicted curve\n",
    "plt.plot(dosage_plot, effectiveness_pred, 'g-', linewidth=3, \n",
    "         label='Neural Network Prediction', alpha=0.8)\n",
    "\n",
    "# Mark optimal dosage\n",
    "plt.axvline(x=OPTIMAL_DOSAGE, color='purple', linestyle=':', linewidth=2, \n",
    "            label=f'True Optimal ({OPTIMAL_DOSAGE} mg)', alpha=0.7)\n",
    "\n",
    "# Find predicted optimal dosage\n",
    "pred_optimal_idx = np.argmax(effectiveness_pred)\n",
    "pred_optimal_dosage = dosage_plot[pred_optimal_idx, 0]\n",
    "plt.axvline(x=pred_optimal_dosage, color='green', linestyle=':', linewidth=2, \n",
    "            label=f'Predicted Optimal ({pred_optimal_dosage:.1f} mg)', alpha=0.7)\n",
    "\n",
    "# Mark therapeutic zones\n",
    "plt.axvspan(0, 25, alpha=0.1, color='red')\n",
    "plt.axvspan(75, 100, alpha=0.1, color='orange')\n",
    "plt.axvspan(25, 75, alpha=0.1, color='green')\n",
    "\n",
    "plt.xlabel('Dosage (mg)', fontsize=12)\n",
    "plt.ylabel('Effectiveness (%)', fontsize=12)\n",
    "plt.title('Drug Dosage-Response: Neural Network vs True Curve', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10, loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('drug_dosage_final_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Final predictions saved as 'drug_dosage_final_predictions.png'\")\n",
    "print(f\"\\nðŸ’Š Optimal Dosage Comparison:\")\n",
    "print(f\"  True Optimal: {OPTIMAL_DOSAGE} mg\")\n",
    "print(f\"  Predicted Optimal: {pred_optimal_dosage:.2f} mg\")\n",
    "print(f\"  Error: {abs(pred_optimal_dosage - OPTIMAL_DOSAGE):.2f} mg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Predictions on test set (denormalized)\n",
    "y_pred_test_norm = model.forward(X_test_norm)\n",
    "y_pred_test = y_pred_test_norm * y_std + y_mean\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred_test)\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "# Mean Absolute Percentage Error\n",
    "mape = np.mean(np.abs((y_test - y_pred_test) / (y_test + 1e-8))) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸ“Š PERFORMANCE METRICS (Test Set)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Mean Squared Error (MSE):  {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error:   {rmse:.4f}%\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}%\")\n",
    "print(f\"RÂ² Score:                  {r2:.4f}\")\n",
    "print(f\"MAPE:                      {mape:.2f}%\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Log to MLflow\n",
    "mlflow.log_metrics({\n",
    "    \"final_mse\": mse,\n",
    "    \"final_rmse\": rmse,\n",
    "    \"final_mae\": mae,\n",
    "    \"final_r2\": r2,\n",
    "    \"final_mape\": mape,\n",
    "    \"predicted_optimal_dosage\": pred_optimal_dosage,\n",
    "    \"optimal_dosage_error\": abs(pred_optimal_dosage - OPTIMAL_DOSAGE)\n",
    "})\n",
    "\n",
    "# Log final parameters\n",
    "final_params = model.get_parameters()\n",
    "print(\"\\nðŸ“Š Final Network Parameters:\")\n",
    "for name, value in final_params.items():\n",
    "    print(f\"  {name}: {value.flatten()[0]:.6f}\")\n",
    "    mlflow.log_param(f\"final_{name}\", value.flatten()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Distribution and Therapeutic Window Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = y_test - y_pred_test\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Error histogram\n",
    "axes[0].hist(errors, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Prediction Error (%)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Error Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].axvline(x=0, color='r', linestyle='--', linewidth=2, label='Zero Error')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual plot\n",
    "axes[1].scatter(y_pred_test, errors, alpha=0.6, s=50)\n",
    "axes[1].set_xlabel('Predicted Effectiveness (%)', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals (%)', fontsize=12)\n",
    "axes[1].set_title('Residual Plot', fontsize=14, fontweight='bold')\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Therapeutic window analysis\n",
    "dosage_zones = ['Underdose\\n(0-25mg)', 'Therapeutic\\n(25-75mg)', 'Overdose\\n(75-100mg)']\n",
    "zone_colors = ['red', 'green', 'orange']\n",
    "\n",
    "# Calculate average effectiveness in each zone\n",
    "underdose_mask = (dosage_plot >= 0) & (dosage_plot <= 25)\n",
    "therapeutic_mask = (dosage_plot > 25) & (dosage_plot <= 75)\n",
    "overdose_mask = (dosage_plot > 75) & (dosage_plot <= 100)\n",
    "\n",
    "avg_effectiveness = [\n",
    "    effectiveness_pred[underdose_mask].mean(),\n",
    "    effectiveness_pred[therapeutic_mask].mean(),\n",
    "    effectiveness_pred[overdose_mask].mean()\n",
    "]\n",
    "\n",
    "bars = axes[2].bar(dosage_zones, avg_effectiveness, color=zone_colors, alpha=0.7, edgecolor='black')\n",
    "axes[2].set_ylabel('Average Effectiveness (%)', fontsize=12)\n",
    "axes[2].set_title('Effectiveness by Dosage Zone', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('drug_dosage_error_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Error analysis saved as 'drug_dosage_error_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close TensorBoard writer\n",
    "writer.close()\n",
    "\n",
    "# Log artifacts to MLflow\n",
    "mlflow.log_artifact('drug_dosage_data.png')\n",
    "mlflow.log_artifact('drug_dosage_loss_curve.png')\n",
    "mlflow.log_artifact('drug_dosage_parameter_evolution.png')\n",
    "mlflow.log_artifact('drug_dosage_gradients.png')\n",
    "mlflow.log_artifact('drug_dosage_final_predictions.png')\n",
    "mlflow.log_artifact('drug_dosage_error_analysis.png')\n",
    "\n",
    "# End MLflow run\n",
    "mlflow.end_run()\n",
    "\n",
    "print(\"\\nâœ… All artifacts saved and logged!\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸŽ‰ TRAINING COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nðŸ“Š View results:\")\n",
    "print(\"  â€¢ TensorBoard: tensorboard --logdir=logs/tensorboard\")\n",
    "print(\"  â€¢ MLflow UI: mlflow ui\")\n",
    "print(\"  â€¢ Generated plots: Check current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Dose-Response Relationship**: Neural networks can learn complex inverted U-shaped curves representing optimal dosage\n",
    "\n",
    "2. **Hidden Layer Role**: \n",
    "   - Layer 1 captures initial non-linear transformation\n",
    "   - Layer 2 refines the curve to match the bell shape\n",
    "   - Together they create the inverted parabola\n",
    "\n",
    "3. **Medical Application**: \n",
    "   - Identified optimal dosage from data\n",
    "   - Visualized therapeutic window\n",
    "   - Understood underdose and overdose risks\n",
    "\n",
    "4. **Backpropagation**: Successfully propagated gradients through 2 layers to learn complex pattern\n",
    "\n",
    "5. **Training Dynamics**: \n",
    "   - Network learned to peak at optimal dosage\n",
    "   - Parameters converged to represent bell curve\n",
    "   - Gradients guided learning toward optimal solution\n",
    "\n",
    "### Real-World Implications:\n",
    "\n",
    "- **Personalized Medicine**: Could adapt to individual patient responses\n",
    "- **Drug Development**: Helps identify optimal dosing ranges\n",
    "- **Safety**: Visualizes risk zones (underdose/overdose)\n",
    "- **Clinical Decisions**: Supports evidence-based dosing\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "- Simplified model (1 neuron per layer)\n",
    "- Assumes symmetric response curve\n",
    "- Doesn't account for individual patient variability\n",
    "- Real pharmacokinetics are more complex\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Add more neurons for asymmetric dose-response curves\n",
    "- Include patient features (age, weight, metabolism)\n",
    "- Model time-dependent effects (pharmacokinetics)\n",
    "- Incorporate safety constraints and side effects"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
