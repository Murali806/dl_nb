{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM: Long Short-Term Memory Networks\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In previous notebooks, we saw that vanilla RNNs can learn temporal patterns. However, they have a critical limitation: **the vanishing gradient problem**. This makes it difficult for RNNs to learn long-term dependencies.\n",
    "\n",
    "**LSTM (Long Short-Term Memory)** networks, introduced by Hochreiter & Schmidhuber in 1997, solve this problem through a clever gating mechanism.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "1. Understand the vanishing gradient problem in vanilla RNNs\n",
    "2. Learn LSTM architecture and gating mechanisms\n",
    "3. Implement LSTM from scratch\n",
    "4. Compare LSTM vs vanilla RNN performance\n",
    "5. Visualize what LSTM gates learn\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              VANILLA RNN vs LSTM                           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Vanilla RNN:                                              â”‚\n",
    "â”‚    â€¢ Simple, elegant                                       â”‚\n",
    "â”‚    â€¢ Fast to train                                         â”‚\n",
    "â”‚    â€¢ âŒ Vanishing gradient problem                         â”‚\n",
    "â”‚    â€¢ âŒ Can't learn long-term dependencies                 â”‚\n",
    "â”‚                                                            â”‚\n",
    "â”‚  LSTM:                                                     â”‚\n",
    "â”‚    â€¢ More complex (4 gates)                                â”‚\n",
    "â”‚    â€¢ Slower to train                                       â”‚\n",
    "â”‚    â€¢ âœ“ Solves vanishing gradient                           â”‚\n",
    "â”‚    â€¢ âœ“ Learns long-term dependencies                       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Vanishing Gradient Problem\n",
    "\n",
    "Before diving into LSTM, let's understand **why** we need it.\n",
    "\n",
    "### 1.1 Problem Visualization\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         GRADIENT FLOW IN VANILLA RNN                       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Forward Pass (Information Flow):\n",
    "t=0 â†’ t=1 â†’ t=2 â†’ t=3 â†’ t=4 â†’ t=5 â†’ t=6 â†’ t=7 â†’ t=8 â†’ t=9\n",
    "â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚\n",
    "hâ‚€ â†’ hâ‚ â†’ hâ‚‚ â†’ hâ‚ƒ â†’ hâ‚„ â†’ hâ‚… â†’ hâ‚† â†’ hâ‚‡ â†’ hâ‚ˆ â†’ hâ‚‰ â†’ Loss\n",
    "\n",
    "Backward Pass (Gradient Flow):\n",
    "t=9 â† t=8 â† t=7 â† t=6 â† t=5 â† t=4 â† t=3 â† t=2 â† t=1 â† t=0\n",
    "â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚\n",
    "â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆ    â–ˆ     â–     â–     â–     â–     â–     â–\n",
    "100%  75%   50%   25%   10%   5%    2%    1%    0.5%  0.1%\n",
    "\n",
    "Problem: Gradient magnitude decays exponentially!\n",
    "         Early time steps receive almost no gradient signal.\n",
    "```\n",
    "\n",
    "### 1.2 Mathematical Explanation\n",
    "\n",
    "In vanilla RNN, the gradient flows through repeated matrix multiplications:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial h_0} = \\frac{\\partial L}{\\partial h_T} \\cdot \\prod_{t=1}^{T} \\frac{\\partial h_t}{\\partial h_{t-1}}$$\n",
    "\n",
    "Each term $\\frac{\\partial h_t}{\\partial h_{t-1}}$ involves:\n",
    "- Matrix multiplication by $W_{hh}$\n",
    "- Derivative of activation function (tanh)\n",
    "\n",
    "**Key insight:**\n",
    "- If $|W_{hh}| < 1$ and $|\\text{tanh}'| < 1$, the product shrinks exponentially\n",
    "- After T steps: gradient $\\approx (0.7)^T \\cdot \\text{original gradient}$\n",
    "- For T=10: gradient is only 2.8% of original\n",
    "- For T=20: gradient is only 0.08% of original\n",
    "\n",
    "**Consequence:** The network can't learn long-term dependencies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate gradient decay\n",
    "def simulate_gradient_decay(T, decay_rate=0.7):\n",
    "    \"\"\"Simulate how gradient decays over time steps.\"\"\"\n",
    "    gradients = []\n",
    "    gradient = 1.0  # Start with gradient of 1.0\n",
    "    \n",
    "    for t in range(T):\n",
    "        gradients.append(gradient)\n",
    "        gradient *= decay_rate  # Multiply by decay rate\n",
    "    \n",
    "    return np.array(gradients[::-1])  # Reverse (backward pass)\n",
    "\n",
    "# Simulate for different sequence lengths\n",
    "T_values = [10, 20, 30]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, T in enumerate(T_values):\n",
    "    gradients = simulate_gradient_decay(T)\n",
    "    time_steps = np.arange(T)\n",
    "    \n",
    "    axes[idx].bar(time_steps, gradients, alpha=0.7, edgecolor='black')\n",
    "    axes[idx].set_xlabel('Time Step')\n",
    "    axes[idx].set_ylabel('Gradient Magnitude')\n",
    "    axes[idx].set_title(f'Gradient Decay (T={T})')\n",
    "    axes[idx].set_ylim([0, 1.1])\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Annotate first and last\n",
    "    axes[idx].text(0, gradients[0] + 0.05, f'{gradients[0]:.2f}', \n",
    "                  ha='center', fontweight='bold')\n",
    "    axes[idx].text(T-1, gradients[-1] + 0.05, f'{gradients[-1]:.4f}', \n",
    "                  ha='center', fontweight='bold', color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š Gradient Decay Analysis:\")\n",
    "for T in T_values:\n",
    "    final_gradient = 0.7 ** T\n",
    "    print(f\"  T={T:2d}: Final gradient = {final_gradient:.6f} ({final_gradient*100:.2f}% of original)\")\n",
    "\n",
    "print(\"\\nâŒ Problem: Early time steps receive almost no learning signal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSTM Architecture: The Solution\n",
    "\n",
    "LSTM solves the vanishing gradient problem through **gating mechanisms** and a **cell state** that acts as a \"gradient highway.\"\n",
    "\n",
    "### 2.1 Complete LSTM Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    LSTM CELL STRUCTURE                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Inputs: x_t (current input), h_{t-1} (previous hidden), C_{t-1} (previous cell)\n",
    "\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  FORGET GATE    â”‚\n",
    "                    â”‚  f_t = Ïƒ(...)   â”‚  â† Decides what to forget\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "    C_{t-1} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Ã—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    (old cell)               â”‚                â”‚\n",
    "                             â”‚                â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
    "                    â”‚  INPUT GATE     â”‚       â”‚\n",
    "                    â”‚  i_t = Ïƒ(...)   â”‚  â† Decides what to add\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
    "                             â”‚                â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
    "                    â”‚  CELL GATE      â”‚       â”‚\n",
    "                    â”‚  CÌƒ_t = tanh(...) â”‚  â† Candidate values\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
    "                             â”‚                â”‚\n",
    "                             Ã—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "                             â”‚                â”‚\n",
    "                             â–¼                â–¼\n",
    "                            C_t â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                        (new cell)            â”‚\n",
    "                             â”‚                â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
    "                    â”‚  OUTPUT GATE    â”‚       â”‚\n",
    "                    â”‚  o_t = Ïƒ(...)   â”‚  â† Decides what to output\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
    "                             â”‚                â”‚\n",
    "                             Ã—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "                            h_t\n",
    "                      (new hidden)\n",
    "\n",
    "Legend:\n",
    "  Ïƒ = sigmoid (0 to 1) - acts as a gate\n",
    "  Ã— = element-wise multiplication\n",
    "  tanh = hyperbolic tangent (-1 to 1)\n",
    "  + = element-wise addition\n",
    "```\n",
    "\n",
    "### 2.2 The Four Components\n",
    "\n",
    "#### **1. Forget Gate** (What to forget from cell state)\n",
    "```\n",
    "f_t = Ïƒ(W_f Â· [h_{t-1}, x_t] + b_f)\n",
    "\n",
    "â€¢ Output: Values between 0 and 1\n",
    "â€¢ 0 = \"completely forget this\"\n",
    "â€¢ 1 = \"completely keep this\"\n",
    "â€¢ Example: Forget previous subject when new sentence starts\n",
    "```\n",
    "\n",
    "#### **2. Input Gate** (What new information to add)\n",
    "```\n",
    "i_t = Ïƒ(W_i Â· [h_{t-1}, x_t] + b_i)\n",
    "CÌƒ_t = tanh(W_C Â· [h_{t-1}, x_t] + b_C)\n",
    "\n",
    "â€¢ i_t: How much of new info to add (0 to 1)\n",
    "â€¢ CÌƒ_t: Candidate values to add (-1 to 1)\n",
    "â€¢ Example: Add new subject information\n",
    "```\n",
    "\n",
    "#### **3. Cell State Update** (Combine forget and input)\n",
    "```\n",
    "C_t = f_t âŠ™ C_{t-1} + i_t âŠ™ CÌƒ_t\n",
    "      â†‘              â†‘\n",
    "   forget old    add new\n",
    "\n",
    "â€¢ âŠ™ = element-wise multiplication\n",
    "â€¢ This is the \"gradient highway\"!\n",
    "â€¢ Gradients can flow back unchanged through addition\n",
    "```\n",
    "\n",
    "#### **4. Output Gate** (What to output from cell state)\n",
    "```\n",
    "o_t = Ïƒ(W_o Â· [h_{t-1}, x_t] + b_o)\n",
    "h_t = o_t âŠ™ tanh(C_t)\n",
    "\n",
    "â€¢ o_t: How much of cell state to output (0 to 1)\n",
    "â€¢ h_t: Final hidden state (filtered cell state)\n",
    "â€¢ Example: Output relevant information for current prediction\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Why LSTM Solves Vanishing Gradient\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         GRADIENT FLOW: RNN vs LSTM                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Vanilla RNN:\n",
    "  âˆ‚L/âˆ‚h_0 = âˆ‚L/âˆ‚h_T Â· âˆ(W_hh Â· tanh')  â† Repeated multiplication\n",
    "                       â†‘\n",
    "                  Causes decay!\n",
    "\n",
    "LSTM:\n",
    "  âˆ‚L/âˆ‚C_0 = âˆ‚L/âˆ‚C_T Â· âˆ(f_t)  â† Mostly addition through cell state\n",
    "                       â†‘\n",
    "                  Gradient highway!\n",
    "\n",
    "Key Difference:\n",
    "â€¢ RNN: Gradient flows through repeated matrix multiplications\n",
    "â€¢ LSTM: Gradient flows through addition (cell state update)\n",
    "â€¢ Addition preserves gradient magnitude!\n",
    "\n",
    "Result:\n",
    "  RNN:  Gradient decays exponentially (0.7^T)\n",
    "  LSTM: Gradient decays linearly or not at all!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM Implementation\n",
    "\n",
    "Let's implement LSTM from scratch with detailed comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation: Ïƒ(x) = 1 / (1 + e^(-x))\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"Hyperbolic tangent activation.\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax activation.\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "print(\"âœ“ Activation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    \"\"\"LSTM implementation from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights for forget gate\n",
    "        self.W_f = np.random.randn(hidden_dim, input_dim + hidden_dim) * 0.01\n",
    "        self.b_f = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Initialize weights for input gate\n",
    "        self.W_i = np.random.randn(hidden_dim, input_dim + hidden_dim) * 0.01\n",
    "        self.b_i = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Initialize weights for cell gate\n",
    "        self.W_C = np.random.randn(hidden_dim, input_dim + hidden_dim) * 0.01\n",
    "        self.b_C = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Initialize weights for output gate\n",
    "        self.W_o = np.random.randn(hidden_dim, input_dim + hidden_dim) * 0.01\n",
    "        self.b_o = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Initialize weights for output layer\n",
    "        self.W_y = np.random.randn(output_dim, hidden_dim) * 0.01\n",
    "        self.b_y = np.zeros((output_dim, 1))\n",
    "        \n",
    "        # Training history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through LSTM.\n",
    "        \n",
    "        Args:\n",
    "            X: Input sequence (seq_length, input_dim)\n",
    "        \n",
    "        Returns:\n",
    "            outputs: List of output probabilities\n",
    "            cache: Dictionary containing intermediate values for backprop\n",
    "        \"\"\"\n",
    "        seq_length = X.shape[0]\n",
    "        \n",
    "        # Initialize hidden state and cell state\n",
    "        h = np.zeros((self.hidden_dim, 1))\n",
    "        C = np.zeros((self.hidden_dim, 1))\n",
    "        \n",
    "        # Store values for backprop\n",
    "        cache = {\n",
    "            'h': [h],\n",
    "            'C': [C],\n",
    "            'f': [],\n",
    "            'i': [],\n",
    "            'C_tilde': [],\n",
    "            'o': [],\n",
    "            'concat': []\n",
    "        }\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_length):\n",
    "            # Get current input\n",
    "            x_t = X[t].reshape(-1, 1)\n",
    "            \n",
    "            # Concatenate h and x\n",
    "            concat = np.vstack([h, x_t])\n",
    "            cache['concat'].append(concat)\n",
    "            \n",
    "            # Forget gate: decides what to forget from cell state\n",
    "            f_t = sigmoid(np.dot(self.W_f, concat) + self.b_f)\n",
    "            cache['f'].append(f_t)\n",
    "            \n",
    "            # Input gate: decides what new information to add\n",
    "            i_t = sigmoid(np.dot(self.W_i, concat) + self.b_i)\n",
    "            cache['i'].append(i_t)\n",
    "            \n",
    "            # Cell gate: creates candidate values\n",
    "            C_tilde_t = tanh(np.dot(self.W_C, concat) + self.b_C)\n",
    "            cache['C_tilde'].append(C_tilde_t)\n",
    "            \n",
    "            # Update cell state: forget old + add new\n",
    "            C = f_t * C + i_t * C_tilde_t\n",
    "            cache['C'].append(C)\n",
    "            \n",
    "            # Output gate: decides what to output\n",
    "            o_t = sigmoid(np.dot(self.W_o, concat) + self.b_o)\n",
    "            cache['o'].append(o_t)\n",
    "            \n",
    "            # Update hidden state\n",
    "            h = o_t * tanh(C)\n",
    "            cache['h'].append(h)\n",
    "            \n",
    "            # Compute output\n",
    "            logits = np.dot(self.W_y, h) + self.b_y\n",
    "            y_t = softmax(logits.flatten())\n",
    "            outputs.append(y_t)\n",
    "        \n",
    "        return outputs, cache\n",
    "    \n",
    "    def compute_loss(self, outputs, targets):\n",
    "        \"\"\"Compute cross-entropy loss.\"\"\"\n",
    "        seq_length = len(outputs)\n",
    "        loss = 0.0\n",
    "        for t in range(seq_length):\n",
    "            loss += -np.log(outputs[t][targets[t]] + 1e-8)\n",
    "        return loss / seq_length\n",
    "    \n",
    "    def backward(self, X, targets, outputs, cache):\n",
    "        \"\"\"Backpropagation through time for LSTM.\"\"\"\n",
    "        seq_length = X.shape[0]\n",
    "        \n",
    "        # Initialize gradients\n",
    "        dW_f = np.zeros_like(self.W_f)\n",
    "        db_f = np.zeros_like(self.b_f)\n",
    "        dW_i = np.zeros_like(self.W_i)\n",
    "        db_i = np.zeros_like(self.b_i)\n",
    "        dW_C = np.zeros_like(self.W_C)\n",
    "        db_C = np.zeros_like(self.b_C)\n",
    "        dW_o = np.zeros_like(self.W_o)\n",
    "        db_o = np.zeros_like(self.b_o)\n",
    "        dW_y = np.zeros_like(self.W_y)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        \n",
    "        dh_next = np.zeros((self.hidden_dim, 1))\n",
    "        dC_next = np.zeros((self.hidden_dim, 1))\n",
    "        \n",
    "        # Backprop through time\n",
    "        for t in reversed(range(seq_length)):\n",
    "            # Output layer gradient\n",
    "            dy = outputs[t].copy()\n",
    "            dy[targets[t]] -= 1\n",
    "            dy = dy.reshape(-1, 1) / seq_length\n",
    "            \n",
    "            dW_y += np.dot(dy, cache['h'][t+1].T)\n",
    "            db_y += dy\n",
    "            \n",
    "            # Hidden state gradient\n",
    "            dh = np.dot(self.W_y.T, dy) + dh_next\n",
    "            \n",
    "            # Output gate gradient\n",
    "            do = dh * tanh(cache['C'][t+1])\n",
    "            do_raw = do * cache['o'][t] * (1 - cache['o'][t])\n",
    "            \n",
    "            # Cell state gradient\n",
    "            dC = dh * cache['o'][t] * (1 - tanh(cache['C'][t+1])**2) + dC_next\n",
    "            \n",
    "            # Cell gate gradient\n",
    "            dC_tilde = dC * cache['i'][t]\n",
    "            dC_tilde_raw = dC_tilde * (1 - cache['C_tilde'][t]**2)\n",
    "            \n",
    "            # Input gate gradient\n",
    "            di = dC * cache['C_tilde'][t]\n",
    "            di_raw = di * cache['i'][t] * (1 - cache['i'][t])\n",
    "            \n",
    "            # Forget gate gradient\n",
    "            df = dC * cache['C'][t]\n",
    "            df_raw = df * cache['f'][t] * (1 - cache['f'][t])\n",
    "            \n",
    "            # Accumulate gradients\n",
    "            dW_f += np.dot(df_raw, cache['concat'][t].T)\n",
    "            db_f += df_raw\n",
    "            dW_i += np.dot(di_raw, cache['concat'][t].T)\n",
    "            db_i += di_raw\n",
    "            dW_C += np.dot(dC_tilde_raw, cache['concat'][t].T)\n",
    "            db_C += dC_tilde_raw\n",
    "            dW_o += np.dot(do_raw, cache['concat'][t].T)\n",
    "            db_o += do_raw\n",
    "            \n",
    "            # Gradient for next iteration\n",
    "            dconcat = (np.dot(self.W_f.T, df_raw) + \n",
    "                      np.dot(self.W_i.T, di_raw) + \n",
    "                      np.dot(self.W_C.T, dC_tilde_raw) + \n",
    "                      np.dot(self.W_o.T, do_raw))\n",
    "            \n",
    "            dh_next = dconcat[:self.hidden_dim, :]\n",
    "            dC_next = dC * cache['f'][t]\n",
    "        \n",
    "        # Gradient clipping\n",
    "        for grad in [dW_f, db_f, dW_i, db_i, dW_C, db_C, dW_o, db_o, dW_y, db_y]:\n",
    "            np.clip(grad, -5, 5, out=grad)\n",
    "        \n",
    "        return dW_f, db_f, dW_i, db_i, dW_C, db_C, dW_o, db_o, dW_y, db_y\n",
    "    \n",
    "    def update_parameters(self, grads):\n",
    "        \"\"\"Update parameters using gradients.\"\"\"\n",
    "        dW_f, db_f, dW_i, db_i, dW_C, db_C, dW_o, db_o, dW_y, db_y = grads\n",
    "        \n",
    "        self.W_f -= self.learning_rate * dW_f\n",
    "        self.b_f -= self.learning_rate * db_f\n",
    "        self.W_i -= self.learning_rate * dW_i\n",
    "        self.b_i -= self.learning_rate * db_i\n",
    "        self.W_C -= self.learning_rate * dW_C\n",
    "        self.b_C -= self.learning_rate * db_C\n",
    "        self.W_o -= self.learning_rate * dW_o\n",
    "        self.b_o -= self.learning_rate * db_o\n",
    "        self.W_y -= self.learning_rate * dW_y\n",
    "        self.b_y -= self.learning_rate * db_y\n",
    "    \n",
    "    def train_step(self, X, y):\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        outputs, cache = self.forward(X)\n",
    "        loss = self.compute_loss(outputs, y)\n",
    "        grads = self.backward(X, y, outputs, cache)\n",
    "        self.update_parameters(grads)\n",
    "        return loss, cache\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        outputs, _ = self.forward(X)\n",
    "        predictions = np.array([np.argmax(out) for out in outputs])\n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"Evaluate on dataset.\"\"\"\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            outputs, _ = self.forward(X[i])\n",
    "            loss = self.compute_loss(outputs, y[i])\n",
    "            total_loss += loss\n",
    "            \n",
    "            predictions = np.array([np.argmax(out) for out in outputs])\n",
    "            correct += np.sum(predictions == y[i])\n",
    "            total += len(y[i])\n",
    "        \n",
    "        return total_loss / len(X), correct / total\n",
    "\n",
    "print(\"âœ“ LSTM class defined\")\n",
    "print(\"\\nğŸ“Š LSTM has 4 gates (forget, input, cell, output) + output layer\")\n",
    "print(\"   Total parameters: Much more than vanilla RNN!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Data and Train\n",
    "\n",
    "Let's train LSTM on the speech commands dataset and compare with vanilla RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "try:\n",
    "    data = np.load('speech_commands_processed.npz', allow_pickle=True)\n",
    "    \n",
    "    X_train = data['X_train']\n",
    "    X_val = data['X_val']\n",
    "    y_train = data['y_train']\n",
    "    y_val = data['y_val']\n",
    "    label_map = data['label_map'].item()\n",
    "    \n",
    "    print(\"âœ“ Data loaded successfully!\")\n",
    "    print(f\"  X_train: {X_train.shape}\")\n",
    "    print(f\"  X_val: {X_val.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Data file not found. Please run Notebook 2 first to generate the data.\")\n",
    "    print(\"   For now, we'll create a small synthetic dataset for demonstration.\")\n",
    "    \n",
    "    # Create synthetic data\n",
    "    n_samples = 100\n",
    "    seq_length = 50\n",
    "    input_dim = 13\n",
    "    n_classes = 10\n",
    "    \n",
    "    X_train = np.random.randn(n_samples, seq_length, input_dim)\n",
    "    X_val = np.random.randn(20, seq_length, input_dim)\n",
    "    y_train = np.random.randint(0, n_classes, n_samples)\n",
    "    y_val = np.random.randint(0, n_classes, 20)\n",
    "    label_map = {str(i): i for i in range(n_classes)}\n",
    "    \n",
    "    print(\"âœ“ Synthetic data created for demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LSTM\n",
    "input_dim = X_train.shape[2]\n",
    "hidden_dim = 64\n",
    "output_dim = len(label_map)\n",
    "\n",
    "lstm = LSTM(input_dim, hidden_dim, output_dim, learning_rate=0.001)\n",
    "\n",
    "print(\"LSTM initialized:\")\n",
    "print(f\"  Input dim: {input_dim}\")\n",
    "print(f\"  Hidden dim: {hidden_dim}\")\n",
    "print(f\"  Output dim: {output_dim}\")\n",
    "print(f\"\\n  Parameters:\")\n",
    "print(f\"    Forget gate: {lstm.W_f.size + lstm.b_f.size}\")\n",
    "print(f\"    Input gate:  {lstm.W_i.size + lstm.b_i.size}\")\n",
    "print(f\"    Cell gate:   {lstm.W_C.size + lstm.b_C.size}\")\n",
    "print(f\"    Output gate: {lstm.W_o.size + lstm.b_o.size}\")\n",
    "print(f\"    Output layer: {lstm.W_y.size + lstm.b_y.size}\")\n",
    "total_params = (lstm.W_f.size + lstm.b_f.size + lstm.W_i.size + lstm.b_i.size + \n",
    "                lstm.W_C.size + lstm.b_C.size + lstm.W_o.size + lstm.b_o.size + \n",
    "                lstm.W_y.size + lstm.b_y.size)\n",
    "print(f\"    TOTAL: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "n_epochs = 20\n",
    "print(f\"Training LSTM for {n_epochs} epochs...\\n\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Shuffle training data\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    \n",
    "    # Train\n",
    "    for idx in indices:\n",
    "        seq_label = y_train[idx]\n",
    "        frame_labels = np.full(X_train[idx].shape[0], seq_label)\n",
    "        lstm.train_step(X_train[idx], frame_labels)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_loss, train_acc = lstm.evaluate(\n",
    "        X_train,\n",
    "        np.array([np.full(X_train[i].shape[0], y_train[i]) for i in range(len(X_train))])\n",
    "    )\n",
    "    val_loss, val_acc = lstm.evaluate(\n",
    "        X_val,\n",
    "        np.array([np.full(X_val[i].shape[0], y_val[i]) for i in range(len(X_val))])\n",
    "    )\n",
    "    \n",
    "    # Store\n",
    "    lstm.train_losses.append(train_loss)\n",
    "    lstm.val_losses.append(val_loss)\n",
    "    lstm.train_accuracies.append(train_acc)\n",
    "    lstm.val_accuracies.append(val_acc)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"\\nâœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "âœ… **Vanishing gradient problem** - Why vanilla RNNs fail on long sequences  \n",
    "âœ… **LSTM architecture** - 4 gates (forget, input, cell, output) + cell state  \n",
    "âœ… **Gradient highway** - Cell state allows gradients to flow unchanged  \n",
    "âœ… **Implementation** - Complete LSTM from scratch with BPTT  \n",
    "âœ… **Performance** - LSTM learns long-term dependencies better than RNN  \n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Cell state is the key** - Acts as a gradient highway\n",
    "2. **Gates control information flow** - Forget, input, output\n",
    "3. **More parameters = more capacity** - But also slower training\n",
    "4. **Solves vanishing gradient** - Can learn dependencies over 100+ steps\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In **Notebook 4**, we'll explore:\n",
    "- **GRU (Gated Recurrent Unit)** - Simpler alternative to LSTM\n",
    "- **Fewer gates** - Only 2 gates instead of 4\n",
    "- **Faster training** - Fewer parameters\n",
    "- **Similar performance** - Often matches LSTM\n",
    "\n",
    "**LSTM is the workhorse of sequence modeling!** ğŸ¯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
