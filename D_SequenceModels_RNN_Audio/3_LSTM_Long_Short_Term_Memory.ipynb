{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM: Long Short-Term Memory Networks\n",
    "\n",
    "## Understanding LSTM Networks\n",
    "\n",
    "**Based on the concepts from \"Understanding LSTM Networks\" by Christopher Olah**\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Humans don't start their thinking from scratch every second. As you read this text, you understand each word based on your understanding of previous words. You don't throw everything away and start thinking from scratch again. Your thoughts have **persistence**.\n",
    "\n",
    "Traditional neural networks can't do this, and it seems like a major shortcoming. **Recurrent Neural Networks** address this issue through loops that allow information to persist. However, vanilla RNNs have a critical limitation: **the vanishing gradient problem**.\n",
    "\n",
    "**LSTM (Long Short-Term Memory)** networks, introduced by Hochreiter & Schmidhuber in 1997, solve this problem through a clever gating mechanism. They are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their **default behavior**!\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "1. Understand the vanishing gradient problem in vanilla RNNs\n",
    "2. Learn LSTM architecture and gating mechanisms step-by-step\n",
    "3. Understand the \"cell state\" as a gradient highway\n",
    "4. Implement LSTM from scratch\n",
    "5. See how LSTM solves long-term dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The Problem of Long-Term Dependencies\n",
    "\n",
    "### 1.1 When RNNs Work Well\n",
    "\n",
    "Sometimes, we only need to look at **recent information** to perform the present task.\n",
    "\n",
    "**Example:** Predicting the next word in \"the clouds are in the **___**\"\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              SHORT-TERM DEPENDENCY                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Input:  \"the\"  \"clouds\"  \"are\"  \"in\"  \"the\"  ???\n",
    "         â”‚       â”‚        â”‚      â”‚      â”‚      â†“\n",
    "        hâ‚€  â†’  hâ‚   â†’   hâ‚‚  â†’  hâ‚ƒ  â†’  hâ‚„  â†’  hâ‚…\n",
    "                                              â†“\n",
    "                                          Predict: \"sky\"\n",
    "\n",
    "âœ“ Recent context is sufficient!\n",
    "âœ“ Vanilla RNN works fine here\n",
    "```\n",
    "\n",
    "We don't need any further context â€“ it's pretty obvious the next word is going to be **sky**. In such cases, where the gap between the relevant information and the place that it's needed is **small**, RNNs can learn to use the past information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 When RNNs Fail: Long-Term Dependencies\n",
    "\n",
    "But there are also cases where we need **more context**.\n",
    "\n",
    "**Example:** Predicting the last word in \"I grew up in France... I speak fluent **___**\"\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              LONG-TERM DEPENDENCY                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Input: \"I\" \"grew\" \"up\" \"in\" \"France\" ... \"I\" \"speak\" \"fluent\" ???\n",
    "        â”‚    â”‚     â”‚    â”‚      â”‚      ...  â”‚     â”‚       â”‚     â†“\n",
    "       hâ‚€ â†’ hâ‚ â†’ hâ‚‚ â†’ hâ‚ƒ â†’  hâ‚„  â†’ ... â†’ hâ‚â‚€ â†’ hâ‚â‚ â†’ hâ‚â‚‚ â†’ hâ‚â‚ƒ\n",
    "                            â†‘                                â†“\n",
    "                      Important info!                  Predict: \"French\"\n",
    "                      (but far away)\n",
    "\n",
    "âŒ Recent context suggests \"a language\"\n",
    "âŒ But which language? Need \"France\" from way back!\n",
    "âŒ Vanilla RNN struggles with this gap\n",
    "```\n",
    "\n",
    "Recent information suggests that the next word is probably the name of a language, but if we want to narrow down **which** language, we need the context of \"France\", from further back. It's entirely possible for the gap between the relevant information and the point where it is needed to become **very large**.\n",
    "\n",
    "Unfortunately, as that gap grows, RNNs become unable to learn to connect the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 The Vanishing Gradient Problem\n",
    "\n",
    "**Why do RNNs fail at long-term dependencies?**\n",
    "\n",
    "The answer lies in how gradients flow during backpropagation.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         GRADIENT FLOW IN VANILLA RNN                       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Forward Pass (Information Flow):\n",
    "t=0 â†’ t=1 â†’ t=2 â†’ t=3 â†’ t=4 â†’ t=5 â†’ t=6 â†’ t=7 â†’ t=8 â†’ t=9\n",
    "â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚\n",
    "hâ‚€ â†’ hâ‚ â†’ hâ‚‚ â†’ hâ‚ƒ â†’ hâ‚„ â†’ hâ‚… â†’ hâ‚† â†’ hâ‚‡ â†’ hâ‚ˆ â†’ hâ‚‰ â†’ Loss\n",
    "\n",
    "Backward Pass (Gradient Flow):\n",
    "t=9 â† t=8 â† t=7 â† t=6 â† t=5 â† t=4 â† t=3 â† t=2 â† t=1 â† t=0\n",
    "â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚\n",
    "â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆ    â–ˆ     â–     â–     â–     â–     â–     â–\n",
    "100%  75%   50%   25%   10%   5%    2%    1%    0.5%  0.1%\n",
    "\n",
    "Problem: Gradient magnitude decays exponentially!\n",
    "         Early time steps receive almost no gradient signal.\n",
    "```\n",
    "\n",
    "#### Mathematical Explanation\n",
    "\n",
    "In vanilla RNN, the gradient flows through repeated matrix multiplications:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial h_0} = \\frac{\\partial L}{\\partial h_T} \\cdot \\prod_{t=1}^{T} \\frac{\\partial h_t}{\\partial h_{t-1}}$$\n",
    "\n",
    "Each term $\\frac{\\partial h_t}{\\partial h_{t-1}}$ involves:\n",
    "- Matrix multiplication by $W_{hh}$\n",
    "- Derivative of activation function (tanh)\n",
    "\n",
    "**Key insight:**\n",
    "- If $|W_{hh}| < 1$ and $|\\text{tanh}'| < 1$, the product shrinks exponentially\n",
    "- After T steps: gradient $\\approx (0.7)^T \\cdot \\text{original gradient}$\n",
    "- For T=10: gradient is only **2.8%** of original\n",
    "- For T=20: gradient is only **0.08%** of original\n",
    "\n",
    "**Consequence:** The network can't learn long-term dependencies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate gradient decay\n",
    "def simulate_gradient_decay(T, decay_rate=0.7):\n",
    "    \"\"\"Simulate how gradient decays over time steps.\"\"\"\n",
    "    gradients = []\n",
    "    gradient = 1.0  # Start with gradient of 1.0\n",
    "    \n",
    "    for t in range(T):\n",
    "        gradients.append(gradient)\n",
    "        gradient *= decay_rate  # Multiply by decay rate\n",
    "    \n",
    "    return np.array(gradients[::-1])  # Reverse (backward pass)\n",
    "\n",
    "# Simulate for different sequence lengths\n",
    "T_values = [10, 20, 30]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, T in enumerate(T_values):\n",
    "    gradients = simulate_gradient_decay(T)\n",
    "    time_steps = np.arange(T)\n",
    "    \n",
    "    axes[idx].bar(time_steps, gradients, alpha=0.7, edgecolor='black')\n",
    "    axes[idx].set_xlabel('Time Step')\n",
    "    axes[idx].set_ylabel('Gradient Magnitude')\n",
    "    axes[idx].set_title(f'Gradient Decay (T={T})')\n",
    "    axes[idx].set_ylim([0, 1.1])\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Annotate first and last\n",
    "    axes[idx].text(0, gradients[0] + 0.05, f'{gradients[0]:.2f}', \n",
    "                  ha='center', fontweight='bold')\n",
    "    axes[idx].text(T-1, gradients[-1] + 0.05, f'{gradients[-1]:.4f}', \n",
    "                  ha='center', fontweight='bold', color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š Gradient Decay Analysis:\")\n",
    "for T in T_values:\n",
    "    final_gradient = 0.7 ** T\n",
    "    print(f\"  T={T:2d}: Final gradient = {final_gradient:.6f} ({final_gradient*100:.2f}% of original)\")\n",
    "\n",
    "print(\"\\nâŒ Problem: Early time steps receive almost no learning signal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. LSTM Networks: The Solution\n",
    "\n",
    "### 2.1 The Core Idea: Cell State\n",
    "\n",
    "The key to LSTMs is the **cell state** â€“ the horizontal line running through the top of the diagram.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              THE CELL STATE: GRADIENT HIGHWAY              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "                C_{t-1} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ C_t â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ C_{t+1}\n",
    "                  â†‘                      â†‘                     â†‘\n",
    "            (cell state)           (cell state)          (cell state)\n",
    "\n",
    "The cell state is kind of like a conveyor belt.\n",
    "It runs straight down the entire chain, with only some minor\n",
    "linear interactions. It's very easy for information to just\n",
    "flow along it unchanged.\n",
    "\n",
    "This is the \"gradient highway\" that solves vanishing gradient!\n",
    "```\n",
    "\n",
    "The cell state is kind of like a **conveyor belt**. It runs straight down the entire chain, with only some minor linear interactions. It's very easy for information to just flow along it unchanged.\n",
    "\n",
    "The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called **gates**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Gates: Controlling Information Flow\n",
    "\n",
    "Gates are a way to optionally let information through. They are composed of:\n",
    "1. A **sigmoid neural net layer** (outputs 0 to 1)\n",
    "2. A **pointwise multiplication operation**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  HOW GATES WORK                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Sigmoid Layer:\n",
    "  Ïƒ(x) â†’ outputs between 0 and 1\n",
    "  \n",
    "  0 = \"let nothing through\"\n",
    "  1 = \"let everything through\"\n",
    "  0.5 = \"let half through\"\n",
    "\n",
    "Example:\n",
    "  Information: [0.8, -0.3, 0.5, 0.2]\n",
    "  Gate:        [1.0,  0.0, 0.5, 0.8]\n",
    "               Ã—     Ã—     Ã—     Ã—\n",
    "  Result:      [0.8,  0.0, 0.25, 0.16]\n",
    "                â†‘     â†‘     â†‘     â†‘\n",
    "              keep  block  half  most\n",
    "```\n",
    "\n",
    "An LSTM has **three of these gates**, to protect and control the cell state:\n",
    "1. **Forget Gate** - What to forget from cell state\n",
    "2. **Input Gate** - What new information to add\n",
    "3. **Output Gate** - What to output from cell state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Complete LSTM Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    LSTM CELL STRUCTURE                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Inputs: x_t (current input), h_{t-1} (previous hidden), C_{t-1} (previous cell)\n",
    "\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  FORGET GATE    â”‚\n",
    "                    â”‚  f_t = Ïƒ(...)   â”‚  â† Step 1: What to forget\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "    C_{t-1} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Ã—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    (old cell)               â”‚                â”‚\n",
    "                             â”‚                â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
    "                    â”‚  INPUT GATE     â”‚       â”‚\n",
    "                    â”‚  i_t = Ïƒ(...)   â”‚  â† Step 2: What to add\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
    "                             â”‚                â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
    "                    â”‚  CELL GATE      â”‚       â”‚\n",
    "                    â”‚  CÌƒ_t = tanh(...) â”‚  â† Step 2: Candidate values\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
    "                             â”‚                â”‚\n",
    "                             Ã—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "                             â”‚                â”‚\n",
    "                             â–¼                â–¼\n",
    "                            C_t â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â† Step 3: Update cell\n",
    "                        (new cell)            â”‚\n",
    "                             â”‚                â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
    "                    â”‚  OUTPUT GATE    â”‚       â”‚\n",
    "                    â”‚  o_t = Ïƒ(...)   â”‚  â† Step 4: What to output\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
    "                             â”‚                â”‚\n",
    "                             Ã—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "                            h_t\n",
    "                      (new hidden)\n",
    "\n",
    "Legend:\n",
    "  Ïƒ = sigmoid (0 to 1) - acts as a gate\n",
    "  Ã— = element-wise multiplication\n",
    "  tanh = hyperbolic tangent (-1 to 1)\n",
    "  + = element-wise addition\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Step-by-Step LSTM Walk Through\n",
    "\n",
    "Let's walk through each step of the LSTM with intuitive examples.\n",
    "\n",
    "### Step 1: Forget Gate Layer\n",
    "\n",
    "The first step in our LSTM is to decide what information we're going to **throw away** from the cell state.\n",
    "\n",
    "```\n",
    "f_t = Ïƒ(W_f Â· [h_{t-1}, x_t] + b_f)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Looks at $h_{t-1}$ and $x_t$\n",
    "- Outputs a number between 0 and 1 for each number in the cell state $C_{t-1}$\n",
    "- **1** = \"completely keep this\"\n",
    "- **0** = \"completely get rid of this\"\n",
    "\n",
    "**Example:** Language modeling\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              FORGET GATE EXAMPLE                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Sentence: \"The cat sat on the mat. The dog ran.\"\n",
    "                                      â†‘\n",
    "                              New sentence starts!\n",
    "\n",
    "Cell state might include:\n",
    "  â€¢ Gender of subject (\"cat\" = neutral/feminine)\n",
    "  â€¢ Number (singular)\n",
    "  â€¢ Tense (past)\n",
    "\n",
    "When we see \"The dog\":\n",
    "  Forget gate â†’ 0 for gender of \"cat\"\n",
    "  (We want to forget the old subject's gender)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Input Gate Layer\n",
    "\n",
    "The next step is to decide what **new information** we're going to store in the cell state.\n",
    "\n",
    "This has two parts:\n",
    "\n",
    "```\n",
    "i_t = Ïƒ(W_i Â· [h_{t-1}, x_t] + b_i)      â† Input gate: what to update\n",
    "CÌƒ_t = tanh(W_C Â· [h_{t-1}, x_t] + b_C)   â† Candidate values: what to add\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- **Input gate** ($i_t$): Decides which values we'll update (0 to 1)\n",
    "- **Candidate values** ($\\tilde{C}_t$): Creates new candidate values (-1 to 1)\n",
    "\n",
    "**Example:** Language modeling (continued)\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              INPUT GATE EXAMPLE                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Sentence: \"The cat sat on the mat. The dog ran.\"\n",
    "                                      â†‘\n",
    "                              New subject: \"dog\"\n",
    "\n",
    "We want to add:\n",
    "  â€¢ Gender of new subject (\"dog\" = masculine/neutral)\n",
    "  â€¢ Number (singular)\n",
    "  â€¢ Tense (past)\n",
    "\n",
    "Input gate â†’ 1 for gender dimension\n",
    "Candidate â†’ value representing \"dog's\" gender\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Update Cell State\n",
    "\n",
    "It's now time to update the old cell state, $C_{t-1}$, into the new cell state $C_t$.\n",
    "\n",
    "```\n",
    "C_t = f_t âŠ™ C_{t-1} + i_t âŠ™ CÌƒ_t\n",
    "      â†‘              â†‘\n",
    "   forget old    add new\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Multiply the old state by $f_t$, forgetting the things we decided to forget\n",
    "- Add $i_t \\odot \\tilde{C}_t$, the new candidate values scaled by how much we decided to update\n",
    "\n",
    "**This is the gradient highway!**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              CELL STATE UPDATE                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Old cell state (C_{t-1}):\n",
    "  [gender: cat, number: singular, tense: past]\n",
    "\n",
    "Forget gate (f_t):\n",
    "  [0, 1, 1]  â† Forget gender, keep number and tense\n",
    "\n",
    "Input gate (i_t):\n",
    "  [1, 0, 0]  â† Add new gender, don't change others\n",
    "\n",
    "Candidate (CÌƒ_t):\n",
    "  [gender: dog, -, -]\n",
    "\n",
    "New cell state (C_t):\n",
    "  [gender: dog, number: singular, tense: past]\n",
    "   â†‘            â†‘                  â†‘\n",
    "  updated      kept               kept\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Output Gate Layer\n",
    "\n",
    "Finally, we need to decide what we're going to **output**.\n",
    "\n",
    "```\n",
    "o_t = Ïƒ(W_o Â· [h_{t-1}, x_t] + b_o)\n",
    "h_t = o_t âŠ™ tanh(C_t)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Output gate decides what parts of the cell state to output\n",
    "- We put the cell state through tanh (to push values to -1 to 1)\n",
    "- Multiply by the output gate, so we only output the parts we decided to\n",
    "\n",
    "**Example:** Language modeling (continued)\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              OUTPUT GATE EXAMPLE                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Sentence: \"The dog ran\"\n",
    "                    â†‘\n",
    "            Just saw subject, what's next?\n",
    "\n",
    "Cell state contains:\n",
    "  [gender: dog, number: singular, tense: past]\n",
    "\n",
    "Since we just saw a subject, we might want to output\n",
    "information relevant to a VERB:\n",
    "  â€¢ Number (singular) â†’ verb should be \"runs\" not \"run\"\n",
    "  â€¢ Tense (past) â†’ verb should be \"ran\" not \"runs\"\n",
    "\n",
    "Output gate â†’ 1 for number and tense\n",
    "Output gate â†’ 0 for gender (not relevant for verb)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Complete LSTM Equations\n",
    "\n",
    "Putting it all together:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_t &= \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) && \\text{(Forget gate)} \\\\\n",
    "i_t &= \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) && \\text{(Input gate)} \\\\\n",
    "\\tilde{C}_t &= \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C) && \\text{(Candidate values)} \\\\\n",
    "C_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t && \\text{(Update cell state)} \\\\\n",
    "o_t &= \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) && \\text{(Output gate)} \\\\\n",
    "h_t &= o_t \\odot \\tanh(C_t) && \\text{(Output hidden state)}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Why LSTM Solves Vanishing Gradient\n",
    "\n",
    "### 4.1 The Gradient Highway\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         GRADIENT FLOW: RNN vs LSTM                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Vanilla RNN:\n",
    "  âˆ‚L/âˆ‚h_0 = âˆ‚L/âˆ‚h_T Â· âˆ(W_hh Â· tanh')  â† Repeated multiplication\n",
    "                       â†‘\n",
    "                  Causes decay!\n",
    "\n",
    "LSTM:\n",
    "  âˆ‚L/âˆ‚C_0 = âˆ‚L/âˆ‚C_T Â· âˆ(f_t)  â† Mostly addition through cell state\n",
    "                       â†‘\n",
    "                  Gradient highway!\n",
    "\n",
    "Key Difference:\n",
    "â€¢ RNN: Gradient flows through repeated matrix multiplications\n",
    "â€¢ LSTM: Gradient flows through addition (cell state update)\n",
    "â€¢ Addition preserves gradient magnitude!\n",
    "\n",
    "Cell State Update:\n",
    "  C_t = f_t âŠ™ C_{t-1} + i_t âŠ™ CÌƒ_t\n",
    "        â†‘\n",
    "  When f_t â‰ˆ 1, gradient flows through unchanged!\n",
    "\n",
    "Result:\n",
    "  RNN:  Gradient decays exponentially (0.7^T)\n",
    "  LSTM: Gradient can flow unchanged through cell state!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. LSTM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation: Ïƒ(x) = 1 / (1 + e^(-x))\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"Hyperbolic tangent activation.\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax activation.\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "print(\"âœ“ Activation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    \"\"\"LSTM implementation from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights for forget gate\n",
    "        self.W_f = np.random.randn(hidden_dim, input_dim + hidden_dim) * 0.01\n",
    "        self.b_f = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Initialize weights for input gate\n",
    "        self.W_i = np.random.randn(hidden_dim, input_dim + hidden_dim) * 0.01\n",
    "        self.b_i = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Initialize weights for cell gate\n",
    "        self.W_C = np.random.randn(hidden_dim, input_dim + hidden_dim) * 0.01\n",
    "        self.b_C = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Initialize weights for output gate\n",
    "        self.W_o = np.random.randn(hidden_dim, input_dim + hidden_dim) * 0.01\n",
    "        self.b_o = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Initialize weights for output layer\n",
    "        self.W_y = np.random.randn(output_dim, hidden_dim) * 0.01\n",
    "        self.b_y = np.zeros((output_dim, 1))\n",
    "        \n",
    "        # Training history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through LSTM.\n",
    "        \n",
    "        Args:\n",
    "            X: Input sequence (seq_length, input_dim)\n",
    "        \n",
    "        Returns:\n",
    "            outputs: List of output probabilities\n",
    "            cache: Dictionary containing intermediate values for backprop\n",
    "        \"\"\"\n",
    "        seq_length = X.shape[0]\n",
    "        \n",
    "        # Initialize hidden state and cell state\n",
    "        h = np.zeros((self.hidden_dim, 1))\n",
    "        C = np.zeros((self.hidden_dim, 1))\n",
    "        \n",
    "        # Store values for backprop\n",
    "        cache = {\n",
    "            'h': [h],\n",
    "            'C': [C],\n",
    "            'f': [],\n",
    "            'i': [],\n",
    "            'C_tilde': [],\n",
    "            'o': [],\n",
    "            'concat': []\n",
    "        }\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_length):\n",
    "            # Get current input\n",
    "            x_t = X[t].reshape(-1, 1)\n",
    "            \n",
    "            # Concatenate h and x\n",
    "            concat = np.vstack([h, x_t])\n",
    "            cache['concat'].append(concat)\n",
    "            \n",
    "            # Forget gate: decides what to forget from cell state\n",
    "            f_t = sigmoid(np.dot(self.W_f, concat) + self.b_f)\n",
    "            cache['f'].append(f_t)\n",
    "            \n",
    "            # Input gate: decides what new information to add\n",
    "            i_t = sigmoid(np.dot(self.W_i, concat) + self.b_i)\n",
    "            cache['i'].append(i_t)\n",
    "            \n",
    "            # Cell gate: creates candidate values\n",
    "            C_tilde_t = tanh(np.dot(self.W_C, concat) + self.b_C)\n",
    "            cache['C_tilde'].append(C_tilde_t)\n",
    "            \n",
    "            # Update cell state: forget old + add new\n",
    "            C = f_t * C + i_t * C_tilde_t\n",
    "            cache['C'].append(C)\n",
    "            \n",
    "            # Output gate: decides what to output\n",
    "            o_t = sigmoid(np.dot(self.W_o, concat) + self.b_o)\n",
    "            cache['o'].append(o_t)\n",
    "            \n",
    "            # Update hidden state\n",
    "            h = o_t * tanh(C)\n",
    "            cache['h'].append(h)\n",
    "            \n",
    "            # Compute output\n",
    "            logits = np.dot(self.W_y, h) + self.b_y\n",
    "            y_t = softmax(logits.flatten())\n",
    "            outputs.append(y_t)\n",
    "        \n",
    "        return outputs, cache\n",
    "    \n",
    "    def compute_loss(self, outputs, targets):\n",
    "        \"\"\"Compute cross-entropy loss.\"\"\"\n",
    "        seq_length = len(outputs)\n",
    "        loss = 0.0\n",
    "        for t in range(seq_length):\n",
    "            loss += -np.log(outputs[t][targets[t]] + 1e-8)\n",
    "        return loss / seq_length\n",
    "    \n",
    "    def backward(self, X, targets, outputs, cache):\n",
    "        \"\"\"Backpropagation through time for LSTM.\"\"\"\n",
    "        seq_length = X.shape[0]\n",
    "        \n",
    "        # Initialize gradients\n",
    "        dW_f = np.zeros_like(self.W_f)\n",
    "        db_f = np.zeros_like(self.b_f)\n",
    "        dW_i = np.zeros_like(self.W_i)\n",
    "        db_i = np.zeros_like(self.b_i)\n",
    "        dW_C = np.zeros_like(self.W_C)\n",
    "        db_C = np.zeros_like(self.b_C)\n",
    "        dW_o = np.zeros_like(self.W_o)\n",
    "        db_o = np.zeros_like(self.b_o)\n",
    "        dW_y = np.zeros_like(self.W_y)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        \n",
    "        dh_next = np.zeros((self.hidden_dim, 1))\n",
    "        dC_next = np.zeros((self.hidden_dim, 1))\n",
    "        \n",
    "        # Backprop through time\n",
    "        for t in reversed(range(seq_length)):\n",
    "            # Output layer gradient\n",
    "            dy = outputs[t].copy()\n",
    "            dy[targets[t]] -= 1\n",
    "            dy = dy.reshape(-1, 1) / seq_length\n",
    "            \n",
    "            dW_y += np.dot(dy, cache['h'][t+1].T)\n",
    "            db_y += dy\n",
    "            \n",
    "            # Hidden state gradient\n",
    "            dh = np.dot(self.W_y.T, dy) + dh_next\n",
    "            \n",
    "            # Output gate gradient\n",
    "            do = dh * tanh(cache['C'][t+1])\n",
    "            do_raw = do * cache['o'][t] * (1 - cache['o'][t])\n",
    "            \n",
    "            # Cell state gradient\n",
    "            dC = dh * cache['o'][t] * (1 - tanh(cache['C'][t+1])**2) + dC_next\n",
    "            \n",
    "            # Cell gate gradient\n",
    "            dC_tilde = dC * cache['i'][t]\n",
    "            dC_tilde_raw = dC_tilde * (1 - cache['C_tilde'][t]**2)\n",
    "            \n",
    "            # Input gate gradient\n",
    "            di = dC * cache['C_tilde'][t]\n",
    "            di_raw = di * cache['i'][t] * (1 - cache['i'][t])\n",
    "            \n",
    "            # Forget gate gradient\n",
    "            df = dC * cache['C'][t]\n",
    "            df_raw = df * cache['f'][t] * (1 - cache['f'][t])\n",
    "            \n",
    "            # Accumulate gradients\n",
    "            dW_f += np.dot(df_raw, cache['concat'][t].T)\n",
    "            db_f += df_raw\n",
    "            dW_i += np.dot(di_raw, cache['concat'][t].T)\n",
    "            db_i += di_raw\n",
    "            dW_C += np.dot(dC_tilde_raw, cache['concat'][t].T)\n",
    "            db_C += dC_tilde_raw\n",
    "            dW_o += np.dot(do_raw, cache['concat'][t].T)\n",
    "            db_o += do_raw\n",
    "            \n",
    "            # Gradient for next iteration\n",
    "            dconcat = (np.dot(self.W_f.T, df_raw) + \n",
    "                      np.dot(self.W_i.T, di_raw) + \n",
    "                      np.dot(self.W_C.T, dC_tilde_raw) + \n",
    "                      np.dot(self.W_o.T, do_raw))\n",
    "            \n",
    "            dh_next = dconcat[:self.hidden_dim, :]\n",
    "            dC_next = dC * cache['f'][t]\n",
    "        \n",
    "        # Gradient clipping\n",
    "        for grad in [dW_f, db_f, dW_i, db_i, dW_C, db_C, dW_o, db_o, dW_y, db_y]:\n",
    "            np.clip(grad, -5, 5, out=grad)\n",
    "        \n",
    "        return dW_f, db_f, dW_i, db_i, dW_C, db_C, dW_o, db_o, dW_y, db_y\n",
    "    \n",
    "    def update_parameters(self, grads):\n",
    "        \"\"\"Update parameters using gradients.\"\"\"\n",
    "        dW_f, db_f, dW_i, db_i, dW_C, db_C, dW_o, db_o, dW_y, db_y = grads\n",
    "        \n",
    "        self.W_f -= self.learning_rate * dW_f\n",
    "        self.b_f -= self.learning_rate * db_f\n",
    "        self.W_i -= self.learning_rate * dW_i\n",
    "        self.b_i -= self.learning_rate * db_i\n",
    "        self.W_C -= self.learning_rate * dW_C\n",
    "        self.b_C -= self.learning_rate * db_C\n",
    "        self.W_o -= self.learning_rate * dW_o\n",
    "        self.b_o -= self.learning_rate * db_o\n",
    "        self.W_y -= self.learning_rate * dW_y\n",
    "        self.b_y -= self.learning_rate * db_y\n",
    "    \n",
    "    def train_step(self, X, y):\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        outputs, cache = self.forward(X)\n",
    "        loss = self.compute_loss(outputs, y)\n",
    "        grads = self.backward(X, y, outputs, cache)\n",
    "        self.update_parameters(grads)\n",
    "        return loss, cache\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        outputs, _ = self.forward(X)\n",
    "        predictions = np.array([np.argmax(out) for out in outputs])\n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"Evaluate on dataset.\"\"\"\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            outputs, _ = self.forward(X[i])\n",
    "            loss = self.compute_loss(outputs, y[i])\n",
    "            total_loss += loss\n",
    "            \n",
    "            predictions = np.array([np.argmax(out) for out in outputs])\n",
    "            correct += np.sum(predictions == y[i])\n",
    "            total += len(y[i])\n",
    "        \n",
    "        return total_loss / len(X), correct / total\n",
    "\n",
    "print(\"âœ“ LSTM class defined\")\n",
    "print(\"\\nğŸ“Š LSTM has 4 gates (forget, input, cell, output) + output layer\")\n",
    "print(\"   Total parameters: Much more than vanilla RNN!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data for demonstration\n",
    "n_samples = 100\n",
    "seq_length = 50\n",
    "input_dim = 13\n",
    "n_classes = 10\n",
    "\n",
    "X_train = np.random.randn(n_samples, seq_length, input_dim)\n",
    "X_val = np.random.randn(20, seq_length, input_dim)\n",
    "y_train = np.random.randint(0, n_classes, n_samples)\n",
    "y_val = np.random.randint(0, n_classes, 20)\n",
    "\n",
    "print(\"âœ“ Synthetic data created for demonstration\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_val: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LSTM\n",
    "hidden_dim = 64\n",
    "output_dim = n_classes\n",
    "\n",
    "lstm = LSTM(input_dim, hidden_dim, output_dim, learning_rate=0.001)\n",
    "\n",
    "print(\"LSTM initialized:\")\n",
    "print(f\"  Input dim: {input_dim}\")\n",
    "print(f\"  Hidden dim: {hidden_dim}\")\n",
    "print(f\"  Output dim: {output_dim}\")\n",
    "print(f\"\\n  Parameters:\")\n",
    "print(f\"    Forget gate: {lstm.W_f.size + lstm.b_f.size}\")\n",
    "print(f\"    Input gate:  {lstm.W_i.size + lstm.b_i.size}\")\n",
    "print(f\"    Cell gate:   {lstm.W_C.size + lstm.b_C.size}\")\n",
    "print(f\"    Output gate: {lstm.W_o.size + lstm.b_o.size}\")\n",
    "print(f\"    Output layer: {lstm.W_y.size + lstm.b_y.size}\")\n",
    "total_params = (lstm.W_f.size + lstm.b_f.size + lstm.W_i.size + lstm.b_i.size + \n",
    "                lstm.W_C.size + lstm.b_C.size + lstm.W_o.size + lstm.b_o.size + \n",
    "                lstm.W_y.size + lstm.b_y.size)\n",
    "print(f\"    TOTAL: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "n_epochs = 20\n",
    "print(f\"Training LSTM for {n_epochs} epochs...\\n\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Shuffle training data\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    \n",
    "    # Train\n",
    "    for idx in indices:\n",
    "        seq_label = y_train[idx]\n",
    "        frame_labels = np.full(X_train[idx].shape[0], seq_label)\n",
    "        lstm.train_step(X_train[idx], frame_labels)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_loss, train_acc = lstm.evaluate(\n",
    "        X_train,\n",
    "        np.array([np.full(X_train[i].shape[0], y_train[i]) for i in range(len(X_train))])\n",
    "    )\n",
    "    val_loss, val_acc = lstm.evaluate(\n",
    "        X_val,\n",
    "        np.array([np.full(X_val[i].shape[0], y_val[i]) for i in range(len(X_val))])\n",
    "    )\n",
    "    \n",
    "    # Store\n",
    "    lstm.train_losses.append(train_loss)\n",
    "    lstm.val_losses.append(val_loss)\n",
    "    lstm.train_accuracies.append(train_acc)\n",
    "    lstm.val_accuracies.append(val_acc)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"\\nâœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Summary and Conclusion\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "âœ… **Long-term dependencies** - Why vanilla RNNs fail on long sequences  \n",
    "âœ… **LSTM architecture** - 4 gates (forget, input, cell, output) + cell state  \n",
    "âœ… **Cell state as gradient highway** - Allows gradients to flow unchanged  \n",
    "âœ… **Step-by-step walkthrough** - Each gate with intuitive examples  \n",
    "âœ… **Implementation** - Complete LSTM from scratch with BPTT  \n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              LSTM: THE BIG PICTURE                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Problem:\n",
    "  Vanilla RNNs can't learn long-term dependencies\n",
    "  â†’ Vanishing gradient problem\n",
    "\n",
    "Solution:\n",
    "  LSTM introduces:\n",
    "  1. Cell state (gradient highway)\n",
    "  2. Gates (control information flow)\n",
    "  3. Forget gate (what to forget)\n",
    "  4. Input gate (what to add)\n",
    "  5. Output gate (what to output)\n",
    "\n",
    "Result:\n",
    "  âœ“ Can learn dependencies over 100+ steps\n",
    "  âœ“ Gradients flow through cell state unchanged\n",
    "  âœ“ Gates learn when to remember/forget\n",
    "  âœ“ Default behavior is to remember!\n",
    "```\n",
    "\n",
    "### Variants and Extensions:\n",
    "\n",
    "The LSTM we described is a pretty normal LSTM. But not all LSTMs are the same. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but worth mentioning:\n",
    "\n",
    "1. **Peephole connections** - Let gates look at cell state\n",
    "2. **Coupled forget and input gates** - Decide together what to forget/add\n",
    "3. **GRU (Gated Recurrent Unit)** - Simpler variant with only 2 gates\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In **Notebook 4**, we'll explore:\n",
    "- **GRU (Gated Recurrent Unit)** - Simpler alternative to LSTM\n",
    "- **Fewer gates** - Only 2 gates instead of 4\n",
    "- **Faster training** - Fewer parameters (~25% less)\n",
    "- **Similar performance** - Often matches LSTM\n",
    "\n",
    "**LSTM is the workhorse of sequence modeling!** ğŸ¯\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook was inspired by Christopher Olah's excellent blog post \"Understanding LSTM Networks\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
