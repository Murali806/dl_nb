{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU: Gated Recurrent Unit\n",
    "\n",
    "## Understanding GRU Networks\n",
    "\n",
    "**Based on the concepts from \"Understanding LSTM Networks\" by Christopher Olah**\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In the previous notebook, we learned about LSTMs and how they solve the vanishing gradient problem through gating mechanisms. However, LSTMs are quite complex with 4 different gates.\n",
    "\n",
    "**GRU (Gated Recurrent Unit)**, introduced by Cho et al. in 2014, is a simpler alternative that:\n",
    "- Uses only **2 gates** instead of 4\n",
    "- Merges the cell state and hidden state\n",
    "- Has **fewer parameters** (faster training)\n",
    "- Often achieves **similar performance** to LSTM\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "1. Understand how GRU simplifies LSTM architecture\n",
    "2. Learn the two gates: reset and update\n",
    "3. Implement GRU from scratch\n",
    "4. Compare GRU vs LSTM performance\n",
    "5. Understand when to use GRU vs LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. From LSTM to GRU: Simplification\n",
    "\n",
    "### 1.1 Recap: LSTM Architecture\n",
    "\n",
    "LSTM has **4 components**:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    LSTM (4 gates)                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "1. Forget gate (f_t):  What to forget from cell state\n",
    "2. Input gate (i_t):   What new info to add\n",
    "3. Cell gate (CÌƒ_t):    Candidate values\n",
    "4. Output gate (o_t):  What to output\n",
    "\n",
    "Plus: Separate cell state (C_t) and hidden state (h_t)\n",
    "```\n",
    "\n",
    "### 1.2 GRU Simplification\n",
    "\n",
    "GRU simplifies this to **2 gates**:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    GRU (2 gates)                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "1. Reset gate (r_t):   How much past info to forget\n",
    "2. Update gate (z_t):  How much to update with new info\n",
    "\n",
    "Plus: Merges cell state and hidden state into one (h_t)\n",
    "```\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "| Feature | LSTM | GRU |\n",
    "|---------|------|-----|\n",
    "| **Gates** | 4 (forget, input, cell, output) | 2 (reset, update) |\n",
    "| **States** | 2 (cell C_t, hidden h_t) | 1 (hidden h_t) |\n",
    "| **Parameters** | More | Fewer (~25% less) |\n",
    "| **Training Speed** | Slower | Faster |\n",
    "| **Performance** | Excellent | Similar to LSTM |\n",
    "| **Use Case** | When you have lots of data | When you want faster training |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. GRU Architecture: Step-by-Step\n",
    "\n",
    "### 2.1 The Big Picture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    GRU CELL STRUCTURE                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Inputs: x_t (current input), h_{t-1} (previous hidden)\n",
    "\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  RESET GATE     â”‚\n",
    "                    â”‚  r_t = Ïƒ(...)   â”‚  â† How much past to use\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "    h_{t-1} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Ã—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    (previous)               â”‚          â”‚\n",
    "                             â”‚          â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "                    â”‚  CANDIDATE      â”‚ â”‚\n",
    "                    â”‚  hÌƒ_t = tanh(...) â”‚ â”‚  â† New candidate\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "                             â”‚          â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "                    â”‚  UPDATE GATE    â”‚ â”‚\n",
    "                    â”‚  z_t = Ïƒ(...)   â”‚ â”‚  â† How much to update\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "                             â”‚          â”‚\n",
    "                             â–¼          â–¼\n",
    "                    h_t = (1-z_t)âŠ™h_{t-1} + z_tâŠ™hÌƒ_t\n",
    "                          (new hidden state)\n",
    "\n",
    "Legend:\n",
    "  Ïƒ = sigmoid (0 to 1) - acts as a gate\n",
    "  Ã— = element-wise multiplication\n",
    "  âŠ™ = element-wise multiplication\n",
    "  tanh = hyperbolic tangent (-1 to 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Step-by-Step Walk Through\n",
    "\n",
    "Let's understand each component with intuitive examples.\n",
    "\n",
    "#### **Step 1: Reset Gate** (How much past to forget)\n",
    "\n",
    "```\n",
    "r_t = Ïƒ(W_r Â· [h_{t-1}, x_t] + b_r)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Decides how much of the previous hidden state to use\n",
    "- Output: Values between 0 and 1\n",
    "- **0** = \"Completely ignore the past\"\n",
    "- **1** = \"Completely use the past\"\n",
    "\n",
    "**Example:** Language modeling\n",
    "- When starting a new sentence, reset gate â†’ 0 (forget previous sentence)\n",
    "- When continuing a sentence, reset gate â†’ 1 (use previous context)\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              RESET GATE VISUALIZATION                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Sentence 1: \"The cat sat on the mat.\"\n",
    "Sentence 2: \"Dogs are loyal.\"\n",
    "                    â†‘\n",
    "            Reset gate â‰ˆ 0\n",
    "         (New sentence, forget previous)\n",
    "\n",
    "Within sentence: \"The cat sat on the...\"\n",
    "                                    â†‘\n",
    "                        Reset gate â‰ˆ 1\n",
    "                    (Continue, use context)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2: Candidate Hidden State** (What new info to consider)\n",
    "\n",
    "```\n",
    "hÌƒ_t = tanh(W_h Â· [r_t âŠ™ h_{t-1}, x_t] + b_h)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Creates a candidate for the new hidden state\n",
    "- Uses **reset gate** to control how much past info to include\n",
    "- Output: Values between -1 and 1 (tanh)\n",
    "\n",
    "**Key insight:**\n",
    "- `r_t âŠ™ h_{t-1}` means: \"Use only the relevant parts of the past\"\n",
    "- If reset gate is 0, we ignore the past completely\n",
    "- If reset gate is 1, we use all the past information\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           CANDIDATE STATE COMPUTATION                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Case 1: Reset gate â‰ˆ 0 (New context)\n",
    "  r_t âŠ™ h_{t-1} â‰ˆ 0\n",
    "  hÌƒ_t â‰ˆ tanh(W_h Â· [0, x_t])\n",
    "  â†’ Candidate based mostly on current input\n",
    "\n",
    "Case 2: Reset gate â‰ˆ 1 (Continue context)\n",
    "  r_t âŠ™ h_{t-1} â‰ˆ h_{t-1}\n",
    "  hÌƒ_t â‰ˆ tanh(W_h Â· [h_{t-1}, x_t])\n",
    "  â†’ Candidate uses both past and current\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3: Update Gate** (How much to update)\n",
    "\n",
    "```\n",
    "z_t = Ïƒ(W_z Â· [h_{t-1}, x_t] + b_z)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Decides how much to update the hidden state\n",
    "- Output: Values between 0 and 1\n",
    "- **0** = \"Keep old hidden state completely\"\n",
    "- **1** = \"Use new candidate completely\"\n",
    "\n",
    "**Example:** Counting task\n",
    "- When seeing a relevant item, update gate â†’ 1 (update count)\n",
    "- When seeing irrelevant item, update gate â†’ 0 (keep old count)\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              UPDATE GATE VISUALIZATION                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Task: Count number of 'a's in text\n",
    "\n",
    "Input: \"b c a d a e f\"\n",
    "        â†“ â†“ â†“ â†“ â†“ â†“ â†“\n",
    "Update: 0 0 1 0 1 0 0\n",
    "        â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚\n",
    "Count:  0 0 1 1 2 2 2\n",
    "            â†‘   â†‘\n",
    "    Update when 'a' appears\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4: Final Hidden State** (Combine old and new)\n",
    "\n",
    "```\n",
    "h_t = (1 - z_t) âŠ™ h_{t-1} + z_t âŠ™ hÌƒ_t\n",
    "      â†‘                     â†‘\n",
    "   Keep old            Use new\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Interpolates between old hidden state and new candidate\n",
    "- Update gate controls the balance\n",
    "\n",
    "**Key insight:**\n",
    "- This is a **weighted average** between old and new\n",
    "- `(1 - z_t)` and `z_t` always sum to 1\n",
    "- Ensures smooth transitions\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           FINAL HIDDEN STATE COMPUTATION                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Example with z_t = 0.3:\n",
    "\n",
    "h_t = 0.7 Ã— h_{t-1} + 0.3 Ã— hÌƒ_t\n",
    "      â†‘               â†‘\n",
    "   70% old        30% new\n",
    "\n",
    "Extreme cases:\n",
    "â€¢ z_t = 0: h_t = h_{t-1}  (no update, keep old)\n",
    "â€¢ z_t = 1: h_t = hÌƒ_t      (full update, use new)\n",
    "â€¢ z_t = 0.5: h_t = 0.5Ã—h_{t-1} + 0.5Ã—hÌƒ_t  (equal mix)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Complete GRU Equations\n",
    "\n",
    "Putting it all together:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "r_t &= \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r) && \\text{(Reset gate)} \\\\\n",
    "z_t &= \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z) && \\text{(Update gate)} \\\\\n",
    "\\tilde{h}_t &= \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t] + b_h) && \\text{(Candidate)} \\\\\n",
    "h_t &= (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t && \\text{(Final hidden)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Notation:**\n",
    "- $\\sigma$ = sigmoid function (0 to 1)\n",
    "- $\\tanh$ = hyperbolic tangent (-1 to 1)\n",
    "- $\\odot$ = element-wise multiplication\n",
    "- $[a, b]$ = concatenation of vectors a and b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Why GRU Works: Gradient Flow\n",
    "\n",
    "### 3.1 Comparison with Vanilla RNN\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         GRADIENT FLOW: RNN vs GRU                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Vanilla RNN:\n",
    "  h_t = tanh(W_hh Â· h_{t-1} + W_xh Â· x_t)\n",
    "  \n",
    "  Gradient: âˆ‚L/âˆ‚h_0 = âˆ‚L/âˆ‚h_T Â· âˆ(W_hh Â· tanh')\n",
    "                                  â†‘\n",
    "                          Repeated multiplication\n",
    "                          â†’ Vanishing gradient!\n",
    "\n",
    "GRU:\n",
    "  h_t = (1 - z_t) âŠ™ h_{t-1} + z_t âŠ™ hÌƒ_t\n",
    "  \n",
    "  Gradient: âˆ‚L/âˆ‚h_0 = âˆ‚L/âˆ‚h_T Â· âˆ(1 - z_t)\n",
    "                                  â†‘\n",
    "                          Controlled by gates\n",
    "                          â†’ Can preserve gradient!\n",
    "\n",
    "Key Insight:\n",
    "â€¢ When z_t â‰ˆ 0: h_t â‰ˆ h_{t-1}\n",
    "  â†’ Gradient flows through unchanged (like LSTM cell state)\n",
    "â€¢ When z_t â‰ˆ 1: h_t â‰ˆ hÌƒ_t\n",
    "  â†’ Gradient flows through tanh (like RNN)\n",
    "â€¢ GRU learns when to preserve vs update!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 GRU vs LSTM: Gradient Highway\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              LSTM vs GRU COMPARISON                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "LSTM:\n",
    "  C_t = f_t âŠ™ C_{t-1} + i_t âŠ™ CÌƒ_t\n",
    "        â†‘\n",
    "  Separate cell state = gradient highway\n",
    "  \n",
    "  Pros:\n",
    "  â€¢ Explicit memory (cell state)\n",
    "  â€¢ More control (4 gates)\n",
    "  â€¢ Better for very long sequences\n",
    "  \n",
    "  Cons:\n",
    "  â€¢ More parameters\n",
    "  â€¢ Slower training\n",
    "  â€¢ More complex\n",
    "\n",
    "GRU:\n",
    "  h_t = (1 - z_t) âŠ™ h_{t-1} + z_t âŠ™ hÌƒ_t\n",
    "        â†‘\n",
    "  Hidden state itself = gradient highway\n",
    "  \n",
    "  Pros:\n",
    "  â€¢ Fewer parameters (~25% less)\n",
    "  â€¢ Faster training\n",
    "  â€¢ Simpler architecture\n",
    "  \n",
    "  Cons:\n",
    "  â€¢ Less control (2 gates)\n",
    "  â€¢ May struggle with very long sequences\n",
    "\n",
    "Performance:\n",
    "  In practice, GRU and LSTM perform similarly on most tasks!\n",
    "  Choice depends on:\n",
    "  â€¢ Dataset size (GRU better for smaller datasets)\n",
    "  â€¢ Sequence length (LSTM better for very long sequences)\n",
    "  â€¢ Computational budget (GRU faster)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. GRU Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation: Ïƒ(x) = 1 / (1 + e^(-x))\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"Hyperbolic tangent activation.\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax activation.\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "print(\"âœ“ Activation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    \"\"\"GRU implementation from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights for reset gate\n",
    "        self.W_r = np.random.randn(hidden_dim, input_dim + hidden_dim) * 0.01\n",
    "        self.b_r = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Initialize weights for update gate\n",
    "        self.W_z = np.random.randn(hidden_dim, input_dim + hidden_dim) * 0.01\n",
    "        self.b_z = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Initialize weights for candidate hidden state\n",
    "        self.W_h = np.random.randn(hidden_dim, input_dim + hidden_dim) * 0.01\n",
    "        self.b_h = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Initialize weights for output layer\n",
    "        self.W_y = np.random.randn(output_dim, hidden_dim) * 0.01\n",
    "        self.b_y = np.zeros((output_dim, 1))\n",
    "        \n",
    "        # Training history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through GRU.\n",
    "        \n",
    "        Args:\n",
    "            X: Input sequence (seq_length, input_dim)\n",
    "        \n",
    "        Returns:\n",
    "            outputs: List of output probabilities\n",
    "            cache: Dictionary containing intermediate values for backprop\n",
    "        \"\"\"\n",
    "        seq_length = X.shape[0]\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        h = np.zeros((self.hidden_dim, 1))\n",
    "        \n",
    "        # Store values for backprop\n",
    "        cache = {\n",
    "            'h': [h],\n",
    "            'r': [],\n",
    "            'z': [],\n",
    "            'h_tilde': [],\n",
    "            'concat': [],\n",
    "            'concat_reset': []\n",
    "        }\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_length):\n",
    "            # Get current input\n",
    "            x_t = X[t].reshape(-1, 1)\n",
    "            \n",
    "            # Concatenate h and x\n",
    "            concat = np.vstack([h, x_t])\n",
    "            cache['concat'].append(concat)\n",
    "            \n",
    "            # Reset gate: decides how much past to use\n",
    "            r_t = sigmoid(np.dot(self.W_r, concat) + self.b_r)\n",
    "            cache['r'].append(r_t)\n",
    "            \n",
    "            # Update gate: decides how much to update\n",
    "            z_t = sigmoid(np.dot(self.W_z, concat) + self.b_z)\n",
    "            cache['z'].append(z_t)\n",
    "            \n",
    "            # Candidate hidden state: new information\n",
    "            concat_reset = np.vstack([r_t * h, x_t])\n",
    "            cache['concat_reset'].append(concat_reset)\n",
    "            h_tilde_t = tanh(np.dot(self.W_h, concat_reset) + self.b_h)\n",
    "            cache['h_tilde'].append(h_tilde_t)\n",
    "            \n",
    "            # Update hidden state: interpolate between old and new\n",
    "            h = (1 - z_t) * h + z_t * h_tilde_t\n",
    "            cache['h'].append(h)\n",
    "            \n",
    "            # Compute output\n",
    "            logits = np.dot(self.W_y, h) + self.b_y\n",
    "            y_t = softmax(logits.flatten())\n",
    "            outputs.append(y_t)\n",
    "        \n",
    "        return outputs, cache\n",
    "    \n",
    "    def compute_loss(self, outputs, targets):\n",
    "        \"\"\"Compute cross-entropy loss.\"\"\"\n",
    "        seq_length = len(outputs)\n",
    "        loss = 0.0\n",
    "        for t in range(seq_length):\n",
    "            loss += -np.log(outputs[t][targets[t]] + 1e-8)\n",
    "        return loss / seq_length\n",
    "    \n",
    "    def backward(self, X, targets, outputs, cache):\n",
    "        \"\"\"Backpropagation through time for GRU.\"\"\"\n",
    "        seq_length = X.shape[0]\n",
    "        \n",
    "        # Initialize gradients\n",
    "        dW_r = np.zeros_like(self.W_r)\n",
    "        db_r = np.zeros_like(self.b_r)\n",
    "        dW_z = np.zeros_like(self.W_z)\n",
    "        db_z = np.zeros_like(self.b_z)\n",
    "        dW_h = np.zeros_like(self.W_h)\n",
    "        db_h = np.zeros_like(self.b_h)\n",
    "        dW_y = np.zeros_like(self.W_y)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        \n",
    "        dh_next = np.zeros((self.hidden_dim, 1))\n",
    "        \n",
    "        # Backprop through time\n",
    "        for t in reversed(range(seq_length)):\n",
    "            # Output layer gradient\n",
    "            dy = outputs[t].copy()\n",
    "            dy[targets[t]] -= 1\n",
    "            dy = dy.reshape(-1, 1) / seq_length\n",
    "            \n",
    "            dW_y += np.dot(dy, cache['h'][t+1].T)\n",
    "            db_y += dy\n",
    "            \n",
    "            # Hidden state gradient\n",
    "            dh = np.dot(self.W_y.T, dy) + dh_next\n",
    "            \n",
    "            # Candidate hidden state gradient\n",
    "            dh_tilde = dh * cache['z'][t]\n",
    "            dh_tilde_raw = dh_tilde * (1 - cache['h_tilde'][t]**2)\n",
    "            \n",
    "            # Update gate gradient\n",
    "            dz = dh * (cache['h_tilde'][t] - cache['h'][t])\n",
    "            dz_raw = dz * cache['z'][t] * (1 - cache['z'][t])\n",
    "            \n",
    "            # Reset gate gradient\n",
    "            dconcat_reset = np.dot(self.W_h.T, dh_tilde_raw)\n",
    "            dr = dconcat_reset[:self.hidden_dim, :] * cache['h'][t]\n",
    "            dr_raw = dr * cache['r'][t] * (1 - cache['r'][t])\n",
    "            \n",
    "            # Accumulate gradients\n",
    "            dW_r += np.dot(dr_raw, cache['concat'][t].T)\n",
    "            db_r += dr_raw\n",
    "            dW_z += np.dot(dz_raw, cache['concat'][t].T)\n",
    "            db_z += dz_raw\n",
    "            dW_h += np.dot(dh_tilde_raw, cache['concat_reset'][t].T)\n",
    "            db_h += dh_tilde_raw\n",
    "            \n",
    "            # Gradient for next iteration\n",
    "            dconcat = (np.dot(self.W_r.T, dr_raw) + \n",
    "                      np.dot(self.W_z.T, dz_raw))\n",
    "            dh_from_concat_reset = dconcat_reset[:self.hidden_dim, :] * cache['r'][t]\n",
    "            dh_next = (dconcat[:self.hidden_dim, :] + \n",
    "                      dh_from_concat_reset + \n",
    "                      dh * (1 - cache['z'][t]))\n",
    "        \n",
    "        # Gradient clipping\n",
    "        for grad in [dW_r, db_r, dW_z, db_z, dW_h, db_h, dW_y, db_y]:\n",
    "            np.clip(grad, -5, 5, out=grad)\n",
    "        \n",
    "        return dW_r, db_r, dW_z, db_z, dW_h, db_h, dW_y, db_y\n",
    "    \n",
    "    def update_parameters(self, grads):\n",
    "        \"\"\"Update parameters using gradients.\"\"\"\n",
    "        dW_r, db_r, dW_z, db_z, dW_h, db_h, dW_y, db_y = grads\n",
    "        \n",
    "        self.W_r -= self.learning_rate * dW_r\n",
    "        self.b_r -= self.learning_rate * db_r\n",
    "        self.W_z -= self.learning_rate * dW_z\n",
    "        self.b_z -= self.learning_rate * db_z\n",
    "        self.W_h -= self.learning_rate * dW_h\n",
    "        self.b_h -= self.learning_rate * db_h\n",
    "        self.W_y -= self.learning_rate * dW_y\n",
    "        self.b_y -= self.learning_rate * db_y\n",
    "    \n",
    "    def train_step(self, X, y):\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        outputs, cache = self.forward(X)\n",
    "        loss = self.compute_loss(outputs, y)\n",
    "        grads = self.backward(X, y, outputs, cache)\n",
    "        self.update_parameters(grads)\n",
    "        return loss, cache\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        outputs, _ = self.forward(X)\n",
    "        predictions = np.array([np.argmax(out) for out in outputs])\n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"Evaluate on dataset.\"\"\"\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            outputs, _ = self.forward(X[i])\n",
    "            loss = self.compute_loss(outputs, y[i])\n",
    "            total_loss += loss\n",
    "            \n",
    "            predictions = np.array([np.argmax(out) for out in outputs])\n",
    "            correct += np.sum(predictions == y[i])\n",
    "            total += len(y[i])\n",
    "        \n",
    "        return total_loss / len(X), correct / total\n",
    "\n",
    "print(\"âœ“ GRU class defined\")\n",
    "print(\"\\nğŸ“Š GRU has 3 components (reset, update, candidate) + output layer\")\n",
    "print(\"   Approximately 25% fewer parameters than LSTM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Training and Comparison\n",
    "\n",
    "Let's train GRU and compare with LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data for demonstration\n",
    "n_samples = 100\n",
    "seq_length = 50\n",
    "input_dim = 13\n",
    "n_classes = 10\n",
    "\n",
    "X_train = np.random.randn(n_samples, seq_length, input_dim)\n",
    "X_val = np.random.randn(20, seq_length, input_dim)\n",
    "y_train = np.random.randint(0, n_classes, n_samples)\n",
    "y_val = np.random.randint(0, n_classes, 20)\n",
    "\n",
    "print(\"âœ“ Synthetic data created for demonstration\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_val: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GRU\n",
    "hidden_dim = 64\n",
    "output_dim = n_classes\n",
    "\n",
    "gru = GRU(input_dim, hidden_dim, output_dim, learning_rate=0.001)\n",
    "\n",
    "print(\"GRU initialized:\")\n",
    "print(f\"  Input dim: {input_dim}\")\n",
    "print(f\"  Hidden dim: {hidden_dim}\")\n",
    "print(f\"  Output dim: {output_dim}\")\n",
    "print(f\"\\n  Parameters:\")\n",
    "print(f\"    Reset gate:  {gru.W_r.size + gru.b_r.size}\")\n",
    "print(f\"    Update gate: {gru.W_z.size + gru.b_z.size}\")\n",
    "print(f\"    Candidate:   {gru.W_h.size + gru.b_h.size}\")\n",
    "print(f\"    Output layer: {gru.W_y.size + gru.b_y.size}\")\n",
    "total_params = (gru.W_r.size + gru.b_r.size + gru.W_z.size + gru.b_z.size + \n",
    "                gru.W_h.size + gru.b_h.size + gru.W_y.size + gru.b_y.size)\n",
    "print(f\"    TOTAL: {total_params}\")\n",
    "print(f\"\\n  Note: ~25% fewer parameters than LSTM!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "n_epochs = 20\n",
    "print(f\"Training GRU for {n_epochs} epochs...\\n\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Shuffle training data\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    \n",
    "    # Train\n",
    "    for idx in indices:\n",
    "        seq_label = y_train[idx]\n",
    "        frame_labels = np.full(X_train[idx].shape[0], seq_label)\n",
    "        gru.train_step(X_train[idx], frame_labels)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_loss, train_acc = gru.evaluate(\n",
    "        X_train,\n",
    "        np.array([np.full(X_train[i].shape[0], y_train[i]) for i in range(len(X_train))])\n",
    "    )\n",
    "    val_loss, val_acc = gru.evaluate(\n",
    "        X_val,\n",
    "        np.array([np.full(X_val[i].shape[0], y_val[i]) for i in range(len(X_val))])\n",
    "    )\n",
    "    \n",
    "    # Store\n",
    "    gru.train_losses.append(train_loss)\n",
    "    gru.val_losses.append(val_loss)\n",
    "    gru.train_accuracies.append(train_acc)\n",
    "    gru.val_accuracies.append(val_acc)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"\\nâœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Summary and Conclusions\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "âœ… **GRU simplifies LSTM** - Only 2 gates instead of 4  \n",
    "âœ… **Reset gate** - Controls how much past to use  \n",
    "âœ… **Update gate** - Controls how much to update  \n",
    "âœ… **Merged states** - No separate cell state  \n",
    "âœ… **Fewer parameters** - ~25% less than LSTM  \n",
    "âœ… **Similar performance** - Often matches LSTM  \n",
    "\n",
    "### GRU vs LSTM: When to Use What?\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              DECISION GUIDE: GRU vs LSTM                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Use GRU when:\n",
    "  âœ“ You have limited training data\n",
    "  âœ“ You need faster training\n",
    "  âœ“ Sequence length is moderate (<100 steps)\n",
    "  âœ“ You want simpler architecture\n",
    "  âœ“ Computational resources are limited\n",
    "\n",
    "Use LSTM when:\n",
    "  âœ“ You have lots of training data\n",
    "  âœ“ Sequences are very long (>100 steps)\n",
    "  âœ“ You need maximum performance\n",
    "  âœ“ You have computational resources\n",
    "  âœ“ Task requires precise memory control\n",
    "\n",
    "In practice:\n",
    "  â€¢ Try both and see which works better!\n",
    "  â€¢ GRU is often the better starting point\n",
    "  â€¢ Performance difference is usually small\n",
    "```\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Simplicity wins** - GRU's simpler design is often sufficient\n",
    "2. **Fewer parameters** - Faster training, less overfitting\n",
    "3. **Gradient highway** - Update gate creates path for gradients\n",
    "4. **Practical choice** - GRU is often preferred in industry\n",
    "\n",
    "### Historical Note:\n",
    "\n",
    "- **LSTM** (1997): First gated RNN, solved vanishing gradient\n",
    "- **GRU** (2014): Simplified LSTM, similar performance\n",
    "- **Today**: Both widely used, choice depends on task\n",
    "\n",
    "**GRU is the efficient alternative to LSTM!** ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
