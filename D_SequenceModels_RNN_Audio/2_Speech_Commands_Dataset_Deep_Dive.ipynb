{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Commands Dataset - Deep Dive with Visualizations\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll work with **real audio data** from the Speech Commands Dataset. Unlike our synthetic phoneme data from Notebook 1, this dataset contains actual human speech with natural co-articulation effects, making it perfect for demonstrating RNN superiority.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "1. Download and explore the Speech Commands Digits dataset\n",
    "2. Understand the complete audio processing pipeline\n",
    "3. Extract MFCC features from real speech\n",
    "4. Visualize temporal patterns in spoken digits\n",
    "5. Prepare data for RNN training\n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "**Speech Commands Dataset (Digits Subset)**\n",
    "- **Words**: \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"\n",
    "- **Size**: ~200MB (10 digits)\n",
    "- **Samples**: ~20,000 audio clips (~2,000 per digit)\n",
    "- **Duration**: 1 second per clip\n",
    "- **Sample Rate**: 16kHz\n",
    "- **Speakers**: Multiple speakers with various accents\n",
    "\n",
    "### Why This Dataset?\n",
    "\n",
    "Real speech has properties that make temporal modeling essential:\n",
    "- **Co-articulation**: Sounds blend together naturally\n",
    "- **Speaking rate variation**: Different speeds\n",
    "- **Accent diversity**: Different pronunciations\n",
    "- **Prosody**: Natural rhythm and intonation\n",
    "\n",
    "These properties create **ambiguous frames** that require temporal context to classify correctly - perfect for demonstrating RNN advantages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import IPython.display as ipd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Librosa version: {librosa.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Audio Processing Pipeline Overview\n",
    "\n",
    "Before downloading the data, let's understand the complete pipeline for converting raw audio to features suitable for RNN training.\n",
    "\n",
    "### Complete Pipeline Diagram\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    AUDIO PROCESSING PIPELINE                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Step 1: RAW AUDIO\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  \"seven.wav\"    â”‚  â€¢ Duration: 1 second\n",
    "â”‚  16,000 samples â”‚  â€¢ Sample rate: 16kHz\n",
    "â”‚  Mono channel   â”‚  â€¢ Format: WAV\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "Step 2: FRAMING\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Split into    â”‚  â€¢ Window: 25ms (400 samples)\n",
    "â”‚   overlapping   â”‚  â€¢ Hop: 10ms (160 samples)\n",
    "â”‚   frames        â”‚  â€¢ Result: ~100 frames\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "Step 3: WINDOWING\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Apply Hamming  â”‚  â€¢ Reduces spectral leakage\n",
    "â”‚  window to each â”‚  â€¢ Smooth frame boundaries\n",
    "â”‚  frame          â”‚  â€¢ w[n] = 0.54 - 0.46Â·cos(2Ï€n/N)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "Step 4: FFT (Fast Fourier Transform)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Time â†’ Freq    â”‚  â€¢ 512-point FFT\n",
    "â”‚  domain         â”‚  â€¢ Power spectrum\n",
    "â”‚  conversion     â”‚  â€¢ |FFT|Â² â†’ Energy per frequency\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "Step 5: MEL FILTERBANK\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Apply 40 mel   â”‚  â€¢ Mimics human hearing\n",
    "â”‚  filters        â”‚  â€¢ More resolution at low freq\n",
    "â”‚                 â”‚  â€¢ Less resolution at high freq\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "Step 6: LOG COMPRESSION\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  log(mel_spec)  â”‚  â€¢ Mimics loudness perception\n",
    "â”‚                 â”‚  â€¢ Compresses dynamic range\n",
    "â”‚                 â”‚  â€¢ logâ‚â‚€(x + Îµ)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "Step 7: DCT (Discrete Cosine Transform)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Decorrelate    â”‚  â€¢ Keep first 13 coefficients\n",
    "â”‚  features       â”‚  â€¢ MFCCs = DCT(log mel spectrum)\n",
    "â”‚                 â”‚  â€¢ Compact representation\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "FINAL OUTPUT\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  MFCC Matrix    â”‚  â€¢ Shape: (100 frames, 13 coeffs)\n",
    "â”‚  100 Ã— 13       â”‚  â€¢ Ready for RNN input\n",
    "â”‚                 â”‚  â€¢ Temporal sequence preserved\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "| Parameter | Value | Reason |\n",
    "|-----------|-------|--------|\n",
    "| Sample Rate | 16kHz | Standard for speech (captures up to 8kHz) |\n",
    "| Frame Length | 25ms | Captures phonetic information |\n",
    "| Frame Hop | 10ms | 60% overlap for smooth transitions |\n",
    "| FFT Size | 512 | Power of 2, sufficient resolution |\n",
    "| Mel Filters | 40 | Standard for speech recognition |\n",
    "| MFCC Coeffs | 13 | Captures essential spectral envelope |\n",
    "\n",
    "### Why MFCCs?\n",
    "\n",
    "MFCCs (Mel-Frequency Cepstral Coefficients) are ideal for speech because:\n",
    "1. **Perceptually motivated**: Mel scale matches human hearing\n",
    "2. **Compact**: 13 numbers capture essential information\n",
    "3. **Decorrelated**: DCT removes redundancy\n",
    "4. **Robust**: Less sensitive to noise than raw spectrum\n",
    "5. **Standard**: Widely used in speech recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Speech Commands Dataset\n",
    "\n",
    "We'll download only the digit folders to keep the size manageable (~200MB instead of 2GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path('speech_commands_data')\n",
    "DATASET_URL = 'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
    "DIGITS = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "\n",
    "def download_speech_commands():\n",
    "    \"\"\"Download and extract Speech Commands dataset.\"\"\"\n",
    "    \n",
    "    # Create data directory\n",
    "    DATA_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Check if already downloaded\n",
    "    if all((DATA_DIR / digit).exists() for digit in DIGITS):\n",
    "        print(\"âœ“ Dataset already downloaded!\")\n",
    "        return\n",
    "    \n",
    "    print(\"Downloading Speech Commands dataset...\")\n",
    "    print(\"This may take a few minutes (~2GB download)\")\n",
    "    \n",
    "    # Download\n",
    "    tar_path = DATA_DIR / 'speech_commands.tar.gz'\n",
    "    \n",
    "    if not tar_path.exists():\n",
    "        urllib.request.urlretrieve(DATASET_URL, tar_path)\n",
    "        print(\"âœ“ Download complete!\")\n",
    "    \n",
    "    # Extract only digit folders\n",
    "    print(\"\\nExtracting digit folders...\")\n",
    "    with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "        members = [m for m in tar.getmembers() \n",
    "                  if any(m.name.startswith(digit + '/') for digit in DIGITS)]\n",
    "        \n",
    "        for member in tqdm(members, desc=\"Extracting\"):\n",
    "            tar.extract(member, DATA_DIR)\n",
    "    \n",
    "    print(\"\\nâœ“ Extraction complete!\")\n",
    "    \n",
    "    # Clean up\n",
    "    if tar_path.exists():\n",
    "        tar_path.unlink()\n",
    "        print(\"âœ“ Cleaned up temporary files\")\n",
    "\n",
    "# Download dataset\n",
    "download_speech_commands()\n",
    "\n",
    "# Count files\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "for digit in DIGITS:\n",
    "    digit_path = DATA_DIR / digit\n",
    "    if digit_path.exists():\n",
    "        n_files = len(list(digit_path.glob('*.wav')))\n",
    "        print(f\"{digit:>6}: {n_files:>5} samples\")\n",
    "\n",
    "total_files = sum(len(list((DATA_DIR / digit).glob('*.wav'))) \n",
    "                 for digit in DIGITS if (DATA_DIR / digit).exists())\n",
    "print(\"=\"*50)\n",
    "print(f\"{'TOTAL':>6}: {total_files:>5} samples\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Raw Audio\n",
    "\n",
    "Let's load and visualize some audio samples to understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_random_sample(digit, sr=16000):\n",
    "    \"\"\"Load a random audio sample for a given digit.\"\"\"\n",
    "    digit_path = DATA_DIR / digit\n",
    "    audio_files = list(digit_path.glob('*.wav'))\n",
    "    \n",
    "    if not audio_files:\n",
    "        raise ValueError(f\"No audio files found for digit: {digit}\")\n",
    "    \n",
    "    # Pick random file\n",
    "    audio_file = np.random.choice(audio_files)\n",
    "    \n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(audio_file, sr=sr)\n",
    "    \n",
    "    return audio, sr, audio_file.name\n",
    "\n",
    "# Load samples for visualization\n",
    "sample_digits = ['zero', 'five', 'seven']\n",
    "samples = {}\n",
    "\n",
    "for digit in sample_digits:\n",
    "    audio, sr, filename = load_random_sample(digit)\n",
    "    samples[digit] = {'audio': audio, 'sr': sr, 'filename': filename}\n",
    "    print(f\"Loaded {digit}: {filename}\")\n",
    "\n",
    "print(f\"\\nâœ“ Loaded {len(samples)} samples\")\n",
    "print(f\"Sample rate: {sr} Hz\")\n",
    "print(f\"Duration: {len(audio)/sr:.2f} seconds\")\n",
    "print(f\"Samples per audio: {len(audio)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Listen to Audio Samples\n",
    "\n",
    "Let's listen to the actual audio to understand what we're analyzing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play audio samples\n",
    "for digit, data in samples.items():\n",
    "    print(f\"\\nðŸ”Š Playing: {digit.upper()}\")\n",
    "    print(f\"   File: {data['filename']}\")\n",
    "    display(ipd.Audio(data['audio'], rate=data['sr']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualize Waveforms\n",
    "\n",
    "Waveforms show amplitude over time - the raw representation of sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(sample_digits), 1, figsize=(14, 10))\n",
    "\n",
    "for idx, (digit, data) in enumerate(samples.items()):\n",
    "    audio = data['audio']\n",
    "    sr = data['sr']\n",
    "    \n",
    "    # Time axis\n",
    "    time = np.linspace(0, len(audio)/sr, len(audio))\n",
    "    \n",
    "    # Plot waveform\n",
    "    axes[idx].plot(time, audio, linewidth=0.5, alpha=0.8)\n",
    "    axes[idx].set_ylabel('Amplitude')\n",
    "    axes[idx].set_title(f'Waveform: \"{digit.upper()}\" - {data[\"filename\"]}', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_xlim([0, len(audio)/sr])\n",
    "    \n",
    "    # Add zero line\n",
    "    axes[idx].axhline(y=0, color='red', linestyle='--', alpha=0.3, linewidth=1)\n",
    "\n",
    "axes[-1].set_xlabel('Time (seconds)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Waveform Observations:\")\n",
    "print(\"  â€¢ Different digits have different temporal patterns\")\n",
    "print(\"  â€¢ Some digits are longer/shorter than others\")\n",
    "print(\"  â€¢ Amplitude varies throughout the utterance\")\n",
    "print(\"  â€¢ Silence at beginning/end of some samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Temporal Pattern Analysis\n",
    "\n",
    "Let's analyze how speech unfolds over time using a detailed example.\n",
    "\n",
    "### 4.1 Phonetic Breakdown of \"SEVEN\"\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              TEMPORAL BREAKDOWN: \"SEVEN\"                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Time:     0ms      200ms     400ms     600ms     800ms    1000ms\n",
    "          â”‚         â”‚         â”‚         â”‚         â”‚         â”‚\n",
    "          â–¼         â–¼         â–¼         â–¼         â–¼         â–¼\n",
    "Sound:   [s]   â†’   [e]   â†’   [v]   â†’   [É™]   â†’   [n]   â†’ [silence]\n",
    "         â”‚         â”‚         â”‚         â”‚         â”‚         â”‚\n",
    "Phoneme: /s/       /É›/       /v/       /É™/       /n/      \n",
    "         â”‚         â”‚         â”‚         â”‚         â”‚         â”‚\n",
    "Type:    Fric.     Vowel     Fric.     Vowel     Nasal    Silent\n",
    "         â”‚         â”‚         â”‚         â”‚         â”‚         â”‚\n",
    "Energy:  â–‚â–‚â–‚â–‚â–‚    â–…â–…â–…â–…â–…    â–ƒâ–ƒâ–ƒâ–ƒâ–ƒ    â–…â–…â–…â–…â–…    â–ƒâ–ƒâ–ƒâ–ƒâ–ƒ    â–â–â–â–â–\n",
    "         â”‚         â”‚         â”‚         â”‚         â”‚         â”‚\n",
    "Freq:    High      Mid       Low       Mid       Low      Silent\n",
    "         4-8kHz    1-3kHz    0.5-2kHz  1-3kHz    0.5-1kHz  \n",
    "         â”‚         â”‚         â”‚         â”‚         â”‚         â”‚\n",
    "MFCC:   Frame1    Frame2    Frame3    Frame4    Frame5   Frame6...\n",
    "        [13]      [13]      [13]      [13]      [13]     [13]\n",
    "         â”‚         â”‚         â”‚         â”‚         â”‚         â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                            Sequence for RNN\n",
    "```\n",
    "\n",
    "### Key Observations:\n",
    "\n",
    "1. **Temporal Structure**: Speech has clear temporal organization\n",
    "2. **Phoneme Transitions**: Sounds blend together (co-articulation)\n",
    "3. **Energy Variation**: Different phonemes have different energy levels\n",
    "4. **Frequency Content**: Each phoneme has characteristic frequencies\n",
    "5. **Sequential Nature**: Order matters - \"seven\" â‰  \"neves\"\n",
    "\n",
    "### Why RNNs Are Necessary:\n",
    "\n",
    "**Problem with Frame-by-Frame (DNN):**\n",
    "- Frame 2 (vowel /É›/) could be from \"seven\", \"ten\", \"eleven\", \"set\", etc.\n",
    "- Without context, impossible to disambiguate\n",
    "\n",
    "**Solution with Sequential (RNN):**\n",
    "- Frame 1 (/s/) + Frame 2 (/É›/) â†’ Likely \"seven\" or \"set\"\n",
    "- Frame 3 (/v/) confirms â†’ Must be \"seven\"!\n",
    "- Context resolves ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Visualize Spectrograms\n",
    "\n",
    "Spectrograms show frequency content over time - revealing the temporal structure of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(sample_digits), 1, figsize=(14, 12))\n",
    "\n",
    "for idx, (digit, data) in enumerate(samples.items()):\n",
    "    audio = data['audio']\n",
    "    sr = data['sr']\n",
    "    \n",
    "    # Compute spectrogram\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
    "    \n",
    "    # Plot\n",
    "    img = librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', \n",
    "                                    ax=axes[idx], cmap='viridis')\n",
    "    axes[idx].set_title(f'Spectrogram: \"{digit.upper()}\"', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Frequency (Hz)')\n",
    "    axes[idx].set_ylim([0, 8000])\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(img, ax=axes[idx], format='%+2.0f dB')\n",
    "\n",
    "axes[-1].set_xlabel('Time (seconds)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Spectrogram Observations:\")\n",
    "print(\"  â€¢ Horizontal bands = Formants (resonances of vocal tract)\")\n",
    "print(\"  â€¢ Bright regions = High energy at those frequencies\")\n",
    "print(\"  â€¢ Dark regions = Low energy\")\n",
    "print(\"  â€¢ Temporal patterns visible - different for each digit\")\n",
    "print(\"  â€¢ Transitions between phonemes create characteristic patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MFCC Feature Extraction\n",
    "\n",
    "Now let's extract MFCC features - the input for our RNN.\n",
    "\n",
    "### 5.1 MFCC Computation Steps\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  MFCC EXTRACTION PROCESS                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Input: audio[16000] (1 second at 16kHz)\n",
    "   â”‚\n",
    "   â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 1. Frame Audio  â”‚  window=25ms, hop=10ms\n",
    "â”‚    into windows â”‚  â†’ 100 frames\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 2. Apply        â”‚  Hamming window\n",
    "â”‚    Windowing    â”‚  â†’ Smooth boundaries\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 3. Compute FFT  â”‚  512-point FFT\n",
    "â”‚                 â”‚  â†’ Power spectrum\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 4. Mel Filter   â”‚  40 triangular filters\n",
    "â”‚    Bank         â”‚  â†’ Mel spectrum\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 5. Log          â”‚  logâ‚â‚€(mel_spec + Îµ)\n",
    "â”‚    Compression  â”‚  â†’ Log mel spectrum\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 6. DCT          â”‚  Keep first 13 coeffs\n",
    "â”‚                 â”‚  â†’ 13 MFCCs\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "Output: mfcc[100, 13] (100 frames Ã— 13 coefficients)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(audio, sr=16000, n_mfcc=13, n_fft=512, hop_length=160):\n",
    "    \"\"\"Extract MFCC features from audio.\n",
    "    \n",
    "    Args:\n",
    "        audio: Audio signal\n",
    "        sr: Sample rate\n",
    "        n_mfcc: Number of MFCC coefficients\n",
    "        n_fft: FFT window size\n",
    "        hop_length: Hop length between frames\n",
    "    \n",
    "    Returns:\n",
    "        mfcc: MFCC features (n_frames, n_mfcc)\n",
    "    \"\"\"\n",
    "    # Extract MFCCs\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=audio,\n",
    "        sr=sr,\n",
    "        n_mfcc=n_mfcc,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length\n",
    "    )\n",
    "    \n",
    "    # Transpose to (time, features)\n",
    "    mfcc = mfcc.T\n",
    "    \n",
    "    return mfcc\n",
    "\n",
    "# Extract MFCCs for our samples\n",
    "mfccs = {}\n",
    "for digit, data in samples.items():\n",
    "    mfcc = extract_mfcc(data['audio'], data['sr'])\n",
    "    mfccs[digit] = mfcc\n",
    "    print(f\"{digit:>6}: MFCC shape = {mfcc.shape} (frames Ã— coefficients)\")\n",
    "\n",
    "print(\"\\nâœ“ MFCC extraction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualize MFCC Features\n",
    "\n",
    "MFCCs show the spectral envelope over time - the key features for speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(sample_digits), 1, figsize=(14, 12))\n",
    "\n",
    "for idx, (digit, mfcc) in enumerate(mfccs.items()):\n",
    "    # Plot MFCC heatmap\n",
    "    im = axes[idx].imshow(mfcc.T, aspect='auto', origin='lower', \n",
    "                          cmap='RdBu_r', interpolation='nearest')\n",
    "    axes[idx].set_ylabel('MFCC Coefficient')\n",
    "    axes[idx].set_title(f'MFCC Features: \"{digit.upper()}\"', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_yticks(range(13))\n",
    "    axes[idx].set_yticklabels([f'C{i}' for i in range(13)])\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(im, ax=axes[idx], label='MFCC Value')\n",
    "\n",
    "axes[-1].set_xlabel('Frame Number (time â†’)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š MFCC Observations:\")\n",
    "print(\"  â€¢ Each row = one MFCC coefficient over time\")\n",
    "print(\"  â€¢ Each column = one time frame (10ms)\")\n",
    "print(\"  â€¢ C0 (bottom) = Energy/loudness\")\n",
    "print(\"  â€¢ C1-C12 = Spectral shape information\")\n",
    "print(\"  â€¢ Temporal patterns visible - different for each digit\")\n",
    "print(\"  â€¢ This is the input sequence for our RNN!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Dataset for Training\n",
    "\n",
    "Now let's process all audio files and create train/validation/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_dir, digits, max_samples_per_digit=None, sr=16000):\n",
    "    \"\"\"Load and process entire dataset.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Path to data directory\n",
    "        digits: List of digit names\n",
    "        max_samples_per_digit: Maximum samples per digit (None = all)\n",
    "        sr: Sample rate\n",
    "    \n",
    "    Returns:\n",
    "        X: MFCC features (n_samples, n_frames, n_mfcc)\n",
    "        y: Labels (n_samples,)\n",
    "        label_map: Dictionary mapping digit names to indices\n",
    "    \"\"\"\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    label_map = {digit: idx for idx, digit in enumerate(digits)}\n",
    "    \n",
    "    print(\"Loading dataset...\")\n",
    "    for digit in digits:\n",
    "        digit_path = data_dir / digit\n",
    "        audio_files = list(digit_path.glob('*.wav'))\n",
    "        \n",
    "        # Limit samples if specified\n",
    "        if max_samples_per_digit:\n",
    "            audio_files = audio_files[:max_samples_per_digit]\n",
    "        \n",
    "        print(f\"  Processing {digit}: {len(audio_files)} samples\")\n",
    "        \n",
    "        for audio_file in tqdm(audio_files, desc=f\"  {digit}\", leave=False):\n",
    "            try:\n",
    "                # Load audio\n",
    "                audio, _ = librosa.load(audio_file, sr=sr)\n",
    "                \n",
    "                # Extract MFCCs\n",
    "                mfcc = extract_mfcc(audio, sr)\n",
    "                \n",
    "                # Store\n",
    "                X_list.append(mfcc)\n",
    "                y_list.append(label_map[digit])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error loading {audio_file.name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Convert to arrays\n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "    \n",
    "    return X, y, label_map\n",
    "\n",
    "# Load dataset (limit to 200 samples per digit for faster processing)\n",
    "print(\"Loading Speech Commands dataset...\")\n",
    "print(\"(Using 200 samples per digit for faster training)\\n\")\n",
    "\n",
    "X, y, label_map = load_dataset(DATA_DIR, DIGITS, max_samples_per_digit=200)\n",
    "\n",
    "print(f\"\\nâœ“ Dataset loaded!\")\n",
    "print(f\"  X shape: {X.shape} (samples, frames, mfcc_coeffs)\")\n",
    "print(f\"  y shape: {y.shape} (samples,)\")\n",
    "print(f\"  Label map: {label_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/validation/test splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: 80% train+val, 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: 75% train, 25% val (of the 80%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Dataset splits:\")\n",
    "print(f\"  Train: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Val:   {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Test:  {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass distribution (train):\")\n",
    "for digit, idx in label_map.items():\n",
    "    count = np.sum(y_train == idx)\n",
    "    print(f\"  {digit:>6}: {count:>3} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Processed Data\n",
    "\n",
    "Save the processed data for use in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "np.savez_compressed(\n",
    "    'speech_commands_processed.npz',\n",
    "    X_train=X_train,\n",
    "    X_val=X_val,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_val=y_val,\n",
    "    y_test=y_test,\n",
    "    label_map=label_map,\n",
    "    digits=DIGITS\n",
    ")\n",
    "\n",
    "print(\"âœ“ Processed data saved to 'speech_commands_processed.npz'\")\n",
    "print(\"\\nReady for RNN training in Notebook 2B!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Accomplished:\n",
    "\n",
    "âœ… **Downloaded real speech data** - Speech Commands digits (~200MB)  \n",
    "âœ… **Understood the pipeline** - From raw audio to MFCC features  \n",
    "âœ… **Visualized temporal patterns** - Waveforms, spectrograms, MFCCs  \n",
    "âœ… **Extracted features** - 13 MFCCs per frame, ~100 frames per audio  \n",
    "âœ… **Prepared dataset** - Train/val/test splits ready for training\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Real speech has temporal structure** - Phonemes unfold over time\n",
    "2. **Co-articulation creates ambiguity** - Individual frames are ambiguous\n",
    "3. **Context is essential** - Need previous frames to disambiguate\n",
    "4. **MFCCs capture essential information** - Compact, perceptually motivated\n",
    "5. **Sequences matter** - Order of frames is crucial\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In **Notebook 2B**, we'll:\n",
    "1. Train a DNN baseline (frame-by-frame classification)\n",
    "2. Train an RNN (sequential classification)\n",
    "3. Compare performance (expect 15-25% improvement)\n",
    "4. Analyze where each model succeeds/fails\n",
    "5. Visualize what the RNN learns\n",
    "\n",
    "**The real data will clearly demonstrate why RNNs are necessary for sequence tasks!** ðŸŽ¯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
