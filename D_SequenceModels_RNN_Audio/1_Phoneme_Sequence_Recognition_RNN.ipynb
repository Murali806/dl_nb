{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phoneme Sequence Recognition with Simple RNN\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook introduces **Recurrent Neural Networks (RNNs)** through a concrete audio problem: **phoneme sequence recognition**.\n",
    "\n",
    "### Learning Objectives\n",
    "1. Understand why sequence models are necessary for temporal audio data\n",
    "2. Learn about phonetics and co-articulation in speech\n",
    "3. Implement a simple RNN from scratch using NumPy\n",
    "4. Learn about activation functions in RNNs (tanh, softmax)\n",
    "5. Train an RNN on phoneme sequences\n",
    "6. Compare RNN performance with a baseline DNN\n",
    "7. Understand limitations that motivate LSTM/GRU\n",
    "\n",
    "### What are Phonemes?\n",
    "\n",
    "**Phonemes** are the basic units of sound in speech. For example:\n",
    "- \"cat\" = /k/ â†’ /Ã¦/ â†’ /t/\n",
    "- \"bat\" = /b/ â†’ /Ã¦/ â†’ /t/\n",
    "- \"hat\" = /h/ â†’ /Ã¦/ â†’ /t/\n",
    "\n",
    "Notice that the middle phoneme /Ã¦/ is the same, but the context (surrounding phonemes) differs.\n",
    "\n",
    "### Why This is a Sequence Problem\n",
    "\n",
    "**Key insight**: Phonemes occur in temporal order, and context matters!\n",
    "\n",
    "- A single audio frame could contain /Ã¦/ from \"cat\", \"bat\", or \"hat\"\n",
    "- To correctly identify the word, we need to know the previous phonemes\n",
    "- A standard DNN looking at single frames will fail\n",
    "- An RNN can maintain context through its hidden state\n",
    "\n",
    "### The Task\n",
    "\n",
    "**Input**: Sequence of audio frames (simplified \"MFCC\" features)  \n",
    "**Output**: Sequence of phoneme labels  \n",
    "**Architecture**: Many-to-many RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Phonetics Fundamentals and Co-articulation\n",
    "\n",
    "Before diving into RNNs, let's understand **why** temporal context is truly necessary in speech. The key lies in a phenomenon called **co-articulation**.\n",
    "\n",
    "### What is Co-articulation?\n",
    "\n",
    "**Co-articulation** is the phenomenon where phonemes in continuous speech **influence each other**. When we speak, phonemes don't occur in isolation - they blend and overlap in time.\n",
    "\n",
    "**Key Insight**: The acoustic features of one phoneme are affected by:\n",
    "- The phoneme that came before it\n",
    "- The phoneme that comes after it  \n",
    "- The speaking rate and style\n",
    "\n",
    "This means that **the same phoneme sounds different in different contexts**!\n",
    "\n",
    "### Example: /Ã¦/ in \"cat\", \"bat\", \"hat\"\n",
    "\n",
    "Even though we write /Ã¦/ as the same phoneme, the **actual acoustic signal is different** in each word due to the preceding consonant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Understanding Three Consonants\n",
    "\n",
    "Let's examine the phonetic properties of /k/, /b/, and /h/:\n",
    "\n",
    "#### Voiceless Velar Stop: /k/ (as in \"cat\")\n",
    "\n",
    "**Articulatory Description:**\n",
    "- **Place**: Velar (back of tongue touches soft palate)\n",
    "- **Manner**: Stop (complete closure, then sudden release)\n",
    "- **Voicing**: Voiceless (vocal cords don't vibrate)\n",
    "\n",
    "**Acoustic Characteristics:**\n",
    "- Sharp burst of noise at release\n",
    "- High-frequency energy spike (2-4 kHz)\n",
    "- No voicing during closure (silence)\n",
    "- Short duration (~50-100ms)\n",
    "\n",
    "**Effect on following /Ã¦/:**\n",
    "- Tongue must move from back to front position\n",
    "- Creates characteristic F2 (second formant) transition\n",
    "- /Ã¦/ starts with residual high-frequency energy\n",
    "\n",
    "---\n",
    "\n",
    "#### Voiced Bilabial Stop: /b/ (as in \"bat\")\n",
    "\n",
    "**Articulatory Description:**\n",
    "- **Place**: Bilabial (both lips close together)\n",
    "- **Manner**: Stop (complete closure, then release)\n",
    "- **Voicing**: Voiced (vocal cords vibrate during closure)\n",
    "\n",
    "**Acoustic Characteristics:**\n",
    "- Voice bar during closure (low-frequency energy ~100-150 Hz)\n",
    "- Weaker burst than /k/ at release\n",
    "- Voicing continues through closure\n",
    "- Longer duration (~80-120ms)\n",
    "\n",
    "**Effect on following /Ã¦/:**\n",
    "- Lips already in front position\n",
    "- Smoother F2 transition (less dramatic than /k/)\n",
    "- /Ã¦/ starts with ongoing voicing from /b/\n",
    "- Low-frequency energy carries over\n",
    "\n",
    "---\n",
    "\n",
    "#### Voiceless Glottal Fricative: /h/ (as in \"hat\")\n",
    "\n",
    "**Articulatory Description:**\n",
    "- **Place**: Glottal (at the glottis/vocal cords)\n",
    "- **Manner**: Fricative (turbulent airflow, no complete closure)\n",
    "- **Voicing**: Voiceless (vocal cords apart, no vibration)\n",
    "\n",
    "**Acoustic Characteristics:**\n",
    "- Aspiration noise (breathy, turbulent sound)\n",
    "- Distributed energy across frequencies\n",
    "- No sharp burst (unlike stops)\n",
    "- Longer duration (~100-150ms)\n",
    "- Relatively flat spectrum\n",
    "\n",
    "**Effect on following /Ã¦/:**\n",
    "- No specific place of articulation to transition from\n",
    "- /Ã¦/ emerges gradually from aspiration noise\n",
    "- Formants appear progressively (not abruptly)\n",
    "- Breathy quality may persist into vowel onset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Visualizing Phonetic Differences\n",
    "\n",
    "Let's generate synthetic waveforms to illustrate these acoustic differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phoneme_waveform(phoneme_type, duration=0.15, sr=16000):\n",
    "    \"\"\"Generate synthetic phoneme waveforms for visualization.\n",
    "    \n",
    "    Args:\n",
    "        phoneme_type: 'k', 'b', or 'h'\n",
    "        duration: Duration in seconds\n",
    "        sr: Sample rate\n",
    "    \n",
    "    Returns:\n",
    "        waveform: Synthetic audio signal\n",
    "    \"\"\"\n",
    "    n_samples = int(duration * sr)\n",
    "    t = np.linspace(0, duration, n_samples)\n",
    "    \n",
    "    if phoneme_type == 'k':  # Voiceless velar stop\n",
    "        # Silence followed by sharp burst\n",
    "        silence_duration = 0.05\n",
    "        burst_duration = 0.02\n",
    "        \n",
    "        silence_samples = int(silence_duration * sr)\n",
    "        burst_samples = int(burst_duration * sr)\n",
    "        \n",
    "        silence = np.zeros(silence_samples)\n",
    "        # High-frequency burst\n",
    "        burst = np.random.randn(burst_samples) * 0.8\n",
    "        # Apply high-pass filter effect\n",
    "        burst = np.convolve(burst, [1, -0.95], mode='same')\n",
    "        \n",
    "        remaining = n_samples - silence_samples - burst_samples\n",
    "        tail = np.zeros(remaining)\n",
    "        \n",
    "        waveform = np.concatenate([silence, burst, tail])\n",
    "        \n",
    "    elif phoneme_type == 'b':  # Voiced bilabial stop\n",
    "        # Voice bar followed by weaker burst\n",
    "        voice_bar_duration = 0.06\n",
    "        burst_duration = 0.015\n",
    "        \n",
    "        voice_bar_samples = int(voice_bar_duration * sr)\n",
    "        burst_samples = int(burst_duration * sr)\n",
    "        \n",
    "        # Low-frequency voicing (F0 ~ 120 Hz)\n",
    "        t_voice = np.linspace(0, voice_bar_duration, voice_bar_samples)\n",
    "        voice_bar = 0.3 * np.sin(2 * np.pi * 120 * t_voice)\n",
    "        \n",
    "        # Weaker burst\n",
    "        burst = np.random.randn(burst_samples) * 0.4\n",
    "        \n",
    "        remaining = n_samples - voice_bar_samples - burst_samples\n",
    "        tail = np.zeros(remaining)\n",
    "        \n",
    "        waveform = np.concatenate([voice_bar, burst, tail])\n",
    "        \n",
    "    elif phoneme_type == 'h':  # Voiceless glottal fricative\n",
    "        # Aspiration noise with gradual onset\n",
    "        noise = np.random.randn(n_samples) * 0.4\n",
    "        \n",
    "        # Apply envelope for gradual onset/offset\n",
    "        envelope = np.ones(n_samples)\n",
    "        fade_in = int(0.03 * sr)\n",
    "        fade_out = int(0.03 * sr)\n",
    "        \n",
    "        envelope[:fade_in] = np.linspace(0, 1, fade_in)\n",
    "        envelope[-fade_out:] = np.linspace(1, 0, fade_out)\n",
    "        \n",
    "        waveform = noise * envelope\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown phoneme type: {phoneme_type}\")\n",
    "    \n",
    "    return waveform\n",
    "\n",
    "# Generate waveforms\n",
    "sr = 16000\n",
    "duration = 0.15\n",
    "\n",
    "waveform_k = generate_phoneme_waveform('k', duration, sr)\n",
    "waveform_b = generate_phoneme_waveform('b', duration, sr)\n",
    "waveform_h = generate_phoneme_waveform('h', duration, sr)\n",
    "\n",
    "# Time axis\n",
    "t = np.linspace(0, duration, len(waveform_k))\n",
    "\n",
    "# Plot waveforms\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# /k/ waveform\n",
    "axes[0].plot(t * 1000, waveform_k, linewidth=0.5, color='#1f77b4')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].set_title('/k/ - Voiceless Velar Stop (Sharp burst after silence)', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xlim([0, duration * 1000])\n",
    "axes[0].axvline(x=50, color='red', linestyle='--', alpha=0.5, label='Burst onset')\n",
    "axes[0].legend()\n",
    "\n",
    "# /b/ waveform\n",
    "axes[1].plot(t * 1000, waveform_b, linewidth=0.5, color='#ff7f0e')\n",
    "axes[1].set_ylabel('Amplitude')\n",
    "axes[1].set_title('/b/ - Voiced Bilabial Stop (Voice bar + weaker burst)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim([0, duration * 1000])\n",
    "axes[1].axvspan(0, 60, alpha=0.2, color='green', label='Voice bar')\n",
    "axes[1].legend()\n",
    "\n",
    "# /h/ waveform\n",
    "axes[2].plot(t * 1000, waveform_h, linewidth=0.5, color='#2ca02c')\n",
    "axes[2].set_xlabel('Time (ms)')\n",
    "axes[2].set_ylabel('Amplitude')\n",
    "axes[2].set_title('/h/ - Voiceless Glottal Fricative (Aspiration noise)', fontsize=12, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_xlim([0, duration * 1000])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Synthetic phoneme waveforms generated\")\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"  /k/: Silence â†’ Sharp burst (high energy)\")\n",
    "print(\"  /b/: Voice bar (periodic) â†’ Weaker burst\")\n",
    "print(\"  /h/: Continuous aspiration noise (aperiodic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Spectral Analysis\n",
    "\n",
    "Let's examine the frequency content of these phonemes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "# Compute spectrograms\n",
    "def compute_spectrogram(waveform, sr, nperseg=256):\n",
    "    \"\"\"Compute spectrogram using STFT.\"\"\"\n",
    "    f, t, Sxx = signal.spectrogram(waveform, sr, nperseg=nperseg, noverlap=nperseg//2)\n",
    "    return f, t, 10 * np.log10(Sxx + 1e-10)  # Convert to dB\n",
    "\n",
    "f_k, t_k, Sxx_k = compute_spectrogram(waveform_k, sr)\n",
    "f_b, t_b, Sxx_b = compute_spectrogram(waveform_b, sr)\n",
    "f_h, t_h, Sxx_h = compute_spectrogram(waveform_h, sr)\n",
    "\n",
    "# Plot spectrograms\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# /k/ spectrogram\n",
    "im1 = axes[0].pcolormesh(t_k * 1000, f_k, Sxx_k, shading='gouraud', cmap='viridis')\n",
    "axes[0].set_ylabel('Frequency (Hz)')\n",
    "axes[0].set_title('/k/ Spectrogram - High-frequency burst', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylim([0, 8000])\n",
    "plt.colorbar(im1, ax=axes[0], label='Power (dB)')\n",
    "\n",
    "# /b/ spectrogram\n",
    "im2 = axes[1].pcolormesh(t_b * 1000, f_b, Sxx_b, shading='gouraud', cmap='viridis')\n",
    "axes[1].set_ylabel('Frequency (Hz)')\n",
    "axes[1].set_title('/b/ Spectrogram - Voice bar (low freq) + burst', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylim([0, 8000])\n",
    "plt.colorbar(im2, ax=axes[1], label='Power (dB)')\n",
    "\n",
    "# /h/ spectrogram\n",
    "im3 = axes[2].pcolormesh(t_h * 1000, f_h, Sxx_h, shading='gouraud', cmap='viridis')\n",
    "axes[2].set_xlabel('Time (ms)')\n",
    "axes[2].set_ylabel('Frequency (Hz)')\n",
    "axes[2].set_title('/h/ Spectrogram - Distributed aspiration noise', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylim([0, 8000])\n",
    "plt.colorbar(im3, ax=axes[2], label='Power (dB)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Spectrograms computed\")\n",
    "print(\"\\nSpectral Characteristics:\")\n",
    "print(\"  /k/: Vertical burst line (broadband, high-frequency emphasis)\")\n",
    "print(\"  /b/: Horizontal voice bar at low frequencies + burst\")\n",
    "print(\"  /h/: Diffuse energy across frequencies (no clear structure)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Co-articulation Effect\n",
    "\n",
    "Now let's demonstrate how these different consonants affect the following vowel /Ã¦/:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vowel_with_coarticulation(preceding_consonant, duration=0.2, sr=16000):\n",
    "    \"\"\"Generate vowel /Ã¦/ with co-articulation effects from preceding consonant.\n",
    "    \n",
    "    Args:\n",
    "        preceding_consonant: 'k', 'b', or 'h'\n",
    "        duration: Duration in seconds\n",
    "        sr: Sample rate\n",
    "    \n",
    "    Returns:\n",
    "        waveform: Synthetic vowel with co-articulation\n",
    "    \"\"\"\n",
    "    n_samples = int(duration * sr)\n",
    "    t = np.linspace(0, duration, n_samples)\n",
    "    \n",
    "    # Base vowel /Ã¦/ with formants\n",
    "    # F1 ~ 700 Hz, F2 ~ 1700 Hz, F3 ~ 2500 Hz\n",
    "    F0 = 120  # Fundamental frequency\n",
    "    \n",
    "    # Generate harmonic series\n",
    "    vowel = np.zeros(n_samples)\n",
    "    for harmonic in range(1, 20):\n",
    "        freq = F0 * harmonic\n",
    "        if freq < sr / 2:  # Nyquist limit\n",
    "            # Amplitude depends on proximity to formants\n",
    "            amp = 0.1 / harmonic\n",
    "            if 600 < freq < 800:  # Near F1\n",
    "                amp *= 3\n",
    "            elif 1500 < freq < 1900:  # Near F2\n",
    "                amp *= 2.5\n",
    "            elif 2300 < freq < 2700:  # Near F3\n",
    "                amp *= 1.5\n",
    "            \n",
    "            vowel += amp * np.sin(2 * np.pi * freq * t)\n",
    "    \n",
    "    # Apply co-articulation effects\n",
    "    transition_duration = 0.05  # 50ms transition\n",
    "    transition_samples = int(transition_duration * sr)\n",
    "    \n",
    "    if preceding_consonant == 'k':\n",
    "        # Add high-frequency noise at onset (from velar burst)\n",
    "        noise = np.random.randn(transition_samples) * 0.2\n",
    "        noise = np.convolve(noise, [1, -0.9], mode='same')  # High-pass\n",
    "        vowel[:transition_samples] += noise * np.linspace(1, 0, transition_samples)\n",
    "        \n",
    "        # F2 transition (back to front)\n",
    "        f2_mod = np.linspace(1200, 1700, transition_samples)\n",
    "        for i, freq in enumerate(f2_mod):\n",
    "            vowel[i] += 0.3 * np.sin(2 * np.pi * freq * t[i])\n",
    "    \n",
    "    elif preceding_consonant == 'b':\n",
    "        # Smoother onset with voicing continuation\n",
    "        voice_continuation = 0.2 * np.sin(2 * np.pi * F0 * t[:transition_samples])\n",
    "        vowel[:transition_samples] += voice_continuation * np.linspace(0.5, 0, transition_samples)\n",
    "        \n",
    "        # F2 transition (front to front, smoother)\n",
    "        f2_mod = np.linspace(1600, 1700, transition_samples)\n",
    "        for i, freq in enumerate(f2_mod):\n",
    "            vowel[i] += 0.2 * np.sin(2 * np.pi * freq * t[i])\n",
    "    \n",
    "    elif preceding_consonant == 'h':\n",
    "        # Gradual formant emergence from aspiration\n",
    "        aspiration = np.random.randn(transition_samples) * 0.15\n",
    "        vowel[:transition_samples] += aspiration * np.linspace(0.8, 0, transition_samples)\n",
    "        \n",
    "        # Gradual formant strengthening\n",
    "        vowel[:transition_samples] *= np.linspace(0.3, 1, transition_samples)\n",
    "    \n",
    "    # Normalize\n",
    "    vowel = vowel / np.max(np.abs(vowel)) * 0.8\n",
    "    \n",
    "    return vowel\n",
    "\n",
    "# Generate vowels with co-articulation\n",
    "vowel_after_k = generate_vowel_with_coarticulation('k', duration=0.2, sr=sr)\n",
    "vowel_after_b = generate_vowel_with_coarticulation('b', duration=0.2, sr=sr)\n",
    "vowel_after_h = generate_vowel_with_coarticulation('h', duration=0.2, sr=sr)\n",
    "\n",
    "# Compute spectrograms\n",
    "f_vk, t_vk, Sxx_vk = compute_spectrogram(vowel_after_k, sr, nperseg=512)\n",
    "f_vb, t_vb, Sxx_vb = compute_spectrogram(vowel_after_b, sr, nperseg=512)\n",
    "f_vh, t_vh, Sxx_vh = compute_spectrogram(vowel_after_h, sr, nperseg=512)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "# /k/ â†’ /Ã¦/\n",
    "im1 = axes[0].pcolormesh(t_vk * 1000, f_vk, Sxx_vk, shading='gouraud', cmap='viridis')\n",
    "axes[0].set_ylabel('Frequency (Hz)')\n",
    "axes[0].set_title('/k/ â†’ /Ã¦/ (\"cat\") - Note F2 transition from back', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylim([0, 4000])\n",
    "axes[0].axvline(x=50, color='red', linestyle='--', alpha=0.7, linewidth=2, label='Transition region')\n",
    "axes[0].legend()\n",
    "plt.colorbar(im1, ax=axes[0], label='Power (dB)')\n",
    "\n",
    "# /b/ â†’ /Ã¦/\n",
    "im2 = axes[1].pcolormesh(t_vb * 1000, f_vb, Sxx_vb, shading='gouraud', cmap='viridis')\n",
    "axes[1].set_ylabel('Frequency (Hz)')\n",
    "axes[1].set_title('/b/ â†’ /Ã¦/ (\"bat\") - Smoother transition, voicing continues', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylim([0, 4000])\n",
    "axes[1].axvline(x=50, color='red', linestyle='--', alpha=0.7, linewidth=2, label='Transition region')\n",
    "axes[1].legend()\n",
    "plt.colorbar(im2, ax=axes[1], label='Power (dB)')\n",
    "\n",
    "# /h/ â†’ /Ã¦/\n",
    "im3 = axes[2].pcolormesh(t_vh * 1000, f_vh, Sxx_vh, shading='gouraud', cmap='viridis')\n",
    "axes[2].set_xlabel('Time (ms)')\n",
    "axes[2].set_ylabel('Frequency (Hz)')\n",
    "axes[2].set_title('/h/ â†’ /Ã¦/ (\"hat\") - Gradual formant emergence', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylim([0, 4000])\n",
    "axes[2].axvline(x=50, color='red', linestyle='--', alpha=0.7, linewidth=2, label='Transition region')\n",
    "axes[2].legend()\n",
    "plt.colorbar(im3, ax=axes[2], label='Power (dB)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Co-articulation effects visualized\")\n",
    "print(\"\\nðŸ”‘ KEY INSIGHT:\")\n",
    "print(\"   The SAME phoneme /Ã¦/ has DIFFERENT acoustic onset in each context!\")\n",
    "print(\"   This is why temporal context (RNN) is necessary for accurate recognition.\")\n",
    "print(\"\\nTransition Differences:\")\n",
    "print(\"  After /k/: Abrupt F2 transition from back position + high-freq noise\")\n",
    "print(\"  After /b/: Smooth transition + voicing continuation\")\n",
    "print(\"  After /h/: Gradual formant emergence from aspiration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Why DNNs Fail and RNNs Succeed\n",
    "\n",
    "**The Problem with Frame-by-Frame Analysis:**\n",
    "\n",
    "If we look at a single audio frame containing /Ã¦/:\n",
    "- **After /k/**: Frame has high-frequency residue + F2 transition\n",
    "- **After /b/**: Frame has voicing continuation + smooth F2\n",
    "- **After /h/**: Frame has aspiration residue + emerging formants\n",
    "\n",
    "**Without temporal context**, these frames are **ambiguous** - we can't tell which word we're hearing!\n",
    "\n",
    "**With RNN's hidden state**, the network:\n",
    "1. Remembers seeing /k/, /b/, or /h/ in the previous timestep\n",
    "2. Uses this context to correctly interpret the current /Ã¦/ frame\n",
    "3. Resolves the ambiguity\n",
    "\n",
    "This is the fundamental reason why **RNNs outperform DNNs on sequence tasks**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Activation Functions for RNNs\n",
    "\n",
    "Now that we understand why temporal context matters, let's learn about the activation functions used in RNNs.\n",
    "\n",
    "### Why tanh for Hidden States?\n",
    "\n",
    "In RNNs, we use **tanh (hyperbolic tangent)** for the hidden state activation:\n",
    "\n",
    "$$h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$\n",
    "\n",
    "**Properties of tanh**:\n",
    "- Range: [-1, 1]\n",
    "- Symmetric around zero\n",
    "- Allows both positive and negative values\n",
    "- Better gradient flow than sigmoid\n",
    "\n",
    "**Why NOT ReLU?**\n",
    "- ReLU is unbounded [0, âˆž) â†’ can cause instability over time\n",
    "- ReLU can't represent negative temporal patterns\n",
    "- tanh provides stability with bounded output\n",
    "\n",
    "### Softmax for Output\n",
    "\n",
    "For the output layer (phoneme classification), we use **softmax**:\n",
    "\n",
    "$$y_t = \\text{softmax}(W_{hy} h_t + b_y)$$\n",
    "\n",
    "This converts scores to probabilities that sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def tanh(x):\n",
    "    \"\"\"Hyperbolic tangent activation function.\n",
    "    \n",
    "    Args:\n",
    "        x: Input array\n",
    "    \n",
    "    Returns:\n",
    "        Output in range [-1, 1]\n",
    "    \"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    \"\"\"Derivative of tanh.\n",
    "    \n",
    "    Args:\n",
    "        x: Input array (output of tanh)\n",
    "    \n",
    "    Returns:\n",
    "        Gradient\n",
    "    \"\"\"\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax activation function.\n",
    "    \n",
    "    Args:\n",
    "        x: Input array of shape (n_classes,) or (batch_size, n_classes)\n",
    "    \n",
    "    Returns:\n",
    "        Probabilities that sum to 1\n",
    "    \"\"\"\n",
    "    # Subtract max for numerical stability\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# Visualize activation functions\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# tanh\n",
    "axes[0].plot(x, tanh(x), linewidth=2, label='tanh(x)')\n",
    "axes[0].plot(x, tanh_derivative(x), linewidth=2, label=\"tanh'(x)\", linestyle='--')\n",
    "axes[0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "axes[0].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('tanh Activation (RNN Hidden State)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# ReLU for comparison\n",
    "relu = np.maximum(0, x)\n",
    "axes[1].plot(x, relu, linewidth=2, label='ReLU(x)', color='orange')\n",
    "axes[1].plot(x, tanh(x), linewidth=2, label='tanh(x)', alpha=0.7)\n",
    "axes[1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "axes[1].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].set_title('ReLU vs tanh (Why tanh for RNNs?)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Softmax example\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "probs = softmax(logits)\n",
    "axes[2].bar(['Phoneme 1', 'Phoneme 2', 'Phoneme 3'], probs, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[2].set_ylabel('Probability')\n",
    "axes[2].set_title('Softmax Output (Phoneme Probabilities)')\n",
    "axes[2].set_ylim([0, 1])\n",
    "for i, p in enumerate(probs):\n",
    "    axes[2].text(i, p + 0.02, f'{p:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Activation functions defined and visualized\")\n",
    "print(f\"  - tanh range: [{tanh(-10):.3f}, {tanh(10):.3f}]\")\n",
    "print(f\"  - Softmax output sums to: {np.sum(probs):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Phoneme Dataset\n",
    "\n",
    "For learning purposes, we'll create a simplified synthetic dataset that simulates co-articulation effects.\n",
    "\n",
    "### Dataset Design\n",
    "\n",
    "- **Phonemes**: 5 phonemes {/a/, /e/, /i/, /o/, /u/}\n",
    "- **Sequences**: Random patterns\n",
    "- **Features**: 8-dimensional \"audio\" features (simplified MFCCs)\n",
    "- **Temporal dependency**: Each phoneme's features depend on the previous phoneme (co-articulation)\n",
    "\n",
    "The key is the **0.7/0.3 mixing ratio**:\n",
    "```python\n",
    "features[t] = 0.7 * current_phoneme + 0.3 * previous_phoneme\n",
    "```\n",
    "\n",
    "This simulates real co-articulation where the current sound is influenced by the previous sound!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhonemeDataset:\n",
    "    \"\"\"Generate synthetic phoneme sequences for RNN training.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_phonemes=5, feature_dim=8, seq_length=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_phonemes: Number of unique phonemes\n",
    "            feature_dim: Dimension of audio features\n",
    "            seq_length: Length of each sequence\n",
    "        \"\"\"\n",
    "        self.n_phonemes = n_phonemes\n",
    "        self.feature_dim = feature_dim\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Phoneme names\n",
    "        self.phoneme_names = ['a', 'e', 'i', 'o', 'u'][:n_phonemes]\n",
    "        \n",
    "        # Create unique \"signature\" for each phoneme\n",
    "        # Each phoneme has a characteristic feature pattern\n",
    "        self.phoneme_signatures = np.random.randn(n_phonemes, feature_dim)\n",
    "        \n",
    "        # Normalize signatures\n",
    "        self.phoneme_signatures = self.phoneme_signatures / np.linalg.norm(\n",
    "            self.phoneme_signatures, axis=1, keepdims=True\n",
    "        )\n",
    "    \n",
    "    def generate_sequence(self):\n",
    "        \"\"\"Generate a single phoneme sequence.\n",
    "        \n",
    "        Returns:\n",
    "            features: (seq_length, feature_dim) array\n",
    "            labels: (seq_length,) array of phoneme indices\n",
    "        \"\"\"\n",
    "        # Random phoneme sequence\n",
    "        labels = np.random.randint(0, self.n_phonemes, size=self.seq_length)\n",
    "        \n",
    "        features = np.zeros((self.seq_length, self.feature_dim))\n",
    "        \n",
    "        for t in range(self.seq_length):\n",
    "            # Base feature from phoneme signature\n",
    "            base_feature = self.phoneme_signatures[labels[t]]\n",
    "            \n",
    "            # Add temporal dependency: current features depend on previous phoneme\n",
    "            # This simulates CO-ARTICULATION!\n",
    "            if t > 0:\n",
    "                prev_feature = self.phoneme_signatures[labels[t-1]]\n",
    "                # Mix current and previous (co-articulation effect)\n",
    "                features[t] = 0.7 * base_feature + 0.3 * prev_feature\n",
    "            else:\n",
    "                features[t] = base_feature\n",
    "            \n",
    "            # Add noise\n",
    "            features[t] += np.random.randn(self.feature_dim) * 0.1\n",
    "        \n",
    "        return features, labels\n",
    "    \n",
    "    def generate_dataset(self, n_sequences):\n",
    "        \"\"\"Generate multiple sequences.\n",
    "        \n",
    "        Args:\n",
    "            n_sequences: Number of sequences to generate\n",
    "        \n",
    "        Returns:\n",
    "            X: (n_sequences, seq_length, feature_dim) array\n",
    "            y: (n_sequences, seq_length) array\n",
    "        \"\"\"\n",
    "        X = np.zeros((n_sequences, self.seq_length, self.feature_dim))\n",
    "        y = np.zeros((n_sequences, self.seq_length), dtype=int)\n",
    "        \n",
    "        for i in range(n_sequences):\n",
    "            X[i], y[i] = self.generate_sequence()\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "# Create dataset\n",
    "dataset = PhonemeDataset(n_phonemes=5, feature_dim=8, seq_length=10)\n",
    "\n",
    "# Generate training and validation data\n",
    "X_train, y_train = dataset.generate_dataset(n_sequences=500)\n",
    "X_val, y_val = dataset.generate_dataset(n_sequences=100)\n",
    "\n",
    "print(\"Dataset generated successfully!\")\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  X_train shape: {X_train.shape} (n_sequences, seq_length, feature_dim)\")\n",
    "print(f\"  y_train shape: {y_train.shape} (n_sequences, seq_length)\")\n",
    "print(f\"\\nValidation set:\")\n",
    "print(f\"  X_val shape: {X_val.shape}\")\n",
    "print(f\"  y_val shape: {y_val.shape}\")\n",
    "print(f\"\\nPhonemes: {dataset.phoneme_names}\")\n",
    "print(f\"\\nðŸ”‘ Co-articulation: Each frame = 70% current + 30% previous phoneme\")\n",
    "\n",
    "# Visualize a sample sequence\n",
    "sample_idx = 0\n",
    "sample_features = X_train[sample_idx]\n",
    "sample_labels = y_train[sample_idx]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 6))\n",
    "\n",
    "# Plot features\n",
    "im = axes[0].imshow(sample_features.T, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "axes[0].set_xlabel('Time Step')\n",
    "axes[0].set_ylabel('Feature Dimension')\n",
    "axes[0].set_title('Sample Phoneme Sequence Features (with Co-articulation)')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Plot labels\n",
    "axes[1].plot(sample_labels, 'o-', markersize=10, linewidth=2)\n",
    "axes[1].set_xlabel('Time Step')\n",
    "axes[1].set_ylabel('Phoneme Index')\n",
    "axes[1].set_title('Phoneme Labels Over Time')\n",
    "axes[1].set_yticks(range(dataset.n_phonemes))\n",
    "axes[1].set_yticklabels(dataset.phoneme_names)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add phoneme names as text\n",
    "for t, label in enumerate(sample_labels):\n",
    "    axes[1].text(t, label + 0.15, dataset.phoneme_names[label], \n",
    "                ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSample sequence: {[dataset.phoneme_names[l] for l in sample_labels]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple RNN Implementation from Scratch\n",
    "\n",
    "Now let's implement a simple RNN using pure NumPy.\n",
    "\n",
    "### RNN Architecture\n",
    "\n",
    "At each timestep $t$:\n",
    "\n",
    "1. **Hidden state update**:\n",
    "   $$h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$\n",
    "\n",
    "2. **Output computation**:\n",
    "   $$y_t = \\text{softmax}(W_{hy} h_t + b_y)$$\n",
    "\n",
    "Where:\n",
    "- $x_t$: input at time $t$ (audio features)\n",
    "- $h_t$: hidden state at time $t$ (memory/context)\n",
    "- $y_t$: output at time $t$ (phoneme probabilities)\n",
    "- $W_{xh}$: input-to-hidden weights\n",
    "- $W_{hh}$: hidden-to-hidden weights (recurrent connection)\n",
    "- $W_{hy}$: hidden-to-output weights\n",
    "- $b_h$, $b_y$: biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    \"\"\"Simple RNN implementation from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Dimension of input features\n",
    "            hidden_dim: Dimension of hidden state\n",
    "            output_dim: Number of output classes (phonemes)\n",
    "            learning_rate: Learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights (Xavier initialization)\n",
    "        self.W_xh = np.random.randn(hidden_dim, input_dim) * np.sqrt(2.0 / input_dim)\n",
    "        self.W_hh = np.random.randn(hidden_dim, hidden_dim) * np.sqrt(2.0 / hidden_dim)\n",
    "        self.W_hy = np.random.randn(output_dim, hidden_dim) * np.sqrt(2.0 / hidden_dim)\n",
    "        \n",
    "        # Initialize biases\n",
    "        self.b_h = np.zeros((hidden_dim, 1))\n",
    "        self.b_y = np.zeros((output_dim, 1))\n",
    "        \n",
    "        # For tracking training\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through the RNN.\n",
    "        \n",
    "        Args:\n",
    "            X: Input sequence of shape (seq_length, input_dim)\n",
    "        \n",
    "        Returns:\n",
    "            outputs: List of output probabilities at each timestep\n",
    "            hidden_states: List of hidden states at each timestep\n",
    "        \"\"\"\n",
    "        seq_length = X.shape[0]\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        h = np.zeros((self.hidden_dim, 1))\n",
    "        \n",
    "        hidden_states = []\n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_length):\n",
    "            # Get input at time t\n",
    "            x_t = X[t].reshape(-1, 1)  # (input_dim, 1)\n",
    "            \n",
    "            # Update hidden state\n",
    "            h = tanh(np.dot(self.W_hh, h) + np.dot(self.W_xh, x_t) + self.b_h)\n",
    "            \n",
    "            # Compute output\n",
    "            logits = np.dot(self.W_hy, h) + self.b_y\n",
    "            y_t = softmax(logits.flatten())\n",
    "            \n",
    "            hidden_states.append(h.copy())\n",
    "            outputs.append(y_t)\n",
    "        \n",
    "        return outputs, hidden_states\n",
    "    \n",
    "    def compute_loss(self, outputs, targets):\n",
    "        \"\"\"Compute cross-entropy loss.\n",
    "        \n",
    "        Args:\n",
    "            outputs: List of output probabilities\n",
    "            targets: Array of target labels (seq_length,)\n",
    "        \n",
    "        Returns:\n",
    "            loss: Average cross-entropy loss\n",
    "        \"\"\"\n",
    "        seq_length = len(outputs)\n",
    "        loss = 0.0\n",
    "        \n",
    "        for t in range(seq_length):\n",
    "            # Cross-entropy loss at timestep t\n",
    "            loss += -np.log(outputs[t][targets[t]] + 1e-8)\n",
    "        \n",
    "        return loss / seq_length\n",
    "    \n",
    "    def backward(self, X, targets, outputs, hidden_states):\n",
    "        \"\"\"Backpropagation through time (BPTT) - simplified version.\n",
    "        \n",
    "        Args:\n",
    "            X: Input sequence\n",
    "            targets: Target labels\n",
    "            outputs: Forward pass outputs\n",
    "            hidden_states: Forward pass hidden states\n",
    "        \n",
    "        Returns:\n",
    "            Gradients for all parameters\n",
    "        \"\"\"\n",
    "        seq_length = X.shape[0]\n",
    "        \n",
    "        # Initialize gradients\n",
    "        dW_xh = np.zeros_like(self.W_xh)\n",
    "        dW_hh = np.zeros_like(self.W_hh)\n",
    "        dW_hy = np.zeros_like(self.W_hy)\n",
    "        db_h = np.zeros_like(self.b_h)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        \n",
    "        # Gradient of hidden state from next timestep\n",
    "        dh_next = np.zeros((self.hidden_dim, 1))\n",
    "        \n",
    "        # Backward pass through time\n",
    "        for t in reversed(range(seq_length)):\n",
    "            # Output gradient\n",
    "            dy = outputs[t].copy()\n",
    "            dy[targets[t]] -= 1  # Softmax + cross-entropy gradient\n",
    "            dy = dy.reshape(-1, 1) / seq_length  # Average over sequence\n",
    "            \n",
    "            # Gradients for output layer\n",
    "            dW_hy += np.dot(dy, hidden_states[t].T)\n",
    "            db_y += dy\n",
    "            \n",
    "            # Gradient flowing back to hidden state\n",
    "            dh = np.dot(self.W_hy.T, dy) + dh_next\n",
    "            \n",
    "            # Gradient through tanh\n",
    "            dh_raw = dh * (1 - hidden_states[t] ** 2)\n",
    "            \n",
    "            # Gradients for hidden layer\n",
    "            x_t = X[t].reshape(-1, 1)\n",
    "            dW_xh += np.dot(dh_raw, x_t.T)\n",
    "            db_h += dh_raw\n",
    "            \n",
    "            if t > 0:\n",
    "                dW_hh += np.dot(dh_raw, hidden_states[t-1].T)\n",
    "                dh_next = np.dot(self.W_hh.T, dh_raw)\n",
    "            else:\n",
    "                h_prev = np.zeros((self.hidden_dim, 1))\n",
    "                dW_hh += np.dot(dh_raw, h_prev.T)\n",
    "        \n",
    "        # Gradient clipping (prevent exploding gradients)\n",
    "        for grad in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n",
    "            np.clip(grad, -5, 5, out=grad)\n",
    "        \n",
    "        return dW_xh, dW_hh, dW_hy, db_h, db_y\n",
    "    \n",
    "    def update_parameters(self, dW_xh, dW_hh, dW_hy, db_h, db_y):\n",
    "        \"\"\"Update parameters using gradients.\"\"\"\n",
    "        self.W_xh -= self.learning_rate * dW_xh\n",
    "        self.W_hh -= self.learning_rate * dW_hh\n",
    "        self.W_hy -= self.learning_rate * dW_hy\n",
    "        self.b_h -= self.learning_rate * db_h\n",
    "        self.b_y -= self.learning_rate * db_y\n",
    "    \n",
    "    def train_step(self, X, y):\n",
    "        \"\"\"Single training step.\n",
    "        \n",
    "        Args:\n",
    "            X: Input sequence\n",
    "            y: Target labels\n",
    "        \n",
    "        Returns:\n",
    "            loss: Training loss\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        outputs, hidden_states = self.forward(X)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.compute_loss(outputs, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        grads = self.backward(X, y, outputs, hidden_states)\n",
    "        \n",
    "        # Update parameters\n",
    "        self.update_parameters(*grads)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\n",
    "        \n",
    "        Args:\n",
    "            X: Input sequence\n",
    "        \n",
    "        Returns:\n",
    "            predictions: Array of predicted labels\n",
    "        \"\"\"\n",
    "        outputs, _ = self.forward(X)\n",
    "        predictions = np.array([np.argmax(out) for out in outputs])\n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"Evaluate on a dataset.\n",
    "        \n",
    "        Args:\n",
    "            X: Input sequences (n_sequences, seq_length, input_dim)\n",
    "            y: Target labels (n_sequences, seq_length)\n",
    "        \n",
    "        Returns:\n",
    "            avg_loss: Average loss\n",
    "            accuracy: Frame-level accuracy\n",
