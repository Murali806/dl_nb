{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp: Root Mean Square Propagation\n",
    "\n",
    "## üéØ What This Notebook Covers\n",
    "\n",
    "**RMSProp** fixes AdaGrad's diminishing learning rate problem by using exponentially weighted averages. In this notebook, we explore:\n",
    "\n",
    "1. ‚úÖ **The Problem** - AdaGrad's limitation\n",
    "2. ‚úÖ **RMSProp Solution** - Exponentially weighted averages\n",
    "3. ‚úÖ **Mathematical Formulation** - How RMSProp works\n",
    "4. ‚úÖ **Implementation** - RMSProp from scratch\n",
    "5. ‚úÖ **Performance Comparison** - RMSProp vs AdaGrad vs SGD\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "- **Fixes AdaGrad**: Learning rate doesn't vanish üîß\n",
    "- **Better for Deep Networks**: Works well in practice üèóÔ∏è\n",
    "- **Foundation for Adam**: Key component of Adam optimizer ‚≠ê\n",
    "\n",
    "Let's master RMSProp! üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Problem: AdaGrad's Diminishing Learning Rate\n",
    "\n",
    "### Recap: AdaGrad\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G_t &= G_{t-1} + g_t^2 \\quad \\text{(accumulate squared gradients)} \\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\frac{\\alpha}{\\sqrt{G_t + \\epsilon}} \\cdot g_t\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### The Problem\n",
    "\n",
    "```\n",
    "G_t only grows (never shrinks)\n",
    "    ‚Üì\n",
    "Effective LR = Œ± / sqrt(G_t)\n",
    "    ‚Üì\n",
    "As G_t ‚Üí ‚àû, Effective LR ‚Üí 0\n",
    "    ‚Üì\n",
    "Training stops! ‚ùå\n",
    "```\n",
    "\n",
    "### RMSProp's Solution\n",
    "\n",
    "**Key Idea**: Use **exponentially weighted average** instead of sum!\n",
    "\n",
    "$$\n",
    "E[g^2]_t = \\beta E[g^2]_{t-1} + (1-\\beta) g_t^2\n",
    "$$\n",
    "\n",
    "This way:\n",
    "- Recent gradients have more weight\n",
    "- Old gradients fade away\n",
    "- Learning rate doesn't vanish! ‚úÖ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mathematical Formulation\n",
    "\n",
    "### RMSProp Update Rule\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[g^2]_t &= \\beta E[g^2]_{t-1} + (1-\\beta) g_t^2 \\quad \\text{(exponentially weighted average)} \\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\frac{\\alpha}{\\sqrt{E[g^2]_t + \\epsilon}} \\cdot g_t \\quad \\text{(parameter update)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $g_t = \\nabla L(\\theta_t)$ = gradient at time $t$\n",
    "- $E[g^2]_t$ = exponentially weighted average of squared gradients\n",
    "- $\\beta$ = decay rate (typically 0.9 or 0.999)\n",
    "- $\\alpha$ = learning rate (e.g., 0.001)\n",
    "- $\\epsilon$ = small constant for numerical stability (e.g., $10^{-8}$)\n",
    "\n",
    "### Comparison: AdaGrad vs RMSProp\n",
    "\n",
    "| Aspect | AdaGrad | RMSProp |\n",
    "|--------|---------|----------|\n",
    "| Gradient Accumulation | $G_t = G_{t-1} + g_t^2$ | $E[g^2]_t = \\beta E[g^2]_{t-1} + (1-\\beta) g_t^2$ |\n",
    "| Type | Sum (monotonic) | Exponential average |\n",
    "| Learning Rate | Diminishes to 0 | Stays active |\n",
    "| Best For | Sparse features | Deep networks |\n",
    "\n",
    "### Why \"Root Mean Square\"?\n",
    "\n",
    "The denominator is the **root mean square** of gradients:\n",
    "\n",
    "$$\n",
    "\\text{RMS}(g) = \\sqrt{E[g^2]_t}\n",
    "$$\n",
    "\n",
    "Hence the name: **RMS**Prop (Root Mean Square Propagation)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize: AdaGrad vs RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate gradient history\n",
    "np.random.seed(42)\n",
    "iterations = 200\n",
    "gradients = np.random.randn(iterations) * 0.5 + 0.2\n",
    "\n",
    "# AdaGrad accumulation\n",
    "G_adagrad = np.zeros(iterations)\n",
    "for t in range(iterations):\n",
    "    if t == 0:\n",
    "        G_adagrad[t] = gradients[t]**2\n",
    "    else:\n",
    "        G_adagrad[t] = G_adagrad[t-1] + gradients[t]**2\n",
    "\n",
    "# RMSProp accumulation\n",
    "beta = 0.9\n",
    "E_rmsprop = np.zeros(iterations)\n",
    "for t in range(iterations):\n",
    "    if t == 0:\n",
    "        E_rmsprop[t] = gradients[t]**2\n",
    "    else:\n",
    "        E_rmsprop[t] = beta * E_rmsprop[t-1] + (1 - beta) * gradients[t]**2\n",
    "\n",
    "# Compute effective learning rates\n",
    "alpha = 0.01\n",
    "epsilon = 1e-8\n",
    "lr_adagrad = alpha / np.sqrt(G_adagrad + epsilon)\n",
    "lr_rmsprop = alpha / np.sqrt(E_rmsprop + epsilon)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Accumulated values\n",
    "axes[0].plot(G_adagrad, linewidth=2.5, label='AdaGrad (Sum)', color='#FF6B6B')\n",
    "axes[0].plot(E_rmsprop, linewidth=2.5, label='RMSProp (Exp. Avg)', color='#4ECDC4')\n",
    "axes[0].set_xlabel('Iteration', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Accumulated Squared Gradients', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Gradient Accumulation', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Effective learning rates\n",
    "axes[1].plot(lr_adagrad, linewidth=2.5, label='AdaGrad (Vanishes)', color='#FF6B6B')\n",
    "axes[1].plot(lr_rmsprop, linewidth=2.5, label='RMSProp (Stable)', color='#4ECDC4')\n",
    "axes[1].set_xlabel('Iteration', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Effective Learning Rate', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Learning Rate Evolution', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observations:\")\n",
    "print(\"  ‚Ä¢ AdaGrad: Accumulation grows unbounded ‚Üí LR vanishes\")\n",
    "print(\"  ‚Ä¢ RMSProp: Accumulation stabilizes ‚Üí LR stays active\")\n",
    "print(\"  ‚Ä¢ RMSProp fixes the diminishing LR problem!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spiral_data(n_samples=300, noise=0.1):\n",
    "    \"\"\"\n",
    "    Generate spiral dataset for binary classification.\n",
    "    \n",
    "    Returns:\n",
    "    - X: Features (n_x, m)\n",
    "    - Y: Labels (1, m)\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    m = n_samples\n",
    "    \n",
    "    # Create spiral\n",
    "    theta = np.linspace(0, 4*np.pi, m//2)\n",
    "    r = np.linspace(0.5, 2, m//2)\n",
    "    \n",
    "    # Class 0: spiral\n",
    "    X_class0 = np.vstack([r * np.cos(theta), r * np.sin(theta)])\n",
    "    Y_class0 = np.zeros((1, m//2))\n",
    "    \n",
    "    # Class 1: spiral (rotated)\n",
    "    X_class1 = np.vstack([r * np.cos(theta + np.pi), r * np.sin(theta + np.pi)])\n",
    "    Y_class1 = np.ones((1, m//2))\n",
    "    \n",
    "    # Combine\n",
    "    X = np.hstack([X_class0, X_class1])\n",
    "    Y = np.hstack([Y_class0, Y_class1])\n",
    "    \n",
    "    # Add noise\n",
    "    X += np.random.randn(*X.shape) * noise\n",
    "    \n",
    "    # Shuffle\n",
    "    indices = np.random.permutation(m)\n",
    "    X = X[:, indices]\n",
    "    Y = Y[:, indices]\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "# Generate data\n",
    "X, Y = generate_spiral_data(n_samples=300, noise=0.1)\n",
    "\n",
    "print(f\"Dataset shape: X={X.shape}, Y={Y.shape}\")\n",
    "print(f\"Number of samples: {X.shape[1]}\")\n",
    "print(f\"Number of features: {X.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Neural Network with RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"Derivative of ReLU.\"\"\"\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "print(\"‚úÖ Activation functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp:\n",
    "    \"\"\"\n",
    "    Neural network with RMSProp optimizer.\n",
    "    \n",
    "    Architecture: Input (2) ‚Üí Hidden (10, ReLU) ‚Üí Output (1, Sigmoid)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_x=2, n_h=10, n_y=1, learning_rate=0.001, \n",
    "                 beta=0.9, epsilon=1e-8, random_seed=42):\n",
    "        \"\"\"\n",
    "        Initialize neural network with RMSProp.\n",
    "        \n",
    "        Parameters:\n",
    "        - learning_rate: Learning rate (Œ±)\n",
    "        - beta: Decay rate for exponential average (typically 0.9 or 0.999)\n",
    "        - epsilon: Small constant for numerical stability\n",
    "        \"\"\"\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        self.lr = learning_rate\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.W1 = np.random.randn(n_h, n_x) * 0.1\n",
    "        self.b1 = np.zeros((n_h, 1))\n",
    "        self.W2 = np.random.randn(n_y, n_h) * 0.1\n",
    "        self.b2 = np.zeros((n_y, 1))\n",
    "        \n",
    "        # Initialize exponentially weighted averages of squared gradients\n",
    "        self.E_dW1 = np.zeros_like(self.W1)\n",
    "        self.E_db1 = np.zeros_like(self.b1)\n",
    "        self.E_dW2 = np.zeros_like(self.W2)\n",
    "        self.E_db2 = np.zeros_like(self.b2)\n",
    "        \n",
    "        # Training history\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"Forward propagation.\"\"\"\n",
    "        Z1 = self.W1 @ X + self.b1\n",
    "        A1 = relu(Z1)\n",
    "        Z2 = self.W2 @ A1 + self.b2\n",
    "        A2 = sigmoid(Z2)\n",
    "        \n",
    "        cache = {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n",
    "        return A2, cache\n",
    "    \n",
    "    def compute_loss(self, Y, A2):\n",
    "        \"\"\"Compute binary cross-entropy loss.\"\"\"\n",
    "        m = Y.shape[1]\n",
    "        loss = -np.mean(Y * np.log(A2 + 1e-8) + (1 - Y) * np.log(1 - A2 + 1e-8))\n",
    "        return loss\n",
    "    \n",
    "    def backward_propagation(self, X, Y, cache):\n",
    "        \"\"\"Backward propagation.\"\"\"\n",
    "        m = X.shape[1]\n",
    "        Z1, A1, Z2, A2 = cache['Z1'], cache['A1'], cache['Z2'], cache['A2']\n",
    "        \n",
    "        # Backprop\n",
    "        dZ2 = A2 - Y\n",
    "        dW2 = (1/m) * (dZ2 @ A1.T)\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        \n",
    "        dA1 = self.W2.T @ dZ2\n",
    "        dZ1 = dA1 * relu_derivative(Z1)\n",
    "        dW1 = (1/m) * (dZ1 @ X.T)\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def update_parameters_rmsprop(self, dW1, db1, dW2, db2):\n",
    "        \"\"\"\n",
    "        Update parameters using RMSProp.\n",
    "        \n",
    "        E[g^2]_t = Œ≤ * E[g^2]_{t-1} + (1-Œ≤) * g_t^2\n",
    "        Œ∏_t = Œ∏_{t-1} - (Œ± / sqrt(E[g^2]_t + Œµ)) * g_t\n",
    "        \"\"\"\n",
    "        # Update exponentially weighted averages of squared gradients\n",
    "        self.E_dW1 = self.beta * self.E_dW1 + (1 - self.beta) * dW1**2\n",
    "        self.E_db1 = self.beta * self.E_db1 + (1 - self.beta) * db1**2\n",
    "        self.E_dW2 = self.beta * self.E_dW2 + (1 - self.beta) * dW2**2\n",
    "        self.E_db2 = self.beta * self.E_db2 + (1 - self.beta) * db2**2\n",
    "        \n",
    "        # Update parameters with adaptive learning rates\n",
    "        self.W1 -= (self.lr / np.sqrt(self.E_dW1 + self.epsilon)) * dW1\n",
    "        self.b1 -= (self.lr / np.sqrt(self.E_db1 + self.epsilon)) * db1\n",
    "        self.W2 -= (self.lr / np.sqrt(self.E_dW2 + self.epsilon)) * dW2\n",
    "        self.b2 -= (self.lr / np.sqrt(self.E_db2 + self.epsilon)) * db2\n",
    "    \n",
    "    def compute_accuracy(self, X, Y):\n",
    "        \"\"\"Compute accuracy.\"\"\"\n",
    "        A2, _ = self.forward_propagation(X)\n",
    "        predictions = (A2 > 0.5).astype(int)\n",
    "        accuracy = np.mean(predictions == Y)\n",
    "        return accuracy\n",
    "    \n",
    "    def fit(self, X, Y, epochs=1000, verbose=False):\n",
    "        \"\"\"Train the network with RMSProp.\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            A2, cache = self.forward_propagation(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(Y, A2)\n",
    "            self.losses.append(loss)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            accuracy = self.compute_accuracy(X, Y)\n",
    "            self.accuracies.append(accuracy)\n",
    "            \n",
    "            # Backward propagation\n",
    "            dW1, db1, dW2, db2 = self.backward_propagation(X, Y, cache)\n",
    "            \n",
    "            # Update parameters with RMSProp\n",
    "            self.update_parameters_rmsprop(dW1, db1, dW2, db2)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch + 1) % 200 == 0:\n",
    "                print(f\"Epoch {epoch+1:4d}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n‚úÖ Training Complete!\")\n",
    "            print(f\"   Final Loss: {self.losses[-1]:.4f}\")\n",
    "            print(f\"   Final Accuracy: {self.accuracies[-1]:.4f}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "print(\"‚úÖ RMSProp class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison: RMSProp vs AdaGrad vs SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need AdaGrad and SGD for comparison\n",
    "class AdaGrad:\n",
    "    \"\"\"AdaGrad for comparison.\"\"\"\n",
    "    def __init__(self, n_x=2, n_h=10, n_y=1, learning_rate=0.1, epsilon=1e-8, random_seed=42):\n",
    "        np.random.seed(random_seed)\n",
    "        self.lr = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.W1 = np.random.randn(n_h, n_x) * 0.1\n",
    "        self.b1 = np.zeros((n_h, 1))\n",
    "        self.W2 = np.random.randn(n_y, n_h) * 0.1\n",
    "        self.b2 = np.zeros((n_y, 1))\n",
    "        self.G_dW1 = np.zeros_like(self.W1)\n",
    "        self.G_db1 = np.zeros_like(self.b1)\n",
    "        self.G_dW2 = np.zeros_like(self.W2)\n",
    "        self.G_db2 = np.zeros_like(self.b2)\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        Z1 = self.W1 @ X + self.b1\n",
    "        A1 = relu(Z1)\n",
    "        Z2 = self.W2 @ A1 + self.b2\n",
    "        A2 = sigmoid(Z2)\n",
    "        return A2, {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n",
    "    \n",
    "    def compute_loss(self, Y, A2):\n",
    "        return -np.mean(Y * np.log(A2 + 1e-8) + (1 - Y) * np.log(1 - A2 + 1e-8))\n",
    "    \n",
    "    def backward_propagation(self, X, Y, cache):\n",
    "        m = X.shape[1]\n",
    "        Z1, A1, A2 = cache['Z1'], cache['A1'], cache['A2']\n",
    "        dZ2 = A2 - Y\n",
    "        dW2 = (1/m) * (dZ2 @ A1.T)\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        dA1 = self.W2.T @ dZ2\n",
    "        dZ1 = dA1 * relu_derivative(Z1)\n",
    "        dW1 = (1/m) * (dZ1 @ X.T)\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def compute_accuracy(self, X, Y):\n",
    "        A2, _ = self.forward_propagation(X)\n",
    "        return np.mean((A2 > 0.5).astype(int) == Y)\n",
    "    \n",
    "    def fit(self, X, Y, epochs=1000, verbose=False):\n",
    "        for epoch in range(epochs):\n",
    "            A2, cache = self.forward_propagation(X)\n",
    "            self.losses.append(self.compute_loss(Y, A2))\n",
    "            self.accuracies.append(self.compute_accuracy(X, Y))\n",
    "            dW1, db1, dW2, db2 = self.backward_propagation(X, Y, cache)\n",
    "            self.G_dW1 += dW1**2\n",
    "            self.G_db1 += db1**2\n",
    "            self.G_dW2 += dW2**2\n",
    "            self.G_db2 += db2**2\n",
    "            self.W1 -= (self.lr / np.sqrt(self.G_dW1 + self.epsilon)) * dW1\n",
    "            self.b1 -= (self.lr / np.sqrt(self.G_db1 + self.epsilon)) * db1\n",
    "            self.W2 -= (self.lr / np.sqrt(self.G_dW2 + self.epsilon)) * dW2\n",
    "            self.b2 -= (self.lr / np.sqrt(self.G_db2 + self.epsilon)) * db2\n",
    "        return self\n",
    "\n",
    "class VanillaSGD:\n",
    "    \"\"\"Vanilla SGD for comparison.\"\"\"\n",
    "    def __init__(self, n_x=2, n_h=10, n_y=1, learning_rate=0.01, random_seed=42):\n",
    "        np.random.seed(random_seed)\n",
    "        self.lr = learning_rate\n",
    "        self.W1 = np.random.randn(n_h, n_x) * 0.1\n",
    "        self.b1 = np.zeros((n_h, 1))\n",
    "        self.W2 = np.random.randn(n_y, n_h) * 0.1\n",
    "        self.b2 = np.zeros((n_y, 1))\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        Z1 = self.W1 @ X + self.b1\n",
    "        A1 = relu(Z1)\n",
    "        Z2 = self.W2 @ A1 + self.b2\n",
    "        A2 = sigmoid(Z2)\n",
    "        return A2, {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n",
    "    \n",
    "    def compute_loss(self, Y, A2):\n",
    "        return -np.mean(Y * np.log(A2 + 1e-8) + (1 - Y) * np.log(1 - A2 + 1e-8))\n",
    "    \n",
    "    def backward_propagation(self, X, Y, cache):\n",
    "        m = X.shape[1]\n",
    "        Z1, A1, A2 = cache['Z1'], cache['A1'], cache['A2']\n",
    "        dZ2 = A2 - Y\n",
    "        dW2 = (1/m) * (dZ2 @ A1.T)\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        dA1 = self.W2.T @ dZ2\n",
    "        dZ1 = dA1 * relu_derivative(Z1)\n",
    "        dW1 = (1/m) * (dZ1 @ X.T)\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def compute_accuracy(self, X, Y):\n",
    "        A2, _ = self.forward_propagation(X)\n",
    "        return np.mean((A2 > 0.5).astype(int) == Y)\n",
    "    \n",
    "    def fit(self, X, Y, epochs=1000, verbose=False):\n",
    "        for epoch in range(epochs):\n",
    "            A2, cache = self.forward_propagation(X)\n",
    "            self.losses.append(self.compute_loss(Y, A2))\n",
    "            self.accuracies.append(self.compute_accuracy(X, Y))\n",
    "            dW1, db1, dW2, db2 = self.backward_propagation(X, Y, cache)\n",
    "            self.W1 -= self.lr * dW1\n",
    "            self.b1 -= self.lr * db1\n",
    "            self.W2 -= self.lr * dW2\n",
    "            self.b2 -= self.lr * db2\n",
    "        return self\n",
    "\n",
    "print(\"‚úÖ Comparison classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 2000\n",
    "\n",
    "print(\"üî¨ Training Models...\\n\")\n",
    "\n",
    "# 1. Vanilla SGD\n",
    "print(\"1Ô∏è‚É£  Training Vanilla SGD...\")\n",
    "model_sgd = VanillaSGD(learning_rate=0.01, random_seed=42)\n",
    "model_sgd.fit(X, Y, epochs=epochs)\n",
    "print(f\"   Final Loss: {model_sgd.losses[-1]:.4f}\")\n",
    "\n",
    "# 2. AdaGrad\n",
    "print(\"\\n2Ô∏è‚É£  Training AdaGrad...\")\n",
    "model_adagrad = AdaGrad(learning_rate=0.1, random_seed=42)\n",
    "model_adagrad.fit(X, Y, epochs=epochs)\n",
    "print(f\"   Final Loss: {model_adagrad.losses[-1]:.4f}\")\n",
    "\n",
    "# 3. RMSProp\n",
    "print(\"\\n3Ô∏è‚É£  Training RMSProp...\")\n",
    "model_rmsprop = RMSProp(learning_rate=0.001, beta=0.9, random_seed=42)\n",
    "model_rmsprop.fit(X, Y, epochs=epochs)\n",
    "print(f\"   Final Loss: {model_rmsprop.losses[-1]:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ All experiments complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "plt.plot(model_sgd.losses, linewidth=2.5, label='Vanilla SGD', \n",
    "        color='#FF6B6B', alpha=0.8)\n",
    "plt.plot(model_adagrad.losses, linewidth=2.5, label='AdaGrad', \n",
    "        color='#4ECDC4', alpha=0.8)\n",
    "plt.plot(model_rmsprop.losses, linewidth=2.5, label='RMSProp', \n",
    "        color='#95E1D3', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('Loss', fontsize=13, fontweight='bold')\n",
    "plt.title('Loss Curves: RMSProp vs AdaGrad vs SGD', fontsize=15, fontweight='bold')\n",
    "plt.legend(fontsize=12, loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observations:\")\n",
    "print(\"  ‚Ä¢ RMSProp: Smooth, consistent convergence\")\n",
    "print(\"  ‚Ä¢ RMSProp: Doesn't suffer from diminishing LR\")\n",
    "print(\"  ‚Ä¢ RMSProp: Better than AdaGrad for long training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "‚úÖ **The Problem**\n",
    "- AdaGrad's learning rate vanishes over time\n",
    "- Caused by monotonically increasing accumulation\n",
    "- Problematic for deep networks and long training\n",
    "\n",
    "‚úÖ **RMSProp Solution**\n",
    "- Use exponentially weighted average instead of sum\n",
    "- Recent gradients have more weight\n",
    "- Old gradients fade away\n",
    "- Learning rate stays active!\n",
    "\n",
    "‚úÖ **Mathematical Foundation**\n",
    "- Exponential average: $E[g^2]_t = \\beta E[g^2]_{t-1} + (1-\\beta) g_t^2$\n",
    "- Adaptive update: $\\theta_t = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{E[g^2]_t + \\epsilon}} g_t$\n",
    "- Typical Œ≤: 0.9 or 0.999\n",
    "\n",
    "‚úÖ **Advantages**\n",
    "- Fixes AdaGrad's diminishing LR problem\n",
    "- Works well for deep networks\n",
    "- Stable and reliable\n",
    "- Foundation for Adam optimizer\n",
    "\n",
    "### When to Use RMSProp?\n",
    "\n",
    "**Good For:**\n",
    "- Deep neural networks\n",
    "- Long training runs\n",
    "- Non-convex optimization\n",
    "- Recurrent neural networks (RNNs)\n",
    "\n",
    "**Hyperparameters:**\n",
    "- Learning rate (Œ±): 0.001 (typical)\n",
    "- Beta (Œ≤): 0.9 or 0.999\n",
    "- Epsilon (Œµ): 1e-8\n",
    "\n",
    "### Connection to Other Notebooks\n",
    "\n",
    "This notebook builds on:\n",
    "- **`7_5_adagrad_adaptive_learning_rates.ipynb`**: AdaGrad and its limitations\n",
    "- **`7_4_sgd_with_momentum.ipynb`**: Exponentially weighted averages\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "üöÄ **Coming Next:**\n",
    "- **7.7 Adam**: Combines momentum + RMSProp (most popular optimizer!)\n",
    "\n",
    "---\n",
    "\n",
    "**üéì Congratulations!** You now understand RMSProp and how it fixes AdaGrad's problems!\n",
    "\n",
    "**Key Insight:** RMSProp uses exponentially weighted averages to keep the learning rate active throughout training!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
