{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization in Deep Neural Networks\n",
    "\n",
    "## ðŸŽ¯ The Power of Vectorization\n",
    "\n",
    "**Question**: Why do we use matrix operations instead of loops in neural networks?\n",
    "\n",
    "**Answer**: Vectorization makes neural networks **10-100x faster** and enables efficient batch processing!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ What This Notebook Covers\n",
    "\n",
    "1. âœ… What is vectorization and why it matters\n",
    "2. âœ… Vectorization basics with NumPy\n",
    "3. âœ… **Mathematical formulation of forward propagation (vectorized)**\n",
    "4. âœ… **Mathematical formulation of backpropagation (vectorized)**\n",
    "5. âœ… Complete implementation: loops vs vectorized\n",
    "6. âœ… Performance benchmarks\n",
    "7. âœ… Batch processing and mini-batch gradient descent\n",
    "8. âœ… Dimension analysis and debugging tips\n",
    "\n",
    "Let's unlock the power of vectorization! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is Vectorization?\n",
    "\n",
    "**Vectorization** is the process of replacing explicit loops with array operations.\n",
    "\n",
    "### Example: Computing dot product\n",
    "\n",
    "**Non-vectorized (Loop)**:\n",
    "```python\n",
    "result = 0\n",
    "for i in range(len(a)):\n",
    "    result += a[i] * b[i]\n",
    "```\n",
    "\n",
    "**Vectorized (NumPy)**:\n",
    "```python\n",
    "result = np.dot(a, b)\n",
    "```\n",
    "\n",
    "The vectorized version is:\n",
    "- âœ… **Faster** (uses optimized C/Fortran libraries)\n",
    "- âœ… **Cleaner** (more readable code)\n",
    "- âœ… **Parallelizable** (can use SIMD, GPU acceleration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Speed Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create large arrays\n",
    "n = 1000000\n",
    "a = np.random.randn(n)\n",
    "b = np.random.randn(n)\n",
    "\n",
    "# Method 1: Loop (non-vectorized)\n",
    "start = time.time()\n",
    "result_loop = 0\n",
    "for i in range(n):\n",
    "    result_loop += a[i] * b[i]\n",
    "time_loop = time.time() - start\n",
    "\n",
    "# Method 2: Vectorized\n",
    "start = time.time()\n",
    "result_vectorized = np.dot(a, b)\n",
    "time_vectorized = time.time() - start\n",
    "\n",
    "print(\"â±ï¸  Performance Comparison:\")\n",
    "print(f\"\\nLoop version:       {time_loop*1000:.2f} ms\")\n",
    "print(f\"Vectorized version: {time_vectorized*1000:.2f} ms\")\n",
    "print(f\"\\nðŸš€ Speedup: {time_loop/time_vectorized:.1f}x faster!\")\n",
    "print(f\"\\nResults match: {np.allclose(result_loop, result_vectorized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vectorization Basics\n",
    "\n",
    "### Key NumPy Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise operations\n",
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([5, 6, 7, 8])\n",
    "\n",
    "print(\"Element-wise Operations:\")\n",
    "print(f\"a + b = {a + b}\")\n",
    "print(f\"a * b = {a * b}\")\n",
    "print(f\"a ** 2 = {a ** 2}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "print(\"\\nMatrix Multiplication:\")\n",
    "print(f\"A @ B =\\n{A @ B}\")\n",
    "\n",
    "# Broadcasting\n",
    "C = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "d = np.array([10, 20, 30])\n",
    "\n",
    "print(\"\\nBroadcasting:\")\n",
    "print(f\"C =\\n{C}\")\n",
    "print(f\"d = {d}\")\n",
    "print(f\"C + d =\\n{C + d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting Rules\n",
    "\n",
    "NumPy automatically broadcasts arrays of different shapes:\n",
    "\n",
    "```\n",
    "Shape (2, 3) + Shape (3,)   â†’ Shape (2, 3)  âœ…\n",
    "Shape (2, 3) + Shape (2, 1) â†’ Shape (2, 3)  âœ…\n",
    "Shape (2, 3) + Shape (2,)   â†’ Error         âŒ\n",
    "```\n",
    "\n",
    "**Rule**: Dimensions are compatible if:\n",
    "1. They are equal, OR\n",
    "2. One of them is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting examples\n",
    "print(\"Broadcasting Examples:\\n\")\n",
    "\n",
    "# Example 1: Add scalar to matrix\n",
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"A + 10 =\\n{A + 10}\\n\")\n",
    "\n",
    "# Example 2: Add row vector to matrix\n",
    "row = np.array([10, 20, 30])\n",
    "print(f\"A + row =\\n{A + row}\\n\")\n",
    "\n",
    "# Example 3: Add column vector to matrix\n",
    "col = np.array([[100], [200]])\n",
    "print(f\"A + col =\\n{A + col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network Notation\n",
    "\n",
    "Before diving into vectorization, let's establish notation:\n",
    "\n",
    "### Single Sample Notation\n",
    "\n",
    "For a single training example:\n",
    "\n",
    "- $x \\in \\mathbb{R}^{n_x}$ - Input features (column vector)\n",
    "- $W^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times n^{[l-1]}}$ - Weight matrix for layer $l$\n",
    "- $b^{[l]} \\in \\mathbb{R}^{n^{[l]}}$ - Bias vector for layer $l$\n",
    "- $z^{[l]} \\in \\mathbb{R}^{n^{[l]}}$ - Linear activation for layer $l$\n",
    "- $a^{[l]} \\in \\mathbb{R}^{n^{[l]}}$ - Activation output for layer $l$\n",
    "\n",
    "### Batch (Vectorized) Notation\n",
    "\n",
    "For $m$ training examples:\n",
    "\n",
    "- $X \\in \\mathbb{R}^{n_x \\times m}$ - Input matrix (each column is one example)\n",
    "- $Z^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times m}$ - Linear activations for all examples\n",
    "- $A^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times m}$ - Activation outputs for all examples\n",
    "\n",
    "**Key Insight**: Capital letters denote matrices containing multiple examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Forward Propagation: Mathematical Formulation\n",
    "\n",
    "### Single Sample (Non-Vectorized)\n",
    "\n",
    "For one training example $x^{(i)}$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z^{[l](i)} &= W^{[l]} a^{[l-1](i)} + b^{[l]} \\quad \\text{(Linear transformation)} \\\\\n",
    "a^{[l](i)} &= g^{[l]}(z^{[l](i)}) \\quad \\text{(Activation function)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $a^{[0](i)} = x^{(i)}$ (input)\n",
    "- $g^{[l]}$ is the activation function (ReLU, tanh, sigmoid, etc.)\n",
    "\n",
    "### Vectorized Batch Form\n",
    "\n",
    "For $m$ training examples simultaneously:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z^{[l]} &= W^{[l]} A^{[l-1]} + b^{[l]} \\quad \\text{Shape: } (n^{[l]}, m) \\\\\n",
    "A^{[l]} &= g^{[l]}(Z^{[l]}) \\quad \\text{Shape: } (n^{[l]}, m)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $A^{[0]} = X$ (input matrix)\n",
    "- $b^{[l]}$ is broadcast across all $m$ examples\n",
    "- Each column of $Z^{[l]}$ and $A^{[l]}$ corresponds to one training example\n",
    "\n",
    "### Dimension Analysis\n",
    "\n",
    "$$\n",
    "\\underbrace{Z^{[l]}}_{(n^{[l]}, m)} = \\underbrace{W^{[l]}}_{(n^{[l]}, n^{[l-1]})} \\underbrace{A^{[l-1]}}_{(n^{[l-1]}, m)} + \\underbrace{b^{[l]}}_{(n^{[l]}, 1)}\n",
    "$$\n",
    "\n",
    "The bias $b^{[l]}$ is broadcast to shape $(n^{[l]}, m)$ automatically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Backpropagation: Mathematical Formulation\n",
    "\n",
    "### Single Sample Gradients\n",
    "\n",
    "For one training example, the gradients are:\n",
    "\n",
    "**Output Layer** ($l = L$):\n",
    "$$\n",
    "dz^{[L](i)} = a^{[L](i)} - y^{(i)} \\quad \\text{(for cross-entropy loss)}\n",
    "$$\n",
    "\n",
    "**Hidden Layers** ($l = L-1, L-2, \\ldots, 1$):\n",
    "$$\n",
    "\\begin{align}\n",
    "dz^{[l](i)} &= (W^{[l+1]})^T dz^{[l+1](i)} \\odot g'^{[l]}(z^{[l](i)}) \\\\\n",
    "dW^{[l]} &= dz^{[l](i)} (a^{[l-1](i)})^T \\\\\n",
    "db^{[l]} &= dz^{[l](i)} \\\\\n",
    "da^{[l-1](i)} &= (W^{[l]})^T dz^{[l](i)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $\\odot$ denotes element-wise multiplication.\n",
    "\n",
    "### Vectorized Batch Gradients\n",
    "\n",
    "For $m$ training examples simultaneously:\n",
    "\n",
    "**Output Layer** ($l = L$):\n",
    "$$\n",
    "dZ^{[L]} = A^{[L]} - Y \\quad \\text{Shape: } (n^{[L]}, m)\n",
    "$$\n",
    "\n",
    "**Hidden Layers** ($l = L-1, L-2, \\ldots, 1$):\n",
    "$$\n",
    "\\begin{align}\n",
    "dZ^{[l]} &= (W^{[l+1]})^T dZ^{[l+1]} \\odot g'^{[l]}(Z^{[l]}) \\quad \\text{Shape: } (n^{[l]}, m) \\\\\n",
    "dW^{[l]} &= \\frac{1}{m} dZ^{[l]} (A^{[l-1]})^T \\quad \\text{Shape: } (n^{[l]}, n^{[l-1]}) \\\\\n",
    "db^{[l]} &= \\frac{1}{m} \\sum_{i=1}^{m} dZ^{[l]} \\quad \\text{Shape: } (n^{[l]}, 1) \\\\\n",
    "dA^{[l-1]} &= (W^{[l]})^T dZ^{[l]} \\quad \\text{Shape: } (n^{[l-1]}, m)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Averaging**: We divide by $m$ when computing $dW^{[l]}$ and $db^{[l]}$ because we're averaging the gradients across all training examples.\n",
    "\n",
    "2. **Bias Gradient**: $db^{[l]} = \\frac{1}{m} \\sum_{i=1}^{m} dZ^{[l]}$ sums across the columns (axis=1) and averages.\n",
    "\n",
    "3. **Matrix Dimensions**: The transpose operations ensure dimensions match correctly.\n",
    "\n",
    "4. **Efficiency**: Computing gradients for all $m$ examples at once is much faster than looping!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dimension Analysis Table\n",
    "\n",
    "Understanding dimensions is crucial for debugging. Here's a complete reference:\n",
    "\n",
    "| Variable | Single Sample | Batch (m samples) | Description |\n",
    "|----------|---------------|-------------------|-------------|\n",
    "| $x$ | $(n_x, 1)$ | $X: (n_x, m)$ | Input features |\n",
    "| $W^{[l]}$ | $(n^{[l]}, n^{[l-1]})$ | $(n^{[l]}, n^{[l-1]})$ | Weights (same for batch) |\n",
    "| $b^{[l]}$ | $(n^{[l]}, 1)$ | $(n^{[l]}, 1)$ | Bias (broadcast to m) |\n",
    "| $z^{[l]}$ | $(n^{[l]}, 1)$ | $Z^{[l]}: (n^{[l]}, m)$ | Linear activation |\n",
    "| $a^{[l]}$ | $(n^{[l]}, 1)$ | $A^{[l]}: (n^{[l]}, m)$ | Activation output |\n",
    "| $dz^{[l]}$ | $(n^{[l]}, 1)$ | $dZ^{[l]}: (n^{[l]}, m)$ | Gradient of z |\n",
    "| $dW^{[l]}$ | $(n^{[l]}, n^{[l-1]})$ | $(n^{[l]}, n^{[l-1]})$ | Gradient of W |\n",
    "| $db^{[l]}$ | $(n^{[l]}, 1)$ | $(n^{[l]}, 1)$ | Gradient of b |\n",
    "\n",
    "**Memory Tip**: \n",
    "- Weights and biases have the same shape for single/batch\n",
    "- Activations and gradients gain an extra dimension $m$ for batches\n",
    "- Each column represents one training example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Implementation: Loop vs Vectorized\n",
    "\n",
    "Let's implement a simple 2-layer neural network both ways to see the difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Dataset: 100 samples, 3 features, binary classification\n",
    "m = 100  # number of samples\n",
    "n_x = 3  # input features\n",
    "n_h = 4  # hidden layer size\n",
    "n_y = 1  # output size\n",
    "\n",
    "# Generate random data\n",
    "X = np.random.randn(n_x, m)\n",
    "Y = (np.random.randn(n_y, m) > 0).astype(float)\n",
    "\n",
    "# Initialize parameters\n",
    "W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "b1 = np.zeros((n_h, 1))\n",
    "W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "b2 = np.zeros((n_y, 1))\n",
    "\n",
    "print(f\"Dataset: {m} samples, {n_x} features\")\n",
    "print(f\"Architecture: {n_x} â†’ {n_h} â†’ {n_y}\")\n",
    "print(f\"\\nShapes:\")\n",
    "print(f\"  X: {X.shape}\")\n",
    "print(f\"  Y: {Y.shape}\")\n",
    "print(f\"  W1: {W1.shape}, b1: {b1.shape}\")\n",
    "print(f\"  W2: {W2.shape}, b2: {b2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Derivative of sigmoid.\"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"Derivative of ReLU.\"\"\"\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "print(\"âœ… Activation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Loop-Based (Non-Vectorized)\n",
    "\n",
    "Process one example at a time using loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_loop(X, Y, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Forward propagation using loops (one example at a time).\n",
    "    \n",
    "    This is SLOW but easier to understand.\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Initialize outputs\n",
    "    Z1 = np.zeros((W1.shape[0], m))\n",
    "    A1 = np.zeros((W1.shape[0], m))\n",
    "    Z2 = np.zeros((W2.shape[0], m))\n",
    "    A2 = np.zeros((W2.shape[0], m))\n",
    "    \n",
    "    # Loop over each training example\n",
    "    for i in range(m):\n",
    "        # Get single example\n",
    "        x = X[:, i:i+1]  # Shape: (n_x, 1)\n",
    "        \n",
    "        # Layer 1\n",
    "        z1 = W1 @ x + b1\n",
    "        a1 = relu(z1)\n",
    "        \n",
    "        # Layer 2\n",
    "        z2 = W2 @ a1 + b2\n",
    "        a2 = sigmoid(z2)\n",
    "        \n",
    "        # Store results\n",
    "        Z1[:, i:i+1] = z1\n",
    "        A1[:, i:i+1] = a1\n",
    "        Z2[:, i:i+1] = z2\n",
    "        A2[:, i:i+1] = a2\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = -np.mean(Y * np.log(A2 + 1e-8) + (1 - Y) * np.log(1 - A2 + 1e-8))\n",
    "    \n",
    "    cache = {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n",
    "    return A2, loss, cache\n",
    "\n",
    "def backward_propagation_loop(X, Y, W1, b1, W2, b2, cache):\n",
    "    \"\"\"\n",
    "    Backward propagation using loops (one example at a time).\n",
    "    \n",
    "    This is SLOW but easier to understand.\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    Z1, A1, Z2, A2 = cache['Z1'], cache['A1'], cache['Z2'], cache['A2']\n",
    "    \n",
    "    # Initialize gradients\n",
    "    dW1 = np.zeros_like(W1)\n",
    "    db1 = np.zeros_like(b1)\n",
    "    dW2 = np.zeros_like(W2)\n",
    "    db2 = np.zeros_like(b2)\n",
    "    \n",
    "    # Loop over each training example\n",
    "    for i in range(m):\n",
    "        # Get single example\n",
    "        x = X[:, i:i+1]\n",
    "        y = Y[:, i:i+1]\n",
    "        a1 = A1[:, i:i+1]\n",
    "        a2 = A2[:, i:i+1]\n",
    "        z1 = Z1[:, i:i+1]\n",
    "        \n",
    "        # Backprop through layer 2\n",
    "        dz2 = a2 - y\n",
    "        dW2 += dz2 @ a1.T\n",
    "        db2 += dz2\n",
    "        \n",
    "        # Backprop through layer 1\n",
    "        da1 = W2.T @ dz2\n",
    "        dz1 = da1 * relu_derivative(z1)\n",
    "        dW1 += dz1 @ x.T\n",
    "        db1 += dz1\n",
    "    \n",
    "    # Average gradients\n",
    "    dW1 /= m\n",
    "    db1 /= m\n",
    "    dW2 /= m\n",
    "    db2 /= m\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "print(\"âœ… Loop-based functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Vectorized\n",
    "\n",
    "Process all examples simultaneously using matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_vectorized(X, Y, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Forward propagation using vectorization (all examples at once).\n",
    "    \n",
    "    This is FAST and efficient!\n",
    "    \"\"\"\n",
    "    # Layer 1: Z1 = W1 @ X + b1\n",
    "    Z1 = W1 @ X + b1  # Shape: (n_h, m)\n",
    "    A1 = relu(Z1)      # Shape: (n_h, m)\n",
    "    \n",
    "    # Layer 2: Z2 = W2 @ A1 + b2\n",
    "    Z2 = W2 @ A1 + b2  # Shape: (n_y, m)\n",
    "    A2 = sigmoid(Z2)   # Shape: (n_y, m)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = -np.mean(Y * np.log(A2 + 1e-8) + (1 - Y) * np.log(1 - A2 + 1e-8))\n",
    "    \n",
    "    cache = {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n",
    "    return A2, loss, cache\n",
    "\n",
    "def backward_propagation_vectorized(X, Y, W1, b1, W2, b2, cache):\n",
    "    \"\"\"\n",
    "    Backward propagation using vectorization (all examples at once).\n",
    "    \n",
    "    This is FAST and efficient!\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    Z1, A1, Z2, A2 = cache['Z1'], cache['A1'], cache['Z2'], cache['A2']\n",
    "    \n",
    "    # Backprop through layer 2\n",
    "    dZ2 = A2 - Y                    # Shape: (n_y, m)\n",
    "    dW2 = (1/m) * (dZ2 @ A1.T)      # Shape: (n_y, n_h)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)  # Shape: (n_y, 1)\n",
    "    \n",
    "    # Backprop through layer 1\n",
    "    dA1 = W2.T @ dZ2                # Shape: (n_h, m)\n",
    "    dZ1 = dA1 * relu_derivative(Z1) # Shape: (n_h, m)\n",
    "    dW1 = (1/m) * (dZ1 @ X.T)       # Shape: (n_h, n_x)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)  # Shape: (n_h, 1)\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "print(\"âœ… Vectorized functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loop-based version\n",
    "start = time.time()\n",
    "A2_loop, loss_loop, cache_loop = forward_propagation_loop(X, Y, W1, b1, W2, b2)\n",
    "dW1_loop, db1_loop, dW2_loop, db2_loop = backward_propagation_loop(X, Y, W1, b1, W2, b2, cache_loop)\n",
    "time_loop = time.time() - start\n",
    "\n",
    "# Test vectorized version\n",
    "start = time.time()\n",
    "A2_vec, loss_vec, cache_vec = forward_propagation_vectorized(X, Y, W1, b1, W2, b2)\n",
    "dW1_vec, db1_vec, dW2_vec, db2_vec = backward_propagation_vectorized(X, Y, W1, b1, W2, b2, cache_vec)\n",
    "time_vec = time.time() - start\n",
    "\n",
    "print(\"â±ï¸  Performance Comparison:\\n\")\n",
    "print(f\"Loop-based:  {time_loop*1000:.2f} ms\")\n",
    "print(f\"Vectorized:  {time_vec*1000:.2f} ms\")\n",
    "print(f\"\\nðŸš€ Speedup: {time_loop/time_vec:.1f}x faster!\")\n",
    "\n",
    "print(\"\\nâœ… Verification (results should match):\")\n",
    "print(f\"Loss match: {np.allclose(loss_loop, loss_vec)}\")\n",
    "print(f\"dW1 match:  {np.allclose(dW1_loop, dW1_vec)}\")\n",
    "print(f\"dW2 match:  {np.allclose(dW2_loop, dW2_vec)}\")\n",
    "print(f\"db1 match:  {np.allclose(db1_loop, db1_vec)}\")\n",
    "print(f\"db2 match:  {np.allclose(db2_loop, db2_vec)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Scaling Test: Different Batch Sizes\n",
    "\n",
    "Let's see how performance scales with batch size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [10, 50, 100, 200, 500, 1000]\n",
    "times_loop = []\n",
    "times_vec = []\n",
    "\n",
    "for m in batch_sizes:\n",
    "    # Generate data\n",
    "    X_test = np.random.randn(n_x, m)\n",
    "    Y_test = (np.random.randn(n_y, m) > 0).astype(float)\n",
    "    \n",
    "    # Time loop version\n",
    "    start = time.time()\n",
    "    A2, loss, cache = forward_propagation_loop(X_test, Y_test, W1, b1, W2, b2)\n",
    "    dW1, db1, dW2, db2 = backward_propagation_loop(X_test, Y_test, W1, b1, W2, b2, cache)\n",
    "    times_loop.append(time.time() - start)\n",
    "    \n",
    "    # Time vectorized version\n",
    "    start = time.time()\n",
    "    A2, loss, cache = forward_propagation_vectorized(X_test, Y_test, W1, b1, W2, b2)\n",
    "    dW1, db1, dW2, db2 = backward_propagation_vectorized(X_test, Y_test, W1, b1, W2, b2, cache)\n",
    "    times_vec.append(time.time() - start)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(batch_sizes, np.array(times_loop)*1000, 'o-', linewidth=2, \n",
    "         markersize=8, label='Loop-based', color='#FF6B6B')\n",
    "plt.plot(batch_sizes, np.array(times_vec)*1000, 's-', linewidth=2, \n",
    "         markersize=8, label='Vectorized', color='#4ECDC4')\n",
    "\n",
    "plt.xlabel('Batch Size (m)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Time (ms)', fontsize=12, fontweight='bold')\n",
    "plt.title('Performance Scaling: Loop vs Vectorized', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Speedup by Batch Size:\")\n",
    "for i, m in enumerate(batch_sizes):\n",
    "    speedup = times_loop[i] / times_vec[i]\n",
    "    print(f\"  m={m:4d}: {speedup:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Complete Training Example\n",
    "\n",
    "Let's train a neural network using vectorized operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a more interesting dataset\n",
    "np.random.seed(42)\n",
    "m = 300\n",
    "n_x = 2\n",
    "n_h = 10\n",
    "n_y = 1\n",
    "\n",
    "# Create spiral dataset\n",
    "theta = np.linspace(0, 4*np.pi, m//2)\n",
    "r = np.linspace(0.5, 2, m//2)\n",
    "\n",
    "# Class 0: spiral\n",
    "X_class0 = np.vstack([r * np.cos(theta), r * np.sin(theta)])\n",
    "Y_class0 = np.zeros((1, m//2))\n",
    "\n",
    "# Class 1: spiral (rotated)\n",
    "X_class1 = np.vstack([r * np.cos(theta + np.pi), r * np.sin(theta + np.pi)])\n",
    "Y_class1 = np.ones((1, m//2))\n",
    "\n",
    "# Combine\n",
    "X = np.hstack([X_class0, X_class1])\n",
    "Y = np.hstack([Y_class0, Y_class1])\n",
    "\n",
    "# Add noise\n",
    "X += np.random.randn(*X.shape) * 0.1\n",
    "\n",
    "# Shuffle\n",
    "indices = np.random.permutation(m)\n",
    "X = X[:, indices]\n",
    "Y = Y[:, indices]\n",
    "\n",
    "print(f\"Dataset: {m} samples, {n_x} features\")\n",
    "print(f\"Architecture: {n_x} â†’ {n_h} â†’ {n_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X[0, Y[0]==0], X[1, Y[0]==0], c='#FF6B6B', s=50, \n",
    "           alpha=0.7, edgecolors='black', linewidth=1, label='Class 0')\n",
    "plt.scatter(X[0, Y[0]==1], X[1, Y[0]==1], c='#4ECDC4', s=50, \n",
    "           alpha=0.7, edgecolors='black', linewidth=1, label='Class 1')\n",
    "plt.xlabel('Feature 1', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Feature 2', fontsize=12, fontweight='bold')\n",
    "plt.title('Spiral Dataset', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(n_h, n_x) * 0.1\n",
    "b1 = np.zeros((n_h, 1))\n",
    "W2 = np.random.randn(n_y, n_h) * 0.1\n",
    "b2 = np.zeros((n_y, 1))\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.5\n",
    "epochs = 2000\n",
    "losses = []\n",
    "\n",
    "print(\"ðŸš€ Training Neural Network with Vectorization...\\n\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward propagation (vectorized!)\n",
    "    A2, loss, cache = forward_propagation_vectorized(X, Y, W1, b1, W2, b2)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Backward propagation (vectorized!)\n",
    "    dW1, db1, dW2, db2 = backward_propagation_vectorized(X, Y, W1, b1, W2, b2, cache)\n",
    "    \n",
    "    # Update parameters\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        accuracy = np.mean((A2 > 0.5) == Y)\n",
    "        print(f\"Epoch {epoch+1:4d}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses, linewidth=2, color='#FF6B6B')\n",
    "plt.xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "plt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundary\n",
    "def plot_decision_boundary(X, Y, W1, b1, W2, b2):\n",
    "    # Create mesh\n",
    "    x_min, x_max = X[0, :].min() - 0.5, X[0, :].max() + 0.5\n",
    "    y_min, y_max = X[1, :].min() - 0.5, X[1, :].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # Predict on mesh\n",
    "    X_mesh = np.c_[xx.ravel(), yy.ravel()].T\n",
    "    A2, _, _ = forward_propagation_vectorized(X_mesh, None, W1, b1, W2, b2)\n",
    "    Z = A2.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    plt.contourf(xx, yy, Z, levels=20, cmap='RdYlBu_r', alpha=0.6)\n",
    "    plt.colorbar(label='P(y=1)')\n",
    "    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=3, linestyles='--')\n",
    "    \n",
    "    plt.scatter(X[0, Y[0]==0], X[1, Y[0]==0], c='#FF6B6B', s=80, \n",
    "               alpha=0.8, edgecolors='black', linewidth=1.5, label='Class 0')\n",
    "    plt.scatter(X[0, Y[0]==1], X[1, Y[0]==1], c='#4ECDC4', s=80, \n",
    "               alpha=0.8, edgecolors='black', linewidth=1.5, label='Class 1')\n",
    "    \n",
    "    plt.xlabel('Feature 1', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Feature 2', fontsize=12, fontweight='bold')\n",
    "    plt.title('Decision Boundary (Trained with Vectorization)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(X, Y, W1, b1, W2, b2)\n",
    "\n",
    "# Final accuracy\n",
    "A2_final, _, _ = forward_propagation_vectorized(X, Y, W1, b1, W2, b2)\n",
    "accuracy = np.mean((A2_final > 0.5) == Y)\n",
    "print(f\"\\nâœ… Final Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Mini-Batch Gradient Descent\n",
    "\n",
    "In practice, we often use **mini-batches** instead of the full dataset:\n",
    "\n",
    "- **Batch Gradient Descent**: Use all $m$ examples (what we did above)\n",
    "- **Stochastic Gradient Descent (SGD)**: Use 1 example at a time\n",
    "- **Mini-Batch Gradient Descent**: Use batches of size $b$ (e.g., 32, 64, 128)\n",
    "\n",
    "### Why Mini-Batches?\n",
    "\n",
    "1. **Memory Efficiency**: Can't fit millions of examples in memory\n",
    "2. **Faster Convergence**: More frequent updates\n",
    "3. **Better Generalization**: Noise in gradients acts as regularization\n",
    "4. **GPU Optimization**: Modern GPUs are optimized for batch sizes like 32, 64, 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(X, Y, batch_size):\n",
    "    \"\"\"\n",
    "    Create mini-batches from dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Input data (n_x, m)\n",
    "    - Y: Labels (n_y, m)\n",
    "    - batch_size: Size of each mini-batch\n",
    "    \n",
    "    Returns:\n",
    "    - List of (X_batch, Y_batch) tuples\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "    \n",
    "    # Shuffle data\n",
    "    permutation = np.random.permutation(m)\n",
    "    X_shuffled = X[:, permutation]\n",
    "    Y_shuffled = Y[:, permutation]\n",
    "    \n",
    "    # Create mini-batches\n",
    "    num_complete_batches = m // batch_size\n",
    "    \n",
    "    for k in range(num_complete_batches):\n",
    "        X_batch = X_shuffled[:, k*batch_size:(k+1)*batch_size]\n",
    "        Y_batch = Y_shuffled[:, k*batch_size:(k+1)*batch_size]\n",
    "        mini_batches.append((X_batch, Y_batch))\n",
    "    \n",
    "    # Handle remaining examples\n",
    "    if m % batch_size != 0:\n",
    "        X_batch = X_shuffled[:, num_complete_batches*batch_size:]\n",
    "        Y_batch = Y_shuffled[:, num_complete_batches*batch_size:]\n",
    "        mini_batches.append((X_batch, Y_batch))\n",
    "    \n",
    "    return mini_batches\n",
    "\n",
    "# Example: Create mini-batches\n",
    "batch_size = 32\n",
    "mini_batches = create_mini_batches(X, Y, batch_size)\n",
    "\n",
    "print(f\"Dataset size: {X.shape[1]} examples\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Number of mini-batches: {len(mini_batches)}\")\n",
    "print(f\"\\nMini-batch shapes:\")\n",
    "for i, (X_batch, Y_batch) in enumerate(mini_batches[:3]):\n",
    "    print(f\"  Batch {i+1}: X={X_batch.shape}, Y={Y_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary: Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "âœ… **Vectorization Basics**\n",
    "- Replace loops with matrix operations\n",
    "- 10-100x faster than loops\n",
    "- Enables GPU acceleration\n",
    "\n",
    "âœ… **Forward Propagation (Vectorized)**\n",
    "$$\n",
    "\\begin{align}\n",
    "Z^{[l]} &= W^{[l]} A^{[l-1]} + b^{[l]} \\quad \\text{Shape: } (n^{[l]}, m) \\\\\n",
    "A^{[l]} &= g^{[l]}(Z^{[l]}) \\quad \\text{Shape: } (n^{[l]}, m)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "âœ… **Backpropagation (Vectorized)**\n",
    "$$\n",
    "\\begin{align}\n",
    "dZ^{[l]} &= (W^{[l+1]})^T dZ^{[l+1]} \\odot g'^{[l]}(Z^{[l]}) \\\\\n",
    "dW^{[l]} &= \\frac{1}{m} dZ^{[l]} (A^{[l-1]})^T \\\\\n",
    "db^{[l]} &= \\frac{1}{m} \\sum_{i=1}^{m} dZ^{[l]}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "âœ… **Key Insights**\n",
    "- Each column represents one training example\n",
    "- Bias is broadcast across all examples\n",
    "- Gradients are averaged over the batch ($\\frac{1}{m}$)\n",
    "- Matrix dimensions must match correctly\n",
    "\n",
    "âœ… **Practical Benefits**\n",
    "- Faster training (10-100x speedup)\n",
    "- Cleaner, more readable code\n",
    "- Enables mini-batch gradient descent\n",
    "- GPU acceleration\n",
    "\n",
    "### Dimension Cheat Sheet\n",
    "\n",
    "For a network with $L$ layers and batch size $m$:\n",
    "\n",
    "| Variable | Shape | Description |\n",
    "|----------|-------|-------------|\n",
    "| $X$ | $(n_x, m)$ | Input features |\n",
    "| $W^{[l]}$ | $(n^{[l]}, n^{[l-1]})$ | Weights layer $l$ |\n",
    "| $b^{[l]}$ | $(n^{[l]}, 1)$ | Bias layer $l$ |\n",
    "| $Z^{[l]}$ | $(n^{[l]}, m)$ | Linear activation |\n",
    "| $A^{[l]}$ | $(n^{[l]}, m)$ | Activation output |\n",
    "| $dZ^{[l]}$ | $(n^{[l]}, m)$ | Gradient of $Z^{[l]}$ |\n",
    "| $dW^{[l]}$ | $(n^{[l]}, n^{[l-1]})$ | Gradient of $W^{[l]}$ |\n",
    "| $db^{[l]}$ | $(n^{[l]}, 1)$ | Gradient of $b^{[l]}$ |\n",
    "\n",
    "### Connection to Other Notebooks\n",
    "\n",
    "This notebook extends concepts from:\n",
    "- **`4_1_a_i_imp_three_layer_math_summary_of_gradient_derivation_explained.md`**: Single sample gradients â†’ Batch gradients\n",
    "- **`4_1_a_iv_n_layer_general_activation_gradient_derivation.md`**: General backpropagation formulas\n",
    "- **`5_a_logistic_regression_pure_implementation.ipynb`**: Simple example of vectorization\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "ðŸš€ **Practice**:\n",
    "- Implement your own vectorized neural network\n",
    "- Experiment with different batch sizes\n",
    "- Try different activation functions\n",
    "- Add more layers\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ“ Congratulations!** You now understand vectorization in deep neural networks, including the complete mathematical formulation of forward and backward propagation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
