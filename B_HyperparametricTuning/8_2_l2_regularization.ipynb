{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 Regularization (Ridge) - Complete Guide\n",
    "\n",
    "## üéØ What This Notebook Covers\n",
    "\n",
    "In this comprehensive notebook, we explore **L2 Regularization** (also known as Ridge Regression or Weight Decay):\n",
    "\n",
    "1. ‚úÖ **The Overfitting Problem** - Why we need regularization\n",
    "2. ‚úÖ **Mathematical Foundation** - Complete derivations with intuition\n",
    "3. ‚úÖ **Multiple Intuitive Examples** - Weight shrinkage, geometry, analogies\n",
    "4. ‚úÖ **Implementation from Scratch** - Pure NumPy implementation\n",
    "5. ‚úÖ **Comprehensive Visualizations** - Visual learning at every step\n",
    "6. ‚úÖ **L2 vs L1 Comparison** - When to use which\n",
    "7. ‚úÖ **Practical Guidelines** - Hyperparameter tuning and best practices\n",
    "\n",
    "### Why L2 Regularization?\n",
    "\n",
    "**Key Property: WEIGHT SHRINKAGE** üéØ\n",
    "\n",
    "L2 regularization makes all weights **small but non-zero**, preventing any single feature from dominating. This makes models:\n",
    "- More stable (less sensitive to individual features)\n",
    "- Better at generalization (smoother decision boundaries)\n",
    "- Numerically stable (well-conditioned matrices)\n",
    "\n",
    "Let's dive deep into the mathematics and intuition! üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Overfitting Problem\n",
    "\n",
    "Before diving into L2 regularization, let's understand **why** we need regularization.\n",
    "\n",
    "### The Problem: Large Weights Lead to Overfitting\n",
    "\n",
    "When a model overfits:\n",
    "- ‚úÖ **Training accuracy**: Very high\n",
    "- ‚ùå **Test accuracy**: Poor\n",
    "- üî¥ **Weights become very large** to fit training noise\n",
    "\n",
    "**L2 regularization** penalizes large weights, keeping them small and controlled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 1: Weight Magnitude and Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate polynomial regression example\n",
    "np.random.seed(42)\n",
    "X_demo = np.linspace(0, 10, 20)\n",
    "y_demo = 2 * X_demo + 1 + np.random.randn(20) * 2\n",
    "\n",
    "# Fit high-degree polynomial WITHOUT regularization\n",
    "degree = 15\n",
    "coef_no_reg = np.polyfit(X_demo, y_demo, degree)\n",
    "\n",
    "# Simulate L2 regularization effect (smaller coefficients)\n",
    "coef_l2 = coef_no_reg * 0.1  # Shrink coefficients\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Coefficients without regularization\n",
    "axes[0, 0].bar(range(len(coef_no_reg)), coef_no_reg, color='red', alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0, 0].axhline(y=0, color='k', linestyle='-', linewidth=1)\n",
    "axes[0, 0].set_xlabel('Polynomial Degree', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Coefficient Value', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('‚ùå Without Regularization\\nVery Large Coefficients!', fontsize=14, fontweight='bold', color='red')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0, 0].text(7, max(coef_no_reg)*0.7, f'Max: {max(abs(coef_no_reg)):.1f}\\nHuge weights!', \n",
    "                fontsize=11, bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7))\n",
    "\n",
    "# Plot 2: Coefficients with L2 regularization\n",
    "axes[0, 1].bar(range(len(coef_l2)), coef_l2, color='green', alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0, 1].axhline(y=0, color='k', linestyle='-', linewidth=1)\n",
    "axes[0, 1].set_xlabel('Polynomial Degree', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Coefficient Value', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('‚úÖ With L2 Regularization\\nSmall, Controlled Coefficients', fontsize=14, fontweight='bold', color='green')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "axes[0, 1].text(7, max(coef_l2)*0.7, f'Max: {max(abs(coef_l2)):.1f}\\nSmall weights!', \n",
    "                fontsize=11, bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "# Plot 3: Fitted curves\n",
    "X_plot = np.linspace(0, 10, 200)\n",
    "y_no_reg = np.polyval(coef_no_reg, X_plot)\n",
    "y_l2 = np.polyval(coef_l2, X_plot)\n",
    "\n",
    "axes[1, 0].scatter(X_demo, y_demo, c='black', s=100, alpha=0.6, edgecolors='black', linewidth=2, label='Training Data')\n",
    "axes[1, 0].plot(X_plot, y_no_reg, 'r-', linewidth=3, label='Without L2 (Overfit)')\n",
    "axes[1, 0].set_xlabel('X', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('y', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('‚ùå Without L2: Wild Oscillations', fontsize=14, fontweight='bold', color='red')\n",
    "axes[1, 0].legend(fontsize=11)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_ylim(-10, 35)\n",
    "\n",
    "axes[1, 1].scatter(X_demo, y_demo, c='black', s=100, alpha=0.6, edgecolors='black', linewidth=2, label='Training Data')\n",
    "axes[1, 1].plot(X_plot, y_l2, 'g-', linewidth=3, label='With L2 (Smooth)')\n",
    "axes[1, 1].set_xlabel('X', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('y', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('‚úÖ With L2: Smooth, Generalizable', fontsize=14, fontweight='bold', color='green')\n",
    "axes[1, 1].legend(fontsize=11)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_ylim(-10, 35)\n",
    "\n",
    "plt.suptitle('Visualization 1: L2 Regularization Controls Weight Magnitude', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key Observations:\")\n",
    "print(\"  ‚Ä¢ Without L2: Large coefficients ‚Üí Wild oscillations ‚Üí Overfitting\")\n",
    "print(\"  ‚Ä¢ With L2: Small coefficients ‚Üí Smooth curve ‚Üí Better generalization\")\n",
    "print(f\"  ‚Ä¢ Weight reduction: {max(abs(coef_no_reg))/max(abs(coef_l2)):.1f}x smaller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. L2 Regularization: Mathematical Foundation\n",
    "\n",
    "Let's build the mathematics from the ground up with complete derivations.\n",
    "\n",
    "### 3.1 The Loss Function with L2 Penalty\n",
    "\n",
    "**Original Loss Function** (e.g., Mean Squared Error):\n",
    "\n",
    "$$\n",
    "L_{\\text{original}} = \\frac{1}{m} \\sum_{i=1}^{m} (y^{(i)} - \\hat{y}^{(i)})^2\n",
    "$$\n",
    "\n",
    "**L2 Regularized Loss Function**:\n",
    "\n",
    "$$\n",
    "\\boxed{L_{\\text{L2}} = L_{\\text{original}} + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} W_j^2}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $L_{\\text{original}}$ = Original loss (MSE, cross-entropy, etc.)\n",
    "- $\\lambda$ = Regularization strength (hyperparameter)\n",
    "- $m$ = Number of training examples\n",
    "- $n$ = Number of weights\n",
    "- $W_j^2$ = Square of weight $j$\n",
    "- Factor of $\\frac{1}{2}$ simplifies derivative\n",
    "\n",
    "### 3.2 Why Squared Weights?\n",
    "\n",
    "The squared term $W^2$ has special properties:\n",
    "- It penalizes weights **quadratically** (larger weights penalized more)\n",
    "- Its derivative is **proportional to W**: $\\frac{dW^2}{dW} = 2W$\n",
    "- This creates **smooth shrinkage** (all weights get smaller, none go to zero)\n",
    "- Also called **Weight Decay** because weights decay exponentially\n",
    "\n",
    "### 3.3 Forward Pass: Computing the Loss\n",
    "\n",
    "**Step 1**: Compute original loss\n",
    "$$\n",
    "L_{\\text{original}} = \\frac{1}{m} \\sum_{i=1}^{m} \\text{Loss}(y^{(i)}, \\hat{y}^{(i)})\n",
    "$$\n",
    "\n",
    "**Step 2**: Compute L2 penalty\n",
    "$$\n",
    "L_{\\text{L2 penalty}} = \\frac{\\lambda}{2m} \\sum_{j=1}^{n} W_j^2\n",
    "$$\n",
    "\n",
    "**Step 3**: Add them together\n",
    "$$\n",
    "L_{\\text{total}} = L_{\\text{original}} + L_{\\text{L2 penalty}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Backward Pass: Computing Gradients\n",
    "\n",
    "This is where L2 regularization differs from L1!\n",
    "\n",
    "**Original Gradient** (without regularization):\n",
    "$$\n",
    "\\frac{\\partial L_{\\text{original}}}{\\partial W_j} = \\text{(computed via backpropagation)}\n",
    "$$\n",
    "\n",
    "**L2 Penalty Gradient**:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial W_j} \\left( \\frac{\\lambda}{2m} W_j^2 \\right) = \\frac{\\lambda}{2m} \\cdot 2W_j = \\frac{\\lambda}{m} \\cdot W_j\n",
    "$$\n",
    "\n",
    "**Total Gradient** (with L2 regularization):\n",
    "$$\n",
    "\\boxed{\\frac{\\partial L_{\\text{L2}}}{\\partial W_j} = \\frac{\\partial L_{\\text{original}}}{\\partial W_j} + \\frac{\\lambda}{m} \\cdot W_j}\n",
    "$$\n",
    "\n",
    "**Parameter Update**:\n",
    "$$\n",
    "W_j := W_j - \\alpha \\left( \\frac{\\partial L_{\\text{original}}}{\\partial W_j} + \\frac{\\lambda}{m} \\cdot W_j \\right)\n",
    "$$\n",
    "\n",
    "**Rearranging**:\n",
    "$$\n",
    "W_j := W_j - \\alpha \\frac{\\partial L_{\\text{original}}}{\\partial W_j} - \\alpha \\frac{\\lambda}{m} W_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_j := W_j \\left(1 - \\alpha \\frac{\\lambda}{m}\\right) - \\alpha \\frac{\\partial L_{\\text{original}}}{\\partial W_j}\n",
    "$$\n",
    "\n",
    "This shows **weight decay**: weights are multiplied by $(1 - \\alpha \\frac{\\lambda}{m}) < 1$ each iteration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Why Does L2 NOT Create Sparsity?\n",
    "\n",
    "**Key Insight**: The gradient is **proportional to W** (not constant like L1)\n",
    "\n",
    "**For any weight** $W$:\n",
    "$$\n",
    "W := W \\left(1 - \\alpha \\frac{\\lambda}{m}\\right) - \\alpha \\frac{\\partial L_{\\text{original}}}{\\partial W}\n",
    "$$\n",
    "\n",
    "- Weight is multiplied by $(1 - \\alpha \\frac{\\lambda}{m})$ each iteration\n",
    "- This is **exponential decay**: $W_t = W_0 \\cdot (1 - \\alpha \\frac{\\lambda}{m})^t$\n",
    "- As $W \\to 0$, the penalty $\\to 0$ (gets weaker!)\n",
    "- Weights get **very small** but **never exactly zero**\n",
    "\n",
    "**Contrast with L1**:\n",
    "$$\n",
    "W := W - \\alpha \\left( \\frac{\\partial L_{\\text{original}}}{\\partial W} + \\frac{\\lambda}{m} \\cdot \\text{sign}(W) \\right)\n",
    "$$\n",
    "- Subtracts a **constant** $\\frac{\\alpha \\lambda}{m}$ each iteration\n",
    "- Penalty stays constant even as $W \\to 0$\n",
    "- Weights can reach **exactly zero**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 2: L2 Penalty Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create L2 penalty visualization\n",
    "x = np.linspace(-3, 3, 1000)\n",
    "l2_penalty = x**2\n",
    "l2_derivative = 2 * x\n",
    "l1_penalty = np.abs(x)\n",
    "l1_derivative = np.sign(x)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: L2 penalty function\n",
    "axes[0].plot(x, l2_penalty, linewidth=4, color='blue', label='W¬≤')\n",
    "axes[0].axhline(y=0, color='k', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "axes[0].axvline(x=0, color='k', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "axes[0].set_xlabel('W', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('W¬≤', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Squared Penalty Function\\n(L2 Penalty)', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].text(0, 7, 'Parabola\\n(Smooth everywhere)', ha='center', fontsize=10,\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "axes[0].set_ylim(0, 9)\n",
    "\n",
    "# Plot 2: L2 derivative (proportional to W)\n",
    "axes[1].plot(x, l2_derivative, linewidth=4, color='blue', label='2W (proportional)')\n",
    "axes[1].axhline(y=0, color='k', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "axes[1].axvline(x=0, color='k', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "axes[1].set_xlabel('W', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('dW¬≤/dW = 2W', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('L2 Derivative\\n(Proportional to W)', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(-7, 7)\n",
    "axes[1].text(2, 5, 'Larger W ‚Üí\\nLarger penalty', fontsize=10, color='blue', fontweight='bold')\n",
    "axes[1].text(-2, -5, 'As W‚Üí0,\\npenalty‚Üí0', fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "# Plot 3: L1 vs L2 comparison\n",
    "axes[2].plot(x, l2_penalty, linewidth=4, color='blue', label='L2: W¬≤ (smooth)', alpha=0.7)\n",
    "axes[2].plot(x, l1_penalty, linewidth=4, color='orange', label='L1: |W| (sharp)', linestyle='--', alpha=0.7)\n",
    "axes[2].axhline(y=0, color='k', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "axes[2].axvline(x=0, color='k', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "axes[2].set_xlabel('W', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Penalty', fontsize=12, fontweight='bold')\n",
    "axes[2].set_title('L1 vs L2 Penalty Functions', fontsize=13, fontweight='bold')\n",
    "axes[2].legend(fontsize=11)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_ylim(0, 9)\n",
    "axes[2].text(0, 7, 'L2: Smooth (differentiable)\\nL1: Sharp corner at 0', ha='center', fontsize=10,\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "\n",
    "plt.suptitle('Visualization 2: L2 Penalty Function and Derivative', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Key Mathematical Insights:\")\n",
    "print(\"  ‚Ä¢ L2 uses squared penalty: W¬≤\")\n",
    "print(\"  ‚Ä¢ Derivative of W¬≤ is 2W (proportional to W)\")\n",
    "print(\"  ‚Ä¢ Proportional penalty ‚Üí weights shrink but never reach zero\")\n",
    "print(\"  ‚Ä¢ L1 uses |W|, derivative is sign(W) (constant)\")\n",
    "print(\"  ‚Ä¢ Constant penalty ‚Üí weights can reach exactly zero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 3: Weight Decay Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate weight decay over iterations\n",
    "iterations = np.arange(0, 100)\n",
    "alpha = 0.01  # Learning rate\n",
    "lambda_val = 0.1  # Regularization strength\n",
    "m = 100  # Number of samples\n",
    "\n",
    "# Initial weights\n",
    "W0_large = 5.0\n",
    "W0_medium = 2.0\n",
    "W0_small = 0.5\n",
    "\n",
    "# Decay factor\n",
    "decay_factor = 1 - alpha * lambda_val / m\n",
    "\n",
    "# Weight evolution (assuming no gradient from loss)\n",
    "W_large = W0_large * (decay_factor ** iterations)\n",
    "W_medium = W0_medium * (decay_factor ** iterations)\n",
    "W_small = W0_small * (decay_factor ** iterations)\n",
    "\n",
    "# L1 for comparison (constant subtraction)\n",
    "l1_constant = alpha * lambda_val / m\n",
    "W_l1_large = np.maximum(0, W0_large - l1_constant * iterations)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: L2 weight decay\n",
    "axes[0].plot(iterations, W_large, linewidth=3, label=f'W‚ÇÄ = {W0_large}', color='red')\n",
    "axes[0].plot(iterations, W_medium, linewidth=3, label=f'W‚ÇÄ = {W0_medium}', color='orange')\n",
    "axes[0].plot(iterations, W_small, linewidth=3, label=f'W‚ÇÄ = {W0_small}', color='blue')\n",
    "axes[0].axhline(y=0, color='k', linestyle='--', linewidth=2, alpha=0.5)\n",
    "axes[0].set_xlabel('Iteration', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Weight Value', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('L2 Weight Decay\\nExponential Decay (Never Reaches Zero)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].text(50, 3, f'Decay factor:\\n(1 - Œ±Œª/m) = {decay_factor:.4f}', \n",
    "             fontsize=11, bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "axes[0].text(70, 0.5, 'Approaches zero\\nbut never reaches it', \n",
    "             fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "\n",
    "# Plot 2: L2 vs L1 comparison\n",
    "axes[1].plot(iterations, W_large, linewidth=3, label='L2: Exponential decay', color='blue')\n",
    "axes[1].plot(iterations, W_l1_large, linewidth=3, label='L1: Linear decay', color='orange', linestyle='--')\n",
    "axes[1].axhline(y=0, color='k', linestyle='--', linewidth=2, alpha=0.5)\n",
    "axes[1].set_xlabel('Iteration', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Weight Value', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('L2 vs L1 Weight Decay\\n(Starting from W‚ÇÄ = 5.0)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].text(30, 3, 'L2: Smooth decay\\n(proportional to W)', \n",
    "             fontsize=10, color='blue', fontweight='bold',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "axes[1].text(30, 1, 'L1: Linear decay\\n(constant rate)', \n",
    "             fontsize=10, color='orange', fontweight='bold',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "\n",
    "plt.suptitle('Visualization 3: Weight Decay Dynamics', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Weight Decay Analysis:\")\n",
    "print(f\"  ‚Ä¢ Decay factor per iteration: {decay_factor:.4f}\")\n",
    "print(f\"  ‚Ä¢ After 100 iterations:\")\n",
    "print(f\"    - Large weight (5.0) ‚Üí {W_large[-1]:.4f}\")\n",
    "print(f\"    - Medium weight (2.0) ‚Üí {W_medium[-1]:.4f}\")\n",
    "print(f\"    - Small weight (0.5) ‚Üí {W_small[-1]:.4f}\")\n",
    "print(\"\\n  ‚Ä¢ L2: Exponential decay (never reaches zero)\")\n",
    "print(\"  ‚Ä¢ L1: Linear decay (reaches zero in finite time)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 4: Gradient Flow with L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a flowchart-style visualization\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.axis('off')\n",
    "\n",
    "# Define box positions\n",
    "boxes = [\n",
    "    # Forward pass\n",
    "    {'xy': (0.5, 0.9), 'text': 'Forward Pass\\nCompute predictions', 'color': 'lightblue'},\n",
    "    {'xy': (0.5, 0.75), 'text': 'Compute Original Loss\\nL_original', 'color': 'lightblue'},\n",
    "    {'xy': (0.5, 0.6), 'text': 'Compute L2 Penalty\\n(Œª/2m) * Œ£W¬≤', 'color': 'lightcoral'},\n",
    "    {'xy': (0.5, 0.45), 'text': 'Total Loss\\nL_total = L_original + L2_penalty', 'color': 'lightgreen'},\n",
    "    \n",
    "    # Backward pass\n",
    "    {'xy': (0.5, 0.3), 'text': 'Backward Pass\\nCompute gradients', 'color': 'lightyellow'},\n",
    "    {'xy': (0.25, 0.15), 'text': 'Original Gradient\\n‚àÇL_original/‚àÇW', 'color': 'lightblue'},\n",
    "    {'xy': (0.75, 0.15), 'text': 'L2 Gradient\\n(Œª/m) * W', 'color': 'lightcoral'},\n",
    "    {'xy': (0.5, 0.0), 'text': 'Total Gradient\\n‚àÇL_total/‚àÇW = ‚àÇL_original/‚àÇW + (Œª/m)*W', 'color': 'lightgreen'},\n",
    "]\n",
    "\n",
    "# Draw boxes\n",
    "for box in boxes:\n",
    "    ax.text(box['xy'][0], box['xy'][1], box['text'], \n",
    "            ha='center', va='center', fontsize=11, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.8', facecolor=box['color'], \n",
    "                     edgecolor='black', linewidth=2))\n",
    "\n",
    "# Draw arrows\n",
    "arrows = [\n",
    "    ((0.5, 0.87), (0.5, 0.78)),\n",
    "    ((0.5, 0.72), (0.5, 0.63)),\n",
    "    ((0.5, 0.57), (0.5, 0.48)),\n",
    "    ((0.5, 0.42), (0.5, 0.33)),\n",
    "    ((0.5, 0.27), (0.25, 0.18)),\n",
    "    ((0.5, 0.27), (0.75, 0.18)),\n",
    "    ((0.25, 0.12), (0.5, 0.03)),\n",
    "    ((0.75, 0.12), (0.5, 0.03)),\n",
    "]\n",
    "\n",
    "for start, end in arrows:\n",
    "    ax.annotate('', xy=end, xytext=start,\n",
    "                arrowprops=dict(arrowstyle='->', lw=3, color='black'))\n",
    "\n",
    "# Add title\n",
    "ax.text(0.5, 0.98, 'Visualization 4: Gradient Flow with L2 Regularization', \n",
    "        ha='center', va='top', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Add legend\n",
    "ax.text(0.05, 0.5, 'Legend:\\n‚Ä¢ Blue: Standard operations\\n‚Ä¢ Red: L2-specific\\n‚Ä¢ Green: Combined result\\n\\nKey Difference from L1:\\nL2 gradient = (Œª/m)*W\\n(proportional to W)', \n",
    "        ha='left', va='center', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='white', edgecolor='black', linewidth=2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Gradient Flow Summary:\")\n",
    "print(\"  1. Forward: Compute predictions and loss\")\n",
    "print(\"  2. Add L2 penalty to loss: (Œª/2m) * Œ£W¬≤\")\n",
    "print(\"  3. Backward: Compute original gradients\")\n",
    "print(\"  4. Add L2 gradient: (Œª/m) * W (proportional!)\")\n",
    "print(\"  5. Update: W := W(1 - Œ±Œª/m) - Œ±‚àÇL/‚àÇW (weight decay!)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
