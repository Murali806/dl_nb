{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam: Adaptive Moment Estimation\n",
    "\n",
    "## üéØ What This Notebook Covers\n",
    "\n",
    "**Adam** (Adaptive Moment Estimation) combines the best of Momentum and RMSProp. In this notebook, we explore:\n",
    "\n",
    "1. ‚úÖ **Motivation** - Why combine Momentum + RMSProp?\n",
    "2. ‚úÖ **Mathematical Formulation** - How Adam works\n",
    "3. ‚úÖ **Implementation** - Adam from scratch\n",
    "4. ‚úÖ **Bias Correction** - Why it's crucial for Adam\n",
    "5. ‚úÖ **Performance Comparison** - Adam vs all other optimizers\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "- **Most Popular**: Industry standard optimizer ‚≠ê\n",
    "- **Best of Both Worlds**: Momentum + adaptive learning rates üéØ\n",
    "- **Robust**: Works well with minimal tuning üîß\n",
    "\n",
    "Let's master Adam! üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Motivation: Best of Both Worlds\n",
    "\n",
    "### Recap: What We've Learned\n",
    "\n",
    "**Momentum** (from 7.4):\n",
    "- Accumulates velocity: $v_t = \\beta_1 v_{t-1} + (1-\\beta_1) g_t$\n",
    "- Smooths updates, accelerates convergence\n",
    "- ‚úÖ Good: Fast convergence\n",
    "- ‚ùå Problem: Fixed learning rate for all parameters\n",
    "\n",
    "**RMSProp** (from 7.6):\n",
    "- Adapts learning rate: $E[g^2]_t = \\beta_2 E[g^2]_{t-1} + (1-\\beta_2) g_t^2$\n",
    "- Different LR per parameter\n",
    "- ‚úÖ Good: Adaptive learning rates\n",
    "- ‚ùå Problem: No momentum\n",
    "\n",
    "### Adam's Idea: Combine Both!\n",
    "\n",
    "```\n",
    "Adam = Momentum + RMSProp\n",
    "\n",
    "From Momentum:\n",
    "    ‚Ä¢ First moment (mean) of gradients\n",
    "    ‚Ä¢ Provides direction and acceleration\n",
    "    \n",
    "From RMSProp:\n",
    "    ‚Ä¢ Second moment (variance) of gradients\n",
    "    ‚Ä¢ Provides adaptive learning rates\n",
    "    \n",
    "Result:\n",
    "    ‚Ä¢ Fast convergence (momentum)\n",
    "    ‚Ä¢ Adaptive per-parameter LR (RMSProp)\n",
    "    ‚Ä¢ Best of both worlds! ‚≠ê\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mathematical Formulation\n",
    "\n",
    "### Adam Update Rule\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "m_t &= \\beta_1 m_{t-1} + (1-\\beta_1) g_t \\quad \\text{(first moment: momentum)} \\\\\n",
    "v_t &= \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 \\quad \\text{(second moment: RMSProp)} \\\\\n",
    "\\hat{m}_t &= \\frac{m_t}{1 - \\beta_1^t} \\quad \\text{(bias correction for first moment)} \\\\\n",
    "\\hat{v}_t &= \\frac{v_t}{1 - \\beta_2^t} \\quad \\text{(bias correction for second moment)} \\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon} \\cdot \\hat{m}_t \\quad \\text{(parameter update)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $g_t = \\nabla L(\\theta_t)$ = gradient at time $t$\n",
    "- $m_t$ = first moment estimate (momentum)\n",
    "- $v_t$ = second moment estimate (RMSProp)\n",
    "- $\\beta_1$ = decay rate for first moment (typically 0.9)\n",
    "- $\\beta_2$ = decay rate for second moment (typically 0.999)\n",
    "- $\\alpha$ = learning rate (typically 0.001)\n",
    "- $\\epsilon$ = small constant for numerical stability (typically $10^{-8}$)\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **First Moment ($m_t$)**: Like momentum, provides direction\n",
    "2. **Second Moment ($v_t$)**: Like RMSProp, adapts learning rate\n",
    "3. **Bias Correction**: Crucial for early iterations (see section 5)\n",
    "\n",
    "### Default Hyperparameters\n",
    "\n",
    "The original Adam paper recommends:\n",
    "- $\\alpha = 0.001$ (learning rate)\n",
    "- $\\beta_1 = 0.9$ (first moment decay)\n",
    "- $\\beta_2 = 0.999$ (second moment decay)\n",
    "- $\\epsilon = 10^{-8}$ (numerical stability)\n",
    "\n",
    "These work well in most cases! üéØ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spiral_data(n_samples=300, noise=0.1):\n",
    "    \"\"\"\n",
    "    Generate spiral dataset for binary classification.\n",
    "    \n",
    "    Returns:\n",
    "    - X: Features (n_x, m)\n",
    "    - Y: Labels (1, m)\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    m = n_samples\n",
    "    \n",
    "    # Create spiral\n",
    "    theta = np.linspace(0, 4*np.pi, m//2)\n",
    "    r = np.linspace(0.5, 2, m//2)\n",
    "    \n",
    "    # Class 0: spiral\n",
    "    X_class0 = np.vstack([r * np.cos(theta), r * np.sin(theta)])\n",
    "    Y_class0 = np.zeros((1, m//2))\n",
    "    \n",
    "    # Class 1: spiral (rotated)\n",
    "    X_class1 = np.vstack([r * np.cos(theta + np.pi), r * np.sin(theta + np.pi)])\n",
    "    Y_class1 = np.ones((1, m//2))\n",
    "    \n",
    "    # Combine\n",
    "    X = np.hstack([X_class0, X_class1])\n",
    "    Y = np.hstack([Y_class0, Y_class1])\n",
    "    \n",
    "    # Add noise\n",
    "    X += np.random.randn(*X.shape) * noise\n",
    "    \n",
    "    # Shuffle\n",
    "    indices = np.random.permutation(m)\n",
    "    X = X[:, indices]\n",
    "    Y = Y[:, indices]\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "# Generate data\n",
    "X, Y = generate_spiral_data(n_samples=300, noise=0.1)\n",
    "\n",
    "print(f\"Dataset shape: X={X.shape}, Y={Y.shape}\")\n",
    "print(f\"Number of samples: {X.shape[1]}\")\n",
    "print(f\"Number of features: {X.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neural Network with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"Derivative of ReLU.\"\"\"\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "print(\"‚úÖ Activation functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    \"\"\"\n",
    "    Neural network with Adam optimizer.\n",
    "    \n",
    "    Architecture: Input (2) ‚Üí Hidden (10, ReLU) ‚Üí Output (1, Sigmoid)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_x=2, n_h=10, n_y=1, learning_rate=0.001, \n",
    "                 beta1=0.9, beta2=0.999, epsilon=1e-8, random_seed=42):\n",
    "        \"\"\"\n",
    "        Initialize neural network with Adam optimizer.\n",
    "        \n",
    "        Parameters:\n",
    "        - learning_rate: Learning rate (Œ±), typically 0.001\n",
    "        - beta1: Decay rate for first moment (momentum), typically 0.9\n",
    "        - beta2: Decay rate for second moment (RMSProp), typically 0.999\n",
    "        - epsilon: Small constant for numerical stability\n",
    "        \"\"\"\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.W1 = np.random.randn(n_h, n_x) * 0.1\n",
    "        self.b1 = np.zeros((n_h, 1))\n",
    "        self.W2 = np.random.randn(n_y, n_h) * 0.1\n",
    "        self.b2 = np.zeros((n_y, 1))\n",
    "        \n",
    "        # Initialize first moments (momentum)\n",
    "        self.m_dW1 = np.zeros_like(self.W1)\n",
    "        self.m_db1 = np.zeros_like(self.b1)\n",
    "        self.m_dW2 = np.zeros_like(self.W2)\n",
    "        self.m_db2 = np.zeros_like(self.b2)\n",
    "        \n",
    "        # Initialize second moments (RMSProp)\n",
    "        self.v_dW1 = np.zeros_like(self.W1)\n",
    "        self.v_db1 = np.zeros_like(self.b1)\n",
    "        self.v_dW2 = np.zeros_like(self.W2)\n",
    "        self.v_db2 = np.zeros_like(self.b2)\n",
    "        \n",
    "        # Training history\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "        self.t = 0  # Time step for bias correction\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"Forward propagation.\"\"\"\n",
    "        Z1 = self.W1 @ X + self.b1\n",
    "        A1 = relu(Z1)\n",
    "        Z2 = self.W2 @ A1 + self.b2\n",
    "        A2 = sigmoid(Z2)\n",
    "        \n",
    "        cache = {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n",
    "        return A2, cache\n",
    "    \n",
    "    def compute_loss(self, Y, A2):\n",
    "        \"\"\"Compute binary cross-entropy loss.\"\"\"\n",
    "        m = Y.shape[1]\n",
    "        loss = -np.mean(Y * np.log(A2 + 1e-8) + (1 - Y) * np.log(1 - A2 + 1e-8))\n",
    "        return loss\n",
    "    \n",
    "    def backward_propagation(self, X, Y, cache):\n",
    "        \"\"\"Backward propagation.\"\"\"\n",
    "        m = X.shape[1]\n",
    "        Z1, A1, Z2, A2 = cache['Z1'], cache['A1'], cache['Z2'], cache['A2']\n",
    "        \n",
    "        # Backprop\n",
    "        dZ2 = A2 - Y\n",
    "        dW2 = (1/m) * (dZ2 @ A1.T)\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        \n",
    "        dA1 = self.W2.T @ dZ2\n",
    "        dZ1 = dA1 * relu_derivative(Z1)\n",
    "        dW1 = (1/m) * (dZ1 @ X.T)\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def update_parameters_adam(self, dW1, db1, dW2, db2):\n",
    "        \"\"\"\n",
    "        Update parameters using Adam optimizer.\n",
    "        \n",
    "        Combines momentum (first moment) and RMSProp (second moment)\n",
    "        with bias correction.\n",
    "        \"\"\"\n",
    "        self.t += 1  # Increment time step\n",
    "        \n",
    "        # Update first moments (momentum)\n",
    "        self.m_dW1 = self.beta1 * self.m_dW1 + (1 - self.beta1) * dW1\n",
    "        self.m_db1 = self.beta1 * self.m_db1 + (1 - self.beta1) * db1\n",
    "        self.m_dW2 = self.beta1 * self.m_dW2 + (1 - self.beta1) * dW2\n",
    "        self.m_db2 = self.beta1 * self.m_db2 + (1 - self.beta1) * db2\n",
    "        \n",
    "        # Update second moments (RMSProp)\n",
    "        self.v_dW1 = self.beta2 * self.v_dW1 + (1 - self.beta2) * dW1**2\n",
    "        self.v_db1 = self.beta2 * self.v_db1 + (1 - self.beta2) * db1**2\n",
    "        self.v_dW2 = self.beta2 * self.v_dW2 + (1 - self.beta2) * dW2**2\n",
    "        self.v_db2 = self.beta2 * self.v_db2 + (1 - self.beta2) * db2**2\n",
    "        \n",
    "        # Bias correction\n",
    "        m_dW1_corrected = self.m_dW1 / (1 - self.beta1**self.t)\n",
    "        m_db1_corrected = self.m_db1 / (1 - self.beta1**self.t)\n",
    "        m_dW2_corrected = self.m_dW2 / (1 - self.beta1**self.t)\n",
    "        m_db2_corrected = self.m_db2 / (1 - self.beta1**self.t)\n",
    "        \n",
    "        v_dW1_corrected = self.v_dW1 / (1 - self.beta2**self.t)\n",
    "        v_db1_corrected = self.v_db1 / (1 - self.beta2**self.t)\n",
    "        v_dW2_corrected = self.v_dW2 / (1 - self.beta2**self.t)\n",
    "        v_db2_corrected = self.v_db2 / (1 - self.beta2**self.t)\n",
    "        \n",
    "        # Update parameters\n",
    "        self.W1 -= self.lr * m_dW1_corrected / (np.sqrt(v_dW1_corrected) + self.epsilon)\n",
    "        self.b1 -= self.lr * m_db1_corrected / (np.sqrt(v_db1_corrected) + self.epsilon)\n",
    "        self.W2 -= self.lr * m_dW2_corrected / (np.sqrt(v_dW2_corrected) + self.epsilon)\n",
    "        self.b2 -= self.lr * m_db2_corrected / (np.sqrt(v_db2_corrected) + self.epsilon)\n",
    "    \n",
    "    def compute_accuracy(self, X, Y):\n",
    "        \"\"\"Compute accuracy.\"\"\"\n",
    "        A2, _ = self.forward_propagation(X)\n",
    "        predictions = (A2 > 0.5).astype(int)\n",
    "        accuracy = np.mean(predictions == Y)\n",
    "        return accuracy\n",
    "    \n",
    "    def fit(self, X, Y, epochs=1000, verbose=False):\n",
    "        \"\"\"Train the network with Adam optimizer.\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            A2, cache = self.forward_propagation(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(Y, A2)\n",
    "            self.losses.append(loss)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            accuracy = self.compute_accuracy(X, Y)\n",
    "            self.accuracies.append(accuracy)\n",
    "            \n",
    "            # Backward propagation\n",
    "            dW1, db1, dW2, db2 = self.backward_propagation(X, Y, cache)\n",
    "            \n",
    "            # Update parameters with Adam\n",
    "            self.update_parameters_adam(dW1, db1, dW2, db2)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch + 1) % 200 == 0:\n",
    "                print(f\"Epoch {epoch+1:4d}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n‚úÖ Training Complete!\")\n",
    "            print(f\"   Final Loss: {self.losses[-1]:.4f}\")\n",
    "            print(f\"   Final Accuracy: {self.accuracies[-1]:.4f}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "print(\"‚úÖ Adam class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison: Adam vs All Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need all previous optimizers for comparison\n",
    "class VanillaSGD:\n",
    "    \"\"\"Vanilla SGD.\"\"\"\n",
    "    def __init__(self, n_x=2, n_h=10, n_y=1, learning_rate=0.01, random_seed=42):\n",
    "        np.random.seed(random_seed)\n",
    "        self.lr = learning_rate\n",
    "        self.W1 = np.random.randn(n_h, n_x) * 0.1\n",
    "        self.b1 = np.zeros((n_h, 1))\n",
    "        self.W2 = np.random.randn(n_y, n_h) * 0.1\n",
    "        self.b2 = np.zeros((n_y, 1))\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        Z1 = self.W1 @ X + self.b1\n",
    "        A1 = relu(Z1)\n",
    "        Z2 = self.W2 @ A1 + self.b2\n",
    "        A2 = sigmoid(Z2)\n",
    "        return A2, {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n",
    "    \n",
    "    def compute_loss(self, Y, A2):\n",
    "        return -np.mean(Y * np.log(A2 + 1e-8) + (1 - Y) * np.log(1 - A2 + 1e-8))\n",
    "    \n",
    "    def backward_propagation(self, X, Y, cache):\n",
    "        m = X.shape[1]\n",
    "        Z1, A1, A2 = cache['Z1'], cache['A1'], cache['A2']\n",
    "        dZ2 = A2 - Y\n",
    "        dW2 = (1/m) * (dZ2 @ A1.T)\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        dA1 = self.W2.T @ dZ2\n",
    "        dZ1 = dA1 * relu_derivative(Z1)\n",
    "        dW1 = (1/m) * (dZ1 @ X.T)\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def compute_accuracy(self, X, Y):\n",
    "        A2, _ = self.forward_propagation(X)\n",
    "        return np.mean((A2 > 0.5).astype(int) == Y)\n",
    "    \n",
    "    def fit(self, X, Y, epochs=1000, verbose=False):\n",
    "        for epoch in range(epochs):\n",
    "            A2, cache = self.forward_propagation(X)\n",
    "            self.losses.append(self.compute_loss(Y, A2))\n",
    "            self.accuracies.append(self.compute_accuracy(X, Y))\n",
    "            dW1, db1, dW2, db2 = self.backward_propagation(X, Y, cache)\n",
    "            self.W1 -= self.lr * dW1\n",
    "            self.b1 -= self.lr * db1\n",
    "            self.W2 -= self.lr * dW2\n",
    "            self.b2 -= self.lr * db2\n",
    "        return self\n",
    "\n",
    "class Momentum:\n",
    "    \"\"\"SGD with Momentum.\"\"\"\n",
    "    def __init__(self, n_x=2, n_h=10, n_y=1, learning_rate=0.01, beta=0.9, random_seed=42):\n",
    "        np.random.seed(random_seed)\n",
    "        self.lr = learning_rate\n",
    "        self.beta = beta\n",
    "        self.W1 = np.random.randn(n_h, n_x) * 0.1\n",
    "        self.b1 = np.zeros((n_h, 1))\n",
    "        self.W2 = np.random.randn(n_y, n_h) * 0.1\n",
    "        self.b2 = np.zeros((n_y, 1))\n",
    "        self.v_dW1 = np.zeros_like(self.W1)\n",
    "        self.v_db1 = np.zeros_like(self.b1)\n",
    "        self.v_dW2 = np.zeros_like(self.W2)\n",
    "        self.v_db2 = np.zeros_like(self.b2)\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        Z1 = self.W1 @ X + self.b1\n",
    "        A1 = relu(Z1)\n",
    "        Z2 = self.W2 @ A1 + self.b2\n",
    "        A2 = sigmoid(Z2)\n",
    "        return A2, {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n",
    "    \n",
    "    def compute_loss(self, Y, A2):\n",
    "        return -np.mean(Y * np.log(A2 + 1e-8) + (1 - Y) * np.log(1 - A2 + 1e-8))\n",
    "    \n",
    "    def backward_propagation(self, X, Y, cache):\n",
    "        m = X.shape[1]\n",
    "        Z1, A1, A2 = cache['Z1'], cache['A1'], cache['A2']\n",
    "        dZ2 = A2 - Y\n",
    "        dW2 = (1/m) * (dZ2 @ A1.T)\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        dA1 = self.W2.T @ dZ2\n",
    "        dZ1 = dA1 * relu_derivative(Z1)\n",
    "        dW1 = (1/m) * (dZ1 @ X.T)\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def compute_accuracy(self, X, Y):\n",
    "        A2, _ = self.forward_propagation(X)\n",
    "        return np.mean((A2 > 0.5).astype(int) == Y)\n",
    "    \n",
    "    def fit(self, X, Y, epochs=1000, verbose=False):\n",
    "        for epoch in range(epochs):\n",
    "            A2, cache = self.forward_propagation(X)\n",
    "            self.losses.append(self.compute_loss(Y, A2))\n",
    "            self.accuracies.append(self.compute_accuracy(X, Y))\n",
    "            dW1, db1, dW2, db2 = self.backward_propagation(X, Y, cache)\n",
    "            self.v_dW1 = self.beta * self.v_dW1 + (1 - self.beta) * dW1\n",
    "            self.v_db1 = self.beta * self.v_db1 + (1 - self.beta) * db1\n",
    "            self.v_dW2 = self.beta * self.v_dW2 + (1 - self.beta) * dW2\n",
    "            self.v_db2 = self.beta * self.v_db2 + (1 - self.beta) * db2\n",
    "            self.W1 -= self.lr * self.v_dW1\n",
    "            self.b1 -= self.lr * self.v_db1\n",
    "            self.W2 -= self.lr * self.v_dW2\n",
    "            self.b2 -= self.lr * self.v_db2\n",
    "        return self\n",
    "\n",
    "class RMSProp:\n",
    "    \"\"\"RMSProp optimizer.\"\"\"\n",
    "    def __init__(self, n_x=2, n_h=10, n_y=1, learning_rate=0.001, beta=0.9, epsilon=1e-8, random_seed=42):\n",
    "        np.random.seed(random_seed)\n",
    "        self.lr = learning_rate\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.W1 = np.random.randn(n_h, n_x) * 0.1\n",
    "        self.b1 = np.zeros((n_h, 1))\n",
    "        self.W2 = np.random.randn(n_y, n_h) * 0.1\n",
    "        self.b2 = np.zeros((n_y, 1))\n",
    "        self.E_dW1 = np.zeros_like(self.W1)\n",
    "        self.E_db1 = np.zeros_like(self.b1)\n",
    "        self.E_dW2 = np.zeros_like(self.W2)\n",
    "        self.E_db2 = np.zeros_like(self.b2)\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        Z1 = self.W1 @ X + self.b1\n",
    "        A1 = relu(Z1)\n",
    "        Z2 = self.W2 @ A1 + self.b2\n",
    "        A2 = sigmoid(Z2)\n",
    "        return A2, {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n",
    "    \n",
    "    def compute_loss(self, Y, A2):\n",
    "        return -np.mean(Y * np.log(A2 + 1e-8) + (1 - Y) * np.log(1 - A2 + 1e-8))\n",
    "    \n",
    "    def backward_propagation(self, X, Y, cache):\n",
    "        m = X.shape[1]\n",
    "        Z1, A1, A2 = cache['Z1'], cache['A1'], cache['A2']\n",
    "        dZ2 = A2 - Y\n",
    "        dW2 = (1/m) * (dZ2 @ A1.T)\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        dA1 = self.W2.T @ dZ2\n",
    "        dZ1 = dA1 * relu_derivative(Z1)\n",
    "        dW1 = (1/m) * (dZ1 @ X.T)\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def compute_accuracy(self, X, Y):\n",
    "        A2, _ = self.forward_propagation(X)\n",
    "        return np.mean((A2 > 0.5).astype(int) == Y)\n",
    "    \n",
    "    def fit(self, X, Y, epochs=1000, verbose=False):\n",
    "        for epoch in range(epochs):\n",
    "            A2, cache = self.forward_propagation(X)\n",
    "            self.losses.append(self.compute_loss(Y, A2))\n",
    "            self.accuracies.append(self.compute_accuracy(X, Y))\n",
    "            dW1, db1, dW2, db2 = self.backward_propagation(X, Y, cache)\n",
    "            self.E_dW1 = self.beta * self.E_dW1 + (1 - self.beta) * dW1**2\n",
    "            self.E_db1 = self.beta * self.E_db1 + (1 - self.beta) * db1**2\n",
    "            self.E_dW2 = self.beta * self.E_dW2 + (1 - self.beta) * dW2**2\n",
    "            self.E_db2 = self.beta * self.E_db2 + (1 - self.beta) * db2**2\n",
    "            self.W1 -= (self.lr / np.sqrt(self.E_dW1 + self.epsilon)) * dW1\n",
    "            self.b1 -= (self.lr / np.sqrt(self.E_db1 + self.epsilon)) * db1\n",
    "            self.W2 -= (self.lr / np.sqrt(self.E_dW2 + self.epsilon)) * dW2\n",
    "            self.b2 -= (self.lr / np.sqrt(self.E_db2 + self.epsilon)) * db2\n",
    "        return self\n",
    "\n",
    "print(\"‚úÖ Comparison classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 2000\n",
    "\n",
    "print(\"üî¨ Training All Optimizers...\\n\")\n",
    "\n",
    "# 1. Vanilla SGD\n",
    "print(\"1Ô∏è‚É£  Training Vanilla SGD...\")\n",
    "model_sgd = VanillaSGD(learning_rate=0.01, random_seed=42)\n",
    "model_sgd.fit(X, Y, epochs=epochs)\n",
    "print(f\"   Final Loss: {model_sgd.losses[-1]:.4f}\")\n",
    "\n",
    "# 2. Momentum\n",
    "print(\"\\n2Ô∏è‚É£  Training Momentum...\")\n",
    "model_momentum = Momentum(learning_rate=0.01, beta=0.9, random_seed=42)\n",
    "model_momentum.fit(X, Y, epochs=epochs)\n",
    "print(f\"   Final Loss: {model_momentum.losses[-1]:.4f}\")\n",
    "\n",
    "# 3. RMSProp\n",
    "print(\"\\n3Ô∏è‚É£  Training RMSProp...\")\n",
    "model_rmsprop = RMSProp(learning_rate=0.001, beta=0.9, random_seed=42)\n",
    "model_rmsprop.fit(X, Y, epochs=epochs)\n",
    "print(f\"   Final Loss: {model_rmsprop.losses[-1]:.4f}\")\n",
    "\n",
    "# 4. Adam\n",
    "print(\"\\n4Ô∏è‚É£  Training Adam...\")\n",
    "model_adam = Adam(learning_rate=0.001, beta1=0.9, beta2=0.999, random_seed=42)\n",
    "model_adam.fit(X, Y, epochs=epochs)\n",
    "print(f\"   Final Loss: {model_adam.losses[-1]:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ All experiments complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "plt.plot(model_sgd.losses, linewidth=2.5, label='Vanilla SGD', \n",
    "        color='#FF6B6B', alpha=0.8)\n",
    "plt.plot(model_momentum.losses, linewidth=2.5, label='Momentum', \n",
    "        color='#4ECDC4', alpha=0.8)\n",
    "plt.plot(model_rmsprop.losses, linewidth=2.5, label='RMSProp', \n",
    "        color='#95E1D3', alpha=0.8)\n",
    "plt.plot(model_adam.losses, linewidth=3.5, label='Adam (Best!)', \n",
    "        color='#F38181', alpha=0.9, linestyle='-')\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('Loss', fontsize=13, fontweight='bold')\n",
    "plt.title('Loss Curves: Adam vs All Optimizers', fontsize=15, fontweight='bold')\n",
    "plt.legend(fontsize=12, loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observations:\")\n",
    "print(\"  ‚Ä¢ Adam: Fastest and most stable convergence\")\n",
    "print(\"  ‚Ä¢ Adam: Combines benefits of Momentum + RMSProp\")\n",
    "print(\"  ‚Ä¢ Adam: Best overall performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = {\n",
    "    'Optimizer': ['Vanilla SGD', 'Momentum', 'RMSProp', 'Adam'],\n",
    "    'Final Loss': [\n",
    "        f\"{model_sgd.losses[-1]:.4f}\",\n",
    "        f\"{model_momentum.losses[-1]:.4f}\",\n",
    "        f\"{model_rmsprop.losses[-1]:.4f}\",\n",
    "        f\"{model_adam.losses[-1]:.4f}\"\n",
    "    ],\n",
    "    'Final Accuracy': [\n",
    "        f\"{model_sgd.accuracies[-1]:.4f}\",\n",
    "        f\"{model_momentum.accuracies[-1]:.4f}\",\n",
    "        f\"{model_rmsprop.accuracies[-1]:.4f}\",\n",
    "        f\"{model_adam.accuracies[-1]:.4f}\"\n",
    "    ],\n",
    "    'Key Feature': [\n",
    "        'Simple baseline',\n",
    "        'Velocity accumulation',\n",
    "        'Adaptive LR per parameter',\n",
    "        'Momentum + Adaptive LR'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nüìä Optimizer Comparison:\\n\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nüèÜ Winner: Adam\")\n",
    "print(f\"   Final Loss: {model_adam.losses[-1]:.4f}\")\n",
    "print(f\"   Final Accuracy: {model_adam.accuracies[-1]:.4f}\")\n",
    "print(\"\\n   Why Adam Wins:\")\n",
    "print(\"   ‚Ä¢ Combines momentum (fast convergence)\")\n",
    "print(\"   ‚Ä¢ Combines RMSProp (adaptive learning rates)\")\n",
    "print(\"   ‚Ä¢ Includes bias correction\")\n",
    "print(\"   ‚Ä¢ Robust with default hyperparameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "‚úÖ **Adam = Momentum + RMSProp**\n",
    "- First moment (momentum): Direction and acceleration\n",
    "- Second moment (RMSProp): Adaptive learning rates\n",
    "- Bias correction: Crucial for early iterations\n",
    "\n",
    "‚úÖ **Mathematical Foundation**\n",
    "- First moment: $m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$\n",
    "- Second moment: $v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$\n",
    "- Bias correction: $\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}$, $\\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$\n",
    "- Update: $\\theta_t = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$\n",
    "\n",
    "‚úÖ **Advantages**\n",
    "- Fast convergence (from momentum)\n",
    "- Adaptive per-parameter learning rates (from RMSProp)\n",
    "- Robust to hyperparameter choices\n",
    "- Works well out-of-the-box\n",
    "- Industry standard!\n",
    "\n",
    "‚úÖ **Default Hyperparameters**\n",
    "- Œ± = 0.001 (learning rate)\n",
    "- Œ≤‚ÇÅ = 0.9 (first moment decay)\n",
    "- Œ≤‚ÇÇ = 0.999 (second moment decay)\n",
    "- Œµ = 10‚Åª‚Å∏ (numerical stability)\n",
    "\n",
    "### When to Use Adam?\n",
    "\n",
    "**Almost Always!** Adam is the default choice for:\n",
    "- Deep neural networks\n",
    "- Computer vision (CNNs)\n",
    "- Natural language processing (Transformers)\n",
    "- Reinforcement learning\n",
    "- Any complex optimization problem\n",
    "\n",
    "**Exceptions:**\n",
    "- Sometimes SGD with momentum generalizes better (requires careful tuning)\n",
    "- Some research suggests SGD for final fine-tuning\n",
    "\n",
    "### Variants of Adam\n",
    "\n",
    "- **AdamW**: Adam with weight decay (better regularization)\n",
    "- **Nadam**: Adam + Nesterov momentum\n",
    "- **RAdam**: Rectified Adam (better early training)\n",
    "- **AdaBelief**: Adapts to gradient predictability\n",
    "\n",
    "### Connection to Other Notebooks\n",
    "\n",
    "This notebook completes the optimizer series:\n",
    "- **`7_1`**: SGD basics\n",
    "- **`7_2`**: Learning rate\n",
    "- **`7_3`**: Learning rate decay\n",
    "- **`7_4`**: Momentum\n",
    "- **`7_5`**: AdaGrad\n",
    "- **`7_6`**: RMSProp\n",
    "- **`7_7`**: Adam (this notebook)\n",
    "\n",
    "---\n",
    "\n",
    "**üéì Congratulations!** You've completed the Optimizers series and mastered Adam!\n",
    "\n",
    "**Key Insight:** Adam combines the best of momentum and adaptive learning rates, making it the go-to optimizer for deep learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
