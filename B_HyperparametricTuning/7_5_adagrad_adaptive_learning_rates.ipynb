{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaGrad: Adaptive Learning Rates\n",
    "\n",
    "## üéØ What This Notebook Covers\n",
    "\n",
    "**AdaGrad** (Adaptive Gradient) adapts the learning rate for each parameter individually. In this notebook, we explore:\n",
    "\n",
    "1. ‚úÖ **Motivation** - Why adapt learning rates per parameter?\n",
    "2. ‚úÖ **Mathematical Formulation** - How AdaGrad works\n",
    "3. ‚úÖ **Implementation** - AdaGrad from scratch\n",
    "4. ‚úÖ **Advantages** - When AdaGrad shines\n",
    "5. ‚úÖ **Limitations** - The diminishing learning rate problem\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "- **Parameter-Specific Learning**: Different parameters need different learning rates üéØ\n",
    "- **Sparse Features**: Excellent for sparse data (NLP, recommender systems) üìä\n",
    "- **Automatic Adaptation**: No manual tuning needed ‚öôÔ∏è\n",
    "\n",
    "Let's master AdaGrad! üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Motivation: Why Adaptive Learning Rates?\n",
    "\n",
    "### The Problem with Fixed Learning Rates\n",
    "\n",
    "Consider a neural network with many parameters:\n",
    "\n",
    "```\n",
    "Parameter 1: Updated frequently (large gradients)\n",
    "    ‚Üí Needs SMALL learning rate\n",
    "    \n",
    "Parameter 2: Updated rarely (small gradients)\n",
    "    ‚Üí Needs LARGE learning rate\n",
    "    \n",
    "Fixed LR: Same for all parameters ‚ùå\n",
    "    ‚Üí Suboptimal for both!\n",
    "```\n",
    "\n",
    "### AdaGrad's Solution\n",
    "\n",
    "**Idea**: Adapt learning rate based on historical gradients\n",
    "\n",
    "- **Frequent updates** ‚Üí Accumulate large gradient sum ‚Üí **Smaller effective LR**\n",
    "- **Rare updates** ‚Üí Accumulate small gradient sum ‚Üí **Larger effective LR**\n",
    "\n",
    "### Real-World Example: Sparse Features\n",
    "\n",
    "In NLP or recommender systems:\n",
    "- Common words (\"the\", \"a\") ‚Üí Updated often ‚Üí Need small LR\n",
    "- Rare words (\"antidisestablishmentarianism\") ‚Üí Updated rarely ‚Üí Need large LR\n",
    "\n",
    "AdaGrad handles this automatically! ‚ú®\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mathematical Formulation\n",
    "\n",
    "### AdaGrad Update Rule\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G_t &= G_{t-1} + g_t^2 \\quad \\text{(accumulate squared gradients)} \\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\frac{\\alpha}{\\sqrt{G_t + \\epsilon}} \\cdot g_t \\quad \\text{(parameter update)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $g_t = \\nabla L(\\theta_t)$ = gradient at time $t$\n",
    "- $G_t$ = sum of squared gradients up to time $t$\n",
    "- $\\alpha$ = initial learning rate (e.g., 0.01)\n",
    "- $\\epsilon$ = small constant for numerical stability (e.g., $10^{-8}$)\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "The effective learning rate for each parameter is:\n",
    "\n",
    "$$\n",
    "\\alpha_{\\text{effective}} = \\frac{\\alpha}{\\sqrt{G_t + \\epsilon}}\n",
    "$$\n",
    "\n",
    "- **Large $G_t$** (frequent updates) ‚Üí **Small effective LR**\n",
    "- **Small $G_t$** (rare updates) ‚Üí **Large effective LR**\n",
    "\n",
    "### Element-wise Operations\n",
    "\n",
    "All operations are **element-wise** (per parameter):\n",
    "\n",
    "$$\n",
    "\\theta_{t+1,i} = \\theta_{t,i} - \\frac{\\alpha}{\\sqrt{G_{t,i} + \\epsilon}} \\cdot g_{t,i}\n",
    "$$\n",
    "\n",
    "Each parameter has its own accumulated gradient sum!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Adaptive Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate gradient history for two parameters\n",
    "np.random.seed(42)\n",
    "iterations = 100\n",
    "\n",
    "# Parameter 1: Frequent large updates\n",
    "gradients_param1 = np.random.randn(iterations) * 2.0 + 1.0\n",
    "\n",
    "# Parameter 2: Rare small updates\n",
    "gradients_param2 = np.random.randn(iterations) * 0.2\n",
    "\n",
    "# Compute AdaGrad learning rates\n",
    "alpha = 0.1\n",
    "epsilon = 1e-8\n",
    "\n",
    "G1 = np.zeros(iterations)\n",
    "G2 = np.zeros(iterations)\n",
    "lr1 = np.zeros(iterations)\n",
    "lr2 = np.zeros(iterations)\n",
    "\n",
    "for t in range(iterations):\n",
    "    if t == 0:\n",
    "        G1[t] = gradients_param1[t]**2\n",
    "        G2[t] = gradients_param2[t]**2\n",
    "    else:\n",
    "        G1[t] = G1[t-1] + gradients_param1[t]**2\n",
    "        G2[t] = G2[t-1] + gradients_param2[t]**2\n",
    "    \n",
    "    lr1[t] = alpha / (np.sqrt(G1[t] + epsilon))\n",
    "    lr2[t] = alpha / (np.sqrt(G2[t] + epsilon))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Accumulated gradients\n",
    "axes[0].plot(G1, linewidth=2.5, label='Parameter 1 (frequent updates)', color='#FF6B6B')\n",
    "axes[0].plot(G2, linewidth=2.5, label='Parameter 2 (rare updates)', color='#4ECDC4')\n",
    "axes[0].set_xlabel('Iteration', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Accumulated Squared Gradients (G)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Gradient Accumulation', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Effective learning rates\n",
    "axes[1].plot(lr1, linewidth=2.5, label='Parameter 1 (decreases fast)', color='#FF6B6B')\n",
    "axes[1].plot(lr2, linewidth=2.5, label='Parameter 2 (decreases slow)', color='#4ECDC4')\n",
    "axes[1].set_xlabel('Iteration', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Effective Learning Rate', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Adaptive Learning Rates', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observations:\")\n",
    "print(\"  ‚Ä¢ Parameter 1: Large gradients ‚Üí Fast accumulation ‚Üí Small LR\")\n",
    "print(\"  ‚Ä¢ Parameter 2: Small gradients ‚Üí Slow accumulation ‚Üí Large LR\")\n",
    "print(\"  ‚Ä¢ AdaGrad automatically adapts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spiral_data(n_samples=300, noise=0.1):\n",
    "    \"\"\"\n",
    "    Generate spiral dataset for binary classification.\n",
    "    \n",
    "    Returns:\n",
    "    - X: Features (n_x, m)\n",
    "    - Y: Labels (1, m)\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    m = n_samples\n",
    "    \n",
    "    # Create spiral\n",
    "    theta = np.linspace(0, 4*np.pi, m//2)\n",
    "    r = np.linspace(0.5, 2, m//2)\n",
    "    \n",
    "    # Class 0: spiral\n",
    "    X_class0 = np.vstack([r * np.cos(theta), r * np.sin(theta)])\n",
    "    Y_class0 = np.zeros((1, m//2))\n",
    "    \n",
    "    # Class 1: spiral (rotated)\n",
    "    X_class1 = np.vstack([r * np.cos(theta + np.pi), r * np.sin(theta + np.pi)])\n",
    "    Y_class1 = np.ones((1, m//2))\n",
    "    \n",
    "    # Combine\n",
    "    X = np.hstack([X_class0, X_class1])\n",
    "    Y = np.hstack([Y_class0, Y_class1])\n",
    "    \n",
    "    # Add noise\n",
    "    X += np.random.randn(*X.shape) * noise\n",
    "    \n",
    "    # Shuffle\n",
    "    indices = np.random.permutation(m)\n",
    "    X = X[:, indices]\n",
    "    Y = Y[:, indices]\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "# Generate data\n",
    "X, Y = generate_spiral_data(n_samples=300, noise=0.1)\n",
    "\n",
    "print(f\"Dataset shape: X={X.shape}, Y={Y.shape}\")\n",
    "print(f\"Number of samples: {X.shape[1]}\")\n",
    "print(f\"Number of features: {X.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Neural Network with AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"Derivative of ReLU.\"\"\"\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "print(\"‚úÖ Activation functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    Neural network with AdaGrad optimizer.\n",
    "    \n",
    "    Architecture: Input (2) ‚Üí Hidden (10, ReLU) ‚Üí Output (1, Sigmoid)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_x=2, n_h=10, n_y=1, learning_rate=0.1, epsilon=1e-8, random_seed=42):\n",
    "        \"\"\"\n",
    "        Initialize neural network with AdaGrad.\n",
    "        \n",
    "        Parameters:\n",
    "        - learning_rate: Initial learning rate (Œ±)\n",
    "        - epsilon: Small constant for numerical stability\n",
    "        \"\"\"\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        self.lr = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.W1 = np.random.randn(n_h, n_x) * 0.1\n",
    "        self.b1 = np.zeros((n_h, 1))\n",
    "        self.W2 = np.random.randn(n_y, n_h) * 0.1\n",
    "        self.b2 = np.zeros((n_y, 1))\n",
    "        \n",
    "        # Initialize accumulated squared gradients\n",
    "        self.G_dW1 = np.zeros_like(self.W1)\n",
    "        self.G_db1 = np.zeros_like(self.b1)\n",
    "        self.G_dW2 = np.zeros_like(self.W2)\n",
    "        self.G_db2 = np.zeros_like(self.b2)\n",
    "        \n",
    "        # Training history\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"Forward propagation.\"\"\"\n",
    "        Z1 = self.W1 @ X + self.b1\n",
    "        A1 = relu(Z1)\n",
    "        Z2 = self.W2 @ A1 + self.b2\n",
    "        A2 = sigmoid(Z2)\n",
    "        \n",
    "        cache = {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n",
    "        return A2, cache\n",
    "    \n",
    "    def compute_loss(self, Y, A2):\n",
    "        \"\"\"Compute binary cross-entropy loss.\"\"\"\n",
    "        m = Y.shape[1]\n",
    "        loss = -np.mean(Y * np.log(A2 + 1e-8) + (1 - Y) * np.log(1 - A2 + 1e-8))\n",
    "        return loss\n",
    "    \n",
    "    def backward_propagation(self, X, Y, cache):\n",
    "        \"\"\"Backward propagation.\"\"\"\n",
    "        m = X.shape[1]\n",
    "        Z1, A1, Z2, A2 = cache['Z1'], cache['A1'], cache['Z2'], cache['A2']\n",
    "        \n",
    "        # Backprop\n",
    "        dZ2 = A2 - Y\n",
    "        dW2 = (1/m) * (dZ2 @ A1.T)\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        \n",
    "        dA1 = self.W2.T @ dZ2\n",
    "        dZ1 = dA1 * relu_derivative(Z1)\n",
    "        dW1 = (1/m) * (dZ1 @ X.T)\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def update_parameters_adagrad(self, dW1, db1, dW2, db2):\n",
    "        \"\"\"\n",
    "        Update parameters using AdaGrad.\n",
    "        \n",
    "        G_t = G_{t-1} + g_t^2\n",
    "        Œ∏_t = Œ∏_{t-1} - (Œ± / sqrt(G_t + Œµ)) * g_t\n",
    "        \"\"\"\n",
    "        # Accumulate squared gradients\n",
    "        self.G_dW1 += dW1**2\n",
    "        self.G_db1 += db1**2\n",
    "        self.G_dW2 += dW2**2\n",
    "        self.G_db2 += db2**2\n",
    "        \n",
    "        # Update parameters with adaptive learning rates\n",
    "        self.W1 -= (self.lr / np.sqrt(self.G_dW1 + self.epsilon)) * dW1\n",
    "        self.b1 -= (self.lr / np.sqrt(self.G_db1 + self.epsilon)) * db1\n",
    "        self.W2 -= (self.lr / np.sqrt(self.G_dW2 + self.epsilon)) * dW2\n",
    "        self.b2 -= (self.lr / np.sqrt(self.G_db2 + self.epsilon)) * db2\n",
    "    \n",
    "    def compute_accuracy(self, X, Y):\n",
    "        \"\"\"Compute accuracy.\"\"\"\n",
    "        A2, _ = self.forward_propagation(X)\n",
    "        predictions = (A2 > 0.5).astype(int)\n",
    "        accuracy = np.mean(predictions == Y)\n",
    "        return accuracy\n",
    "    \n",
    "    def fit(self, X, Y, epochs=1000, verbose=False):\n",
    "        \"\"\"Train the network with AdaGrad.\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            A2, cache = self.forward_propagation(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(Y, A2)\n",
    "            self.losses.append(loss)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            accuracy = self.compute_accuracy(X, Y)\n",
    "            self.accuracies.append(accuracy)\n",
    "            \n",
    "            # Backward propagation\n",
    "            dW1, db1, dW2, db2 = self.backward_propagation(X, Y, cache)\n",
    "            \n",
    "            # Update parameters with AdaGrad\n",
    "            self.update_parameters_adagrad(dW1, db1, dW2, db2)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch + 1) % 200 == 0:\n",
    "                print(f\"Epoch {epoch+1:4d}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n‚úÖ Training Complete!\")\n",
    "            print(f\"   Final Loss: {self.losses[-1]:.4f}\")\n",
    "            print(f\"   Final Accuracy: {self.accuracies[-1]:.4f}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "print(\"‚úÖ AdaGrad class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison: AdaGrad vs SGD vs Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need SGD and Momentum classes for comparison\n",
    "class VanillaSGD:\n",
    "    \"\"\"Vanilla SGD for comparison.\"\"\"\n",
    "    def __init__(self, n_x=2, n_h=10, n_y=1, learning_rate=0.01, random_seed=42):\n",
    "        np.random.seed(random_seed)\n",
    "        self.lr = learning_rate\n",
    "        self.W1 = np.random.randn(n_h, n_x) * 0.1\n",
    "        self.b1 = np.zeros((n_h, 1))\n",
    "        self.W2 = np.random.randn(n_y, n_h) * 0.1\n",
    "        self.b2 = np.zeros((n_y, 1))\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        Z1 = self.W1 @ X + self.b1\n",
    "        A1 = relu(Z1)\n",
    "        Z2 = self.W2 @ A1 + self.b2\n",
    "        A2 = sigmoid(Z2)\n",
    "        return A2, {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n",
    "    \n",
    "    def compute_loss(self, Y, A2):\n",
    "        return -np.mean(Y * np.log(A2 + 1e-8) + (1 - Y) * np.log(1 - A2 + 1e-8))\n",
    "    \n",
    "    def backward_propagation(self, X, Y, cache):\n",
    "        m = X.shape[1]\n",
    "        Z1, A1, A2 = cache['Z1'], cache['A1'], cache['A2']\n",
    "        dZ2 = A2 - Y\n",
    "        dW2 = (1/m) * (dZ2 @ A1.T)\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        dA1 = self.W2.T @ dZ2\n",
    "        dZ1 = dA1 * relu_derivative(Z1)\n",
    "        dW1 = (1/m) * (dZ1 @ X.T)\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def compute_accuracy(self, X, Y):\n",
    "        A2, _ = self.forward_propagation(X)\n",
    "        return np.mean((A2 > 0.5).astype(int) == Y)\n",
    "    \n",
    "    def fit(self, X, Y, epochs=1000, verbose=False):\n",
    "        for epoch in range(epochs):\n",
    "            A2, cache = self.forward_propagation(X)\n",
    "            self.losses.append(self.compute_loss(Y, A2))\n",
    "            self.accuracies.append(self.compute_accuracy(X, Y))\n",
    "            dW1, db1, dW2, db2 = self.backward_propagation(X, Y, cache)\n",
    "            self.W1 -= self.lr * dW1\n",
    "            self.b1 -= self.lr * db1\n",
    "            self.W2 -= self.lr * dW2\n",
    "            self.b2 -= self.lr * db2\n",
    "        return self\n",
    "\n",
    "print(\"‚úÖ Comparison classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 2000\n",
    "\n",
    "print(\"üî¨ Training Models...\\n\")\n",
    "\n",
    "# 1. Vanilla SGD\n",
    "print(\"1Ô∏è‚É£  Training Vanilla SGD...\")\n",
    "model_sgd = VanillaSGD(learning_rate=0.01, random_seed=42)\n",
    "model_sgd.fit(X, Y, epochs=epochs)\n",
    "print(f\"   Final Loss: {model_sgd.losses[-1]:.4f}\")\n",
    "\n",
    "# 2. AdaGrad\n",
    "print(\"\\n2Ô∏è‚É£  Training AdaGrad...\")\n",
    "model_adagrad = AdaGrad(learning_rate=0.1, random_seed=42)\n",
    "model_adagrad.fit(X, Y, epochs=epochs)\n",
    "print(f\"   Final Loss: {model_adagrad.losses[-1]:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ All experiments complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "plt.plot(model_sgd.losses, linewidth=2.5, label='Vanilla SGD', \n",
    "        color='#FF6B6B', alpha=0.8)\n",
    "plt.plot(model_adagrad.losses, linewidth=2.5, label='AdaGrad', \n",
    "        color='#4ECDC4', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('Loss', fontsize=13, fontweight='bold')\n",
    "plt.title('Loss Curves: AdaGrad vs Vanilla SGD', fontsize=15, fontweight='bold')\n",
    "plt.legend(fontsize=12, loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observations:\")\n",
    "print(\"  ‚Ä¢ AdaGrad: Faster initial convergence\")\n",
    "print(\"  ‚Ä¢ AdaGrad: Automatic learning rate adaptation\")\n",
    "print(\"  ‚Ä¢ AdaGrad: May slow down in later epochs (diminishing LR)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. The Diminishing Learning Rate Problem\n",
    "\n",
    "### AdaGrad's Main Limitation\n",
    "\n",
    "Since $G_t$ only **accumulates** (never decreases):\n",
    "\n",
    "$$\n",
    "G_t = G_{t-1} + g_t^2 \\implies G_t \\text{ grows monotonically}\n",
    "$$\n",
    "\n",
    "This means:\n",
    "\n",
    "$$\n",
    "\\alpha_{\\text{effective}} = \\frac{\\alpha}{\\sqrt{G_t + \\epsilon}} \\to 0 \\text{ as } t \\to \\infty\n",
    "$$\n",
    "\n",
    "### Consequences\n",
    "\n",
    "```\n",
    "Early Training:\n",
    "    ‚Ä¢ G_t is small\n",
    "    ‚Ä¢ Effective LR is large\n",
    "    ‚Ä¢ ‚úÖ Fast progress\n",
    "\n",
    "Late Training:\n",
    "    ‚Ä¢ G_t is very large\n",
    "    ‚Ä¢ Effective LR is tiny\n",
    "    ‚Ä¢ ‚ùå Training stops (premature convergence)\n",
    "```\n",
    "\n",
    "### When This is a Problem\n",
    "\n",
    "- **Long training runs**: LR becomes too small\n",
    "- **Non-convex problems**: May get stuck in saddle points\n",
    "- **Deep networks**: Accumulation happens faster\n",
    "\n",
    "### Solution: RMSProp\n",
    "\n",
    "RMSProp (next notebook) fixes this by using **exponentially weighted average** instead of sum!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "‚úÖ **Motivation**\n",
    "- Different parameters need different learning rates\n",
    "- Sparse features benefit greatly\n",
    "- Automatic adaptation\n",
    "\n",
    "‚úÖ **Mathematical Foundation**\n",
    "- Accumulate squared gradients: $G_t = G_{t-1} + g_t^2$\n",
    "- Adaptive update: $\\theta_t = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{G_t + \\epsilon}} g_t$\n",
    "- Element-wise operations\n",
    "\n",
    "‚úÖ **Advantages**\n",
    "- No manual learning rate tuning per parameter\n",
    "- Excellent for sparse data (NLP, recommender systems)\n",
    "- Fast initial convergence\n",
    "\n",
    "‚úÖ **Limitations**\n",
    "- Diminishing learning rate problem\n",
    "- May stop learning prematurely\n",
    "- Not ideal for deep networks\n",
    "\n",
    "### When to Use AdaGrad?\n",
    "\n",
    "**Good For:**\n",
    "- Sparse features (NLP, recommender systems)\n",
    "- Convex optimization\n",
    "- Short training runs\n",
    "\n",
    "**Not Good For:**\n",
    "- Deep neural networks\n",
    "- Long training runs\n",
    "- Non-convex optimization\n",
    "\n",
    "### Connection to Other Notebooks\n",
    "\n",
    "This notebook builds on:\n",
    "- **`7_1-7_4`**: Previous optimizer notebooks\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "üöÄ **Coming Next:**\n",
    "- **7.6 RMSProp**: Fixes AdaGrad's diminishing LR problem\n",
    "- **7.7 Adam**: Combines momentum + RMSProp (most popular!)\n",
    "\n",
    "---\n",
    "\n",
    "**üéì Congratulations!** You now understand AdaGrad and its adaptive learning rates!\n",
    "\n",
    "**Key Insight:** AdaGrad adapts learning rates per parameter, but watch out for the diminishing learning rate problem!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
